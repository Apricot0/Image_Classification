{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XSfnuObtYYMH"
   },
   "source": [
    "# Birds Classification\n",
    "\n",
    "\n",
    "## Google Colab Tutorial\n",
    "---\n",
    "https://colab.research.google.com/notebooks/\n",
    "\n",
    "Settings used for: ***Edit -> Notebook Settings -> Runtime Type (Python 3)***.\n",
    "\n",
    "\n",
    "## Description\n",
    "---\n",
    "This project is an introduction to deep learning tools for computer vision. [PyTorch](http://pytorch.org).\n",
    "\n",
    "In step 1 of the project I trained a deep convolutional network from scratch to recognize scenes. I designed a simple network architecture with jittering, normalization, and regularization.\n",
    "\n",
    "For step 2 Fine-tuning a pre-trained deep network is involved. We used the pretrained AlexNet & ResNet network which was not trained to recognize scenes at all.\n",
    "\n",
    "These two approaches represent the most common approaches to recognition problems in computer vision today -- train a deep network from scratch if you have enough data (it's not always obvious whether or not you do), and if you cannot then instead fine-tune a pre-trained network.\n",
    "\n",
    "For Step 3 I trained a small Vision Transformer (ViT) from scratch. ViT is pre-trained on large amounts of\n",
    "data and transferred to multiple mid-sized or small image recognition benchmarks(ImageNet, CIFAR-100, VTAB, etc.), ViT attains excellent results compared to state-of-the-art convolutional networks.\n",
    "\n",
    "\n",
    "## Dataset\n",
    "---\n",
    "Under the root folder, there should be a folder named \"data\" containing the images.\n",
    "\n",
    "## Some Tutorials (PyTorch)\n",
    "---\n",
    "- PyTorch for deep learning toolbox (follow the [link](http://pytorch.org) for installation).\n",
    "- For PyTorch beginners, please read this [tutorial](http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html).\n",
    "- Feel free to study more tutorials at http://pytorch.org/tutorials/.\n",
    "- Find cool visualization here at http://playground.tensorflow.org.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "0UA6WFgcYYMI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jasper\\AppData\\Local\\Programs\\Python\\Python312\\python.exe\n"
     ]
    }
   ],
   "source": [
    "# import packages here\n",
    "import os\n",
    "import sys\n",
    "print (sys.executable)\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import random\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch import Tensor\n",
    "from typing import List, Optional\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L7T72O-1ks-a",
    "outputId": "a2ae467c-9042-4d25-863b-ca333945160c"
   },
   "outputs": [],
   "source": [
    "# # Mount your google drive where you've saved your assignment folder\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xIT3WIuykrg0",
    "outputId": "6449b1a8-b4c7-41c3-9141-2de00054d956"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C has no label.\n",
      " Volume Serial Number is C0E0-2290\n",
      "\n",
      " Directory of C:\\Users\\Jasper\\Downloads\\CSE527_2\n",
      "\n",
      "03/23/2024  01:02 AM    <DIR>          .\n",
      "03/22/2024  02:46 PM    <DIR>          ..\n",
      "03/22/2024  05:51 PM    <DIR>          .ipynb_checkpoints\n",
      "03/23/2024  01:02 AM           765,728 CSE527_24S_hw2.ipynb\n",
      "03/21/2024  07:19 PM    <DIR>          data\n",
      "               1 File(s)        765,728 bytes\n",
      "               4 Dir(s)  539,128,975,360 bytes free\n"
     ]
    }
   ],
   "source": [
    "# # Set your working directory (in your google drive)\n",
    "# #   change it to your specific homework directory.\n",
    "# %cd '/content/gdrive/MyDrive/CSE527_2'\n",
    "%ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HnkJ-LmJyBod"
   },
   "source": [
    "## Loading and Preprocessing data\n",
    "---\n",
    "Loads data into minibatches for training and testing in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_names: {0: '001.Black_footed_Albatross', 1: '002.Laysan_Albatross', 2: '005.Crested_Auklet', 3: '010.Red_winged_Blackbird', 4: '015.Lazuli_Bunting', 5: '016.Painted_Bunting', 6: '017.Cardinal', 7: '044.Frigatebird', 8: '047.American_Goldfinch', 9: '054.Blue_Grosbeak', 10: '070.Green_Violetear', 11: '074.Florida_Jay', 12: '087.Mallard', 13: '101.White_Pelican', 14: '159.Black_and_white_Warbler', 15: '187.American_Three_toed_Woodpecker', 16: '188.Pileated_Woodpecker', 17: '189.Red_bellied_Woodpecker', 18: '191.Red_headed_Woodpecker', 19: '192.Downy_Woodpecker'} \n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------------\n",
    "#    Load Training Data and Testing Data\n",
    "#--------------------------------------------------\n",
    "\n",
    "def set_random_seed(seed):\n",
    "     torch.manual_seed(seed)\n",
    "     torch.cuda.manual_seed_all(seed)\n",
    "     np.random.seed(seed)\n",
    "     random.seed(seed)\n",
    "     torch.backends.cudnn.deterministic = True\n",
    "set_random_seed(0)\n",
    "\n",
    "class_names = [name[13:] for name in glob.glob('./data/train/*')] # name attributes\n",
    "class_names = dict(zip(range(len(class_names)), class_names))\n",
    "print(\"class_names: %s \" % class_names)\n",
    "\n",
    "def img_rot(img):\n",
    "  # Rotate the image by 90 degrees clockwise\n",
    "  rotated_img = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n",
    "  return rotated_img\n",
    "\n",
    "\n",
    "# split data into train and test\n",
    "def load_dataset(path, img_size, num_per_class=-1, batch_size=16, shuffle=False,\n",
    "           augment=False, zoom=False, rotate=False, crop=False, is_color=False, zero_centered=False):\n",
    "    data = []\n",
    "    labels = []\n",
    "    channel_num = 3 if is_color else 1\n",
    "\n",
    "    # read images and resizing\n",
    "    for id, class_name in class_names.items():\n",
    "        print(\"Loading images from class: %s\" % id)\n",
    "        img_path_class = glob.glob(path + class_name + '/*.jpg')\n",
    "        if num_per_class > 0:\n",
    "            img_path_class = img_path_class[:num_per_class]\n",
    "        labels.extend([id]*len(img_path_class))\n",
    "        for filename in img_path_class:\n",
    "            if is_color:\n",
    "                img = cv2.imread(filename)\n",
    "            else:\n",
    "                img = cv2.imread(filename, 0)\n",
    "\n",
    "            # resize the image\n",
    "            img = cv2.resize(img, img_size, cv2.INTER_LINEAR)\n",
    "\n",
    "            if is_color:\n",
    "                img = np.transpose(img, [2, 0, 1])\n",
    "\n",
    "            # norm pixel values to [-1, 1]\n",
    "            data.append(img.astype(np.float64)/255*2-1)\n",
    "    # Data Augmentation\n",
    "    # print(\"data before augmentation: \", data)\n",
    "    if augment:\n",
    "        data.extend([img_rot(img) for img in data])\n",
    "        labels.extend(labels)\n",
    "        data.extend([img_rot(img_rot(img)) for img in data])\n",
    "        labels.extend(labels)\n",
    "        data.extend([np.fliplr(img) for img in data])\n",
    "        labels.extend(labels)\n",
    "        # print(\"data after augmentation: \", data)\n",
    "\n",
    "    # Data Normalization\n",
    "    if zero_centered:\n",
    "        # for img in data:\n",
    "        #     img -= np.mean(img)\n",
    "        #     # img /= np.max(img) - np.min(img)\n",
    "        data -= np.mean(data, 0)\n",
    "\n",
    "    # randomly permute (this step is important for training)\n",
    "    if shuffle:\n",
    "        bundle = list(zip(data, labels))\n",
    "        random.shuffle(bundle)\n",
    "        data, labels = zip(*bundle)\n",
    "\n",
    "    # divide data into minibatches of TorchTensors\n",
    "    if batch_size > 1:\n",
    "        batch_data = []\n",
    "        batch_labels = []\n",
    "\n",
    "        for i in range(int(len(data) / batch_size)):\n",
    "            minibatch_d = data[i*batch_size: (i+1)*batch_size]\n",
    "            minibatch_d = np.reshape(minibatch_d, (batch_size, channel_num, img_size[0], img_size[1]))\n",
    "            batch_data.append(torch.from_numpy(minibatch_d))\n",
    "\n",
    "            minibatch_l = labels[i*batch_size: (i+1)*batch_size]\n",
    "            batch_labels.append(torch.LongTensor(minibatch_l))\n",
    "        data, labels = batch_data, batch_labels\n",
    "    return zip(batch_data, batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wHI2VYbDloje",
    "outputId": "3d628f4a-63a8-49ef-b1a2-a9ab05763d99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images from class: 0\n",
      "Loading images from class: 1\n",
      "Loading images from class: 2\n",
      "Loading images from class: 3\n",
      "Loading images from class: 4\n",
      "Loading images from class: 5\n",
      "Loading images from class: 6\n",
      "Loading images from class: 7\n",
      "Loading images from class: 8\n",
      "Loading images from class: 9\n",
      "Loading images from class: 10\n",
      "Loading images from class: 11\n",
      "Loading images from class: 12\n",
      "Loading images from class: 13\n",
      "Loading images from class: 14\n",
      "Loading images from class: 15\n",
      "Loading images from class: 16\n",
      "Loading images from class: 17\n",
      "Loading images from class: 18\n",
      "Loading images from class: 19\n",
      "Finish loading 34 minibatches (batch_size=16) of training samples.\n",
      "Loading images from class: 0\n",
      "Loading images from class: 1\n",
      "Loading images from class: 2\n",
      "Loading images from class: 3\n",
      "Loading images from class: 4\n",
      "Loading images from class: 5\n",
      "Loading images from class: 6\n",
      "Loading images from class: 7\n",
      "Loading images from class: 8\n",
      "Loading images from class: 9\n",
      "Loading images from class: 10\n",
      "Loading images from class: 11\n",
      "Loading images from class: 12\n",
      "Loading images from class: 13\n",
      "Loading images from class: 14\n",
      "Loading images from class: 15\n",
      "Loading images from class: 16\n",
      "Loading images from class: 17\n",
      "Loading images from class: 18\n",
      "Loading images from class: 19\n",
      "Finish loading 3 minibatches (batch_size=16) of testing samples.\n"
     ]
    }
   ],
   "source": [
    "# load data into size (64, 64)\n",
    "img_size = (64, 64)\n",
    "batch_size = 16 # training sample number per batch\n",
    "\n",
    "# load training dataset\n",
    "trainloader_small = list(load_dataset('./data/train/', img_size, batch_size=batch_size, shuffle=True))\n",
    "train_num = len(trainloader_small)\n",
    "print(\"Finish loading %d minibatches (batch_size=%d) of training samples.\" % (train_num, batch_size))\n",
    "\n",
    "# load testing dataset\n",
    "testloader_small = list(load_dataset('./data/test/', img_size, num_per_class=50, batch_size=batch_size))\n",
    "test_num = len(testloader_small)\n",
    "print(\"Finish loading %d minibatches (batch_size=%d) of testing samples.\" % (test_num, batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "krCXjDOzlq0f",
    "outputId": "184c0079-5aeb-4aa2-87e1-1b76df206595"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "054.Blue_Grosbeak\n",
      "torch.Size([64, 64])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show some images\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    if len(npimg.shape) > 2:\n",
    "        npimg = np.transpose(img, [1, 2, 0])\n",
    "    plt.figure\n",
    "    # plt.imshow(npimg, 'gray')\n",
    "    plt.imshow(npimg)\n",
    "    plt.show()\n",
    "img, label = trainloader_small[0][0][11][0], trainloader_small[0][1][11]\n",
    "label = int(np.array(label))\n",
    "print(class_names[label])\n",
    "print(img.shape)\n",
    "imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JGA-K6QzYYMR"
   },
   "source": [
    "## Training a Network From Scratch [40 points]\n",
    "Gone are the days of hand designed features. Now we have end-to-end learning in which a highly non-linear representation is learned for our data to maximize our objective function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s6OPghPL1ZGt"
   },
   "source": [
    "A simpe network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LI9qG3x-zbe-"
   },
   "outputs": [],
   "source": [
    "#--------------------------------------------------\n",
    "#       Define Network Architecture\n",
    "#--------------------------------------------------\n",
    "class TNet(nn.Module):\n",
    "    def __init__(self):\n",
    "      super(TNet,self).__init__()\n",
    "      self.inners = torch.nn.Sequential(\n",
    "        nn.Conv2d(1, 32, 5, stride=1, padding=2),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(4, stride=4), \n",
    "      )\n",
    "      \n",
    "      self.classifier = nn.Sequential(\n",
    "         nn.Linear(8192, 20), \n",
    "      )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "      x = self.inners(x)  \n",
    "      x = torch.flatten(x, 1)\n",
    "      x = self.classifier(x)\n",
    "      return x \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-PBerwk-1leX"
   },
   "source": [
    "Model training and evaluation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "sNJTDG8xoJwH"
   },
   "outputs": [],
   "source": [
    "#--------------------------------------------------\n",
    "#       Model Training Function\n",
    "#--------------------------------------------------\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "def trainModel(net, trainloader, train_option, testloader=None):\n",
    "  loss_func = nn.CrossEntropyLoss()\n",
    "  lr = train_option['lr']\n",
    "  epoch = train_option['epoch']\n",
    "  device = train_option['device'] if 'device' in train_option.keys() else 'cpu'\n",
    "  log_iter = train_option['log_iter'] if 'log_iter' in train_option.keys() else 20\n",
    "  eval_epoch = 1\n",
    "\n",
    "  if 'optimizer' in train_option.keys():\n",
    "    optimizer = train_option['optimizer']\n",
    "  else:\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "  start_time = time.time()\n",
    "  if device == 'gpu':\n",
    "    net = net.cuda()\n",
    "\n",
    "  iters = 0\n",
    "  running_loss = 0.0\n",
    "  for ep in range(epoch):\n",
    "    net.train()\n",
    "    for iter, (x, y) in enumerate(trainloader):\n",
    "      iters += 1\n",
    "      batch_x = Variable(x).float()\n",
    "      batch_y = Variable(y).long()\n",
    "      if device == 'gpu':\n",
    "        batch_x = batch_x.cuda()\n",
    "        batch_y = batch_y.cuda()\n",
    "\n",
    "      outputs = net(batch_x)\n",
    "      loss = loss_func(outputs, batch_y)\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      running_loss += loss.item()\n",
    "\n",
    "      time_lapse = time.strftime('%H:%M:%S', time.gmtime(time.time() - start_time))\n",
    "      if iter % log_iter == 0:\n",
    "        print('Epoch:{:2d} | Iter:{:5d} | Time: {} | Train Loss: {:.4f} | Average Loss: {:.4f} '.format(ep+1, iter, time_lapse, loss.item(), running_loss/iters))\n",
    "\n",
    "    if testloader is not None and ep % eval_epoch == 0:\n",
    "      evalModel(net, testloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "m5sadf8qoOW6"
   },
   "outputs": [],
   "source": [
    "#--------------------------------------------------\n",
    "#       Model Evaluating Function\n",
    "#--------------------------------------------------\n",
    "import time\n",
    "\n",
    "def evalModel(net, testloader):\n",
    "  acc = 0.0\n",
    "  count = 0\n",
    "  start_time = time.time()\n",
    "  device = 'gpu' if next(net.parameters()).is_cuda else 'cpu'\n",
    "  net.eval()\n",
    "\n",
    "  for iter, (x, y) in enumerate(testloader):\n",
    "        count += x.shape[0]\n",
    "        batch_x = Variable(x).float()\n",
    "        batch_y = Variable(y).long()\n",
    "        if device == 'gpu':\n",
    "          batch_x = batch_x.cuda()\n",
    "          batch_y = batch_y.cuda()\n",
    "        outputs = net(batch_x)\n",
    "        acc += torch.sum(outputs.max(1)[1]==batch_y)\n",
    "\n",
    "  time_lapse = time.strftime('%H:%M:%S', time.gmtime(time.time() - start_time))\n",
    "  print('Accuracy: {:5f} | Time: {}'.format(acc/count,time_lapse))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mrm2SErt7wzZ"
   },
   "source": [
    "Training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "KDwrBoHVukOK",
    "outputId": "3aedc43b-cd52-437b-cc70-c846d3aab253"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.2.1+cu121\n",
      "Is CUDA enabled? True\n",
      "Epoch: 1 | Iter:    0 | Time: 00:00:00 | Train Loss: 3.0882 | Average Loss: 3.0882 \n",
      "Epoch: 1 | Iter:   20 | Time: 00:00:00 | Train Loss: 2.9925 | Average Loss: 3.5141 \n",
      "Accuracy: 0.208333 | Time: 00:00:00\n",
      "Epoch: 2 | Iter:    0 | Time: 00:00:00 | Train Loss: 2.0423 | Average Loss: 3.2714 \n",
      "Epoch: 2 | Iter:   20 | Time: 00:00:00 | Train Loss: 2.3818 | Average Loss: 2.8944 \n",
      "Accuracy: 0.250000 | Time: 00:00:00\n",
      "Epoch: 3 | Iter:    0 | Time: 00:00:00 | Train Loss: 1.4545 | Average Loss: 2.7348 \n",
      "Epoch: 3 | Iter:   20 | Time: 00:00:00 | Train Loss: 1.9310 | Average Loss: 2.5032 \n",
      "Accuracy: 0.250000 | Time: 00:00:00\n",
      "Epoch: 4 | Iter:    0 | Time: 00:00:00 | Train Loss: 1.1724 | Average Loss: 2.3767 \n",
      "Epoch: 4 | Iter:   20 | Time: 00:00:00 | Train Loss: 1.3963 | Average Loss: 2.1887 \n",
      "Accuracy: 0.250000 | Time: 00:00:00\n",
      "Epoch: 5 | Iter:    0 | Time: 00:00:00 | Train Loss: 0.9051 | Average Loss: 2.0809 \n",
      "Epoch: 5 | Iter:   20 | Time: 00:00:00 | Train Loss: 0.9554 | Average Loss: 1.9223 \n",
      "Accuracy: 0.291667 | Time: 00:00:00\n",
      "Epoch: 6 | Iter:    0 | Time: 00:00:00 | Train Loss: 0.6885 | Average Loss: 1.8281 \n",
      "Epoch: 6 | Iter:   20 | Time: 00:00:00 | Train Loss: 0.6060 | Average Loss: 1.6942 \n",
      "Accuracy: 0.229167 | Time: 00:00:00\n",
      "Epoch: 7 | Iter:    0 | Time: 00:00:00 | Train Loss: 0.5345 | Average Loss: 1.6123 \n",
      "Epoch: 7 | Iter:   20 | Time: 00:00:00 | Train Loss: 0.4328 | Average Loss: 1.5009 \n",
      "Accuracy: 0.250000 | Time: 00:00:00\n",
      "Epoch: 8 | Iter:    0 | Time: 00:00:00 | Train Loss: 0.4463 | Average Loss: 1.4314 \n",
      "Epoch: 8 | Iter:   20 | Time: 00:00:00 | Train Loss: 0.3205 | Average Loss: 1.3398 \n",
      "Accuracy: 0.208333 | Time: 00:00:00\n",
      "Epoch: 9 | Iter:    0 | Time: 00:00:00 | Train Loss: 0.3507 | Average Loss: 1.2825 \n",
      "Epoch: 9 | Iter:   20 | Time: 00:00:00 | Train Loss: 0.2342 | Average Loss: 1.2075 \n",
      "Accuracy: 0.166667 | Time: 00:00:00\n",
      "Epoch:10 | Iter:    0 | Time: 00:00:00 | Train Loss: 0.2532 | Average Loss: 1.1602 \n",
      "Epoch:10 | Iter:   20 | Time: 00:00:00 | Train Loss: 0.1839 | Average Loss: 1.0986 \n",
      "Accuracy: 0.229167 | Time: 00:00:00\n",
      "Epoch:11 | Iter:    0 | Time: 00:00:00 | Train Loss: 0.1756 | Average Loss: 1.0594 \n",
      "Epoch:11 | Iter:   20 | Time: 00:00:00 | Train Loss: 0.1638 | Average Loss: 1.0086 \n",
      "Accuracy: 0.229167 | Time: 00:00:00\n",
      "Epoch:12 | Iter:    0 | Time: 00:00:00 | Train Loss: 0.1316 | Average Loss: 0.9759 \n",
      "Epoch:12 | Iter:   20 | Time: 00:00:00 | Train Loss: 0.1103 | Average Loss: 0.9334 \n",
      "Accuracy: 0.208333 | Time: 00:00:00\n",
      "Epoch:13 | Iter:    0 | Time: 00:00:00 | Train Loss: 0.1129 | Average Loss: 0.9057 \n",
      "Epoch:13 | Iter:   20 | Time: 00:00:00 | Train Loss: 0.1703 | Average Loss: 0.8693 \n",
      "Accuracy: 0.208333 | Time: 00:00:00\n",
      "Epoch:14 | Iter:    0 | Time: 00:00:00 | Train Loss: 0.1005 | Average Loss: 0.8462 \n",
      "Epoch:14 | Iter:   20 | Time: 00:00:00 | Train Loss: 0.1345 | Average Loss: 0.8150 \n",
      "Accuracy: 0.229167 | Time: 00:00:00\n",
      "Epoch:15 | Iter:    0 | Time: 00:00:00 | Train Loss: 0.0769 | Average Loss: 0.7949 \n",
      "Epoch:15 | Iter:   20 | Time: 00:00:01 | Train Loss: 0.1712 | Average Loss: 0.7679 \n",
      "Accuracy: 0.187500 | Time: 00:00:00\n",
      "Epoch:16 | Iter:    0 | Time: 00:00:01 | Train Loss: 0.0655 | Average Loss: 0.7503 \n",
      "Epoch:16 | Iter:   20 | Time: 00:00:01 | Train Loss: 0.0454 | Average Loss: 0.7271 \n",
      "Accuracy: 0.104167 | Time: 00:00:00\n",
      "Epoch:17 | Iter:    0 | Time: 00:00:01 | Train Loss: 0.0635 | Average Loss: 0.7117 \n",
      "Epoch:17 | Iter:   20 | Time: 00:00:01 | Train Loss: 0.1618 | Average Loss: 0.6910 \n",
      "Accuracy: 0.145833 | Time: 00:00:00\n",
      "Epoch:18 | Iter:    0 | Time: 00:00:01 | Train Loss: 0.0874 | Average Loss: 0.6774 \n",
      "Epoch:18 | Iter:   20 | Time: 00:00:01 | Train Loss: 0.1484 | Average Loss: 0.6584 \n",
      "Accuracy: 0.229167 | Time: 00:00:00\n",
      "Epoch:19 | Iter:    0 | Time: 00:00:01 | Train Loss: 0.0244 | Average Loss: 0.6452 \n",
      "Epoch:19 | Iter:   20 | Time: 00:00:01 | Train Loss: 0.0700 | Average Loss: 0.6266 \n",
      "Accuracy: 0.229167 | Time: 00:00:00\n",
      "Epoch:20 | Iter:    0 | Time: 00:00:01 | Train Loss: 0.0264 | Average Loss: 0.6139 \n",
      "Epoch:20 | Iter:   20 | Time: 00:00:01 | Train Loss: 0.0509 | Average Loss: 0.5966 \n",
      "Accuracy: 0.229167 | Time: 00:00:00\n",
      "Epoch:21 | Iter:    0 | Time: 00:00:01 | Train Loss: 0.0534 | Average Loss: 0.5850 \n",
      "Epoch:21 | Iter:   20 | Time: 00:00:01 | Train Loss: 0.0162 | Average Loss: 0.5690 \n",
      "Accuracy: 0.229167 | Time: 00:00:00\n",
      "Epoch:22 | Iter:    0 | Time: 00:00:01 | Train Loss: 0.0252 | Average Loss: 0.5582 \n",
      "Epoch:22 | Iter:   20 | Time: 00:00:01 | Train Loss: 0.0341 | Average Loss: 0.5434 \n",
      "Accuracy: 0.250000 | Time: 00:00:00\n",
      "Epoch:23 | Iter:    0 | Time: 00:00:01 | Train Loss: 0.0156 | Average Loss: 0.5334 \n",
      "Epoch:23 | Iter:   20 | Time: 00:00:01 | Train Loss: 0.0099 | Average Loss: 0.5197 \n",
      "Accuracy: 0.270833 | Time: 00:00:00\n",
      "Epoch:24 | Iter:    0 | Time: 00:00:01 | Train Loss: 0.0128 | Average Loss: 0.5105 \n",
      "Epoch:24 | Iter:   20 | Time: 00:00:01 | Train Loss: 0.0087 | Average Loss: 0.4979 \n",
      "Accuracy: 0.270833 | Time: 00:00:00\n",
      "Epoch:25 | Iter:    0 | Time: 00:00:01 | Train Loss: 0.0114 | Average Loss: 0.4895 \n",
      "Epoch:25 | Iter:   20 | Time: 00:00:01 | Train Loss: 0.0079 | Average Loss: 0.4779 \n",
      "Accuracy: 0.250000 | Time: 00:00:00\n",
      "Epoch:26 | Iter:    0 | Time: 00:00:01 | Train Loss: 0.0103 | Average Loss: 0.4702 \n",
      "Epoch:26 | Iter:   20 | Time: 00:00:01 | Train Loss: 0.0073 | Average Loss: 0.4595 \n",
      "Accuracy: 0.250000 | Time: 00:00:00\n",
      "Epoch:27 | Iter:    0 | Time: 00:00:01 | Train Loss: 0.0094 | Average Loss: 0.4523 \n",
      "Epoch:27 | Iter:   20 | Time: 00:00:01 | Train Loss: 0.0067 | Average Loss: 0.4424 \n",
      "Accuracy: 0.250000 | Time: 00:00:00\n",
      "Epoch:28 | Iter:    0 | Time: 00:00:01 | Train Loss: 0.0086 | Average Loss: 0.4357 \n",
      "Epoch:28 | Iter:   20 | Time: 00:00:01 | Train Loss: 0.0063 | Average Loss: 0.4265 \n",
      "Accuracy: 0.250000 | Time: 00:00:00\n",
      "Epoch:29 | Iter:    0 | Time: 00:00:01 | Train Loss: 0.0079 | Average Loss: 0.4203 \n",
      "Epoch:29 | Iter:   20 | Time: 00:00:01 | Train Loss: 0.0059 | Average Loss: 0.4118 \n",
      "Accuracy: 0.250000 | Time: 00:00:00\n",
      "Epoch:30 | Iter:    0 | Time: 00:00:01 | Train Loss: 0.0073 | Average Loss: 0.4060 \n",
      "Epoch:30 | Iter:   20 | Time: 00:00:01 | Train Loss: 0.0055 | Average Loss: 0.3980 \n",
      "Accuracy: 0.250000 | Time: 00:00:00\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------------\n",
    "#       Start Training & Evaluation\n",
    "#--------------------------------------------------\n",
    "\n",
    "print(\"Torch version:\",torch.__version__)\n",
    "\n",
    "print(\"Is CUDA enabled?\",torch.cuda.is_available())\n",
    "net = TNet()\n",
    "train_option = {}\n",
    "train_option['lr'] = 0.001\n",
    "train_option['epoch'] = 30\n",
    "train_option['device'] = 'gpu'\n",
    "trainModel(net, trainloader_small, train_option, testloader_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HaRIpcUeAYSl"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "### Boost the accuracy\n",
    "\n",
    "**Data Augmentation**: We don't have enough training data, augment the training data is needed.\n",
    "If you rotate an image of a scene, it almost never changes categories. A kitchen doesn't become a forest when rotated. We can synthetically increase our amount of training data by rotating training images during the learning process.\n",
    "\n",
    "\n",
    "After we implement rotation, we notice that our training error doesn't drop as quickly. That means the network isn't overfitting to the original training images as much (because it sees more training images now, although they're not as good as truly independent samples). Because the training and test errors fall more slowly, we may need more training epochs.\n",
    "\n",
    "**Data Normalization**: The images aren't zero-centered. One simple trick which can help a lot is to subtract the mean from every image. It would arguably be more proper to only compute the mean from the training images (since the test/validation images should be strictly held out) but it won't make much of a difference.\n",
    "\n",
    "**Network Regularization**: Add a dropout layer. The training error can decrease to zero while the val top1 error hovers at 40% to 50%. The network has learned weights which can perfectly recognize the training data, but those weights don't generalize to held out test data. The best regularization would be more training data but we don't have that. Instead we will use dropout regularization.\n",
    "\n",
    "What does dropout regularization do? It randomly turns off network connections at training time to fight overfitting. This prevents a unit in one layer from relying too strongly on a single unit in the previous layer. Dropout regularization can be interpreted as simultaneously training many \"thinned\" versions of your network. At test, all connections are restored which is analogous to taking an average prediction over all of the \"thinned\" networks. You can see a more complete discussion of dropout regularization in this [paper](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf).\n",
    "\n",
    "The dropout layer has only one free parameter — the dropout rate — the proportion of connections that are randomly deleted. The default of 0.5 should be fine. We insert a dropout layer between the convolutional layers. In particular, we inserted it directly before the last convolutional layer. The training accuracy should increase much more slowly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "3S9dgoEWFCcI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images from class: 0\n",
      "Loading images from class: 1\n",
      "Loading images from class: 2\n",
      "Loading images from class: 3\n",
      "Loading images from class: 4\n",
      "Loading images from class: 5\n",
      "Loading images from class: 6\n",
      "Loading images from class: 7\n",
      "Loading images from class: 8\n",
      "Loading images from class: 9\n",
      "Loading images from class: 10\n",
      "Loading images from class: 11\n",
      "Loading images from class: 12\n",
      "Loading images from class: 13\n",
      "Loading images from class: 14\n",
      "Loading images from class: 15\n",
      "Loading images from class: 16\n",
      "Loading images from class: 17\n",
      "Loading images from class: 18\n",
      "Loading images from class: 19\n",
      "Finish loading 279 minibatches (batch_size=16) of training samples.\n",
      "Loading images from class: 0\n",
      "Loading images from class: 1\n",
      "Loading images from class: 2\n",
      "Loading images from class: 3\n",
      "Loading images from class: 4\n",
      "Loading images from class: 5\n",
      "Loading images from class: 6\n",
      "Loading images from class: 7\n",
      "Loading images from class: 8\n",
      "Loading images from class: 9\n",
      "Loading images from class: 10\n",
      "Loading images from class: 11\n",
      "Loading images from class: 12\n",
      "Loading images from class: 13\n",
      "Loading images from class: 14\n",
      "Loading images from class: 15\n",
      "Loading images from class: 16\n",
      "Loading images from class: 17\n",
      "Loading images from class: 18\n",
      "Loading images from class: 19\n",
      "Finish loading 3 minibatches (batch_size=16) of testing samples.\n"
     ]
    }
   ],
   "source": [
    "# load data into size (64, 64)\n",
    "img_size = (64, 64)\n",
    "batch_size = 16 # training sample number per batch\n",
    "\n",
    "# load training dataset\n",
    "trainloader_small = list(load_dataset('./data/train/', img_size, batch_size=batch_size, shuffle=True, augment=True, zero_centered=True))\n",
    "train_num = len(trainloader_small)\n",
    "print(\"Finish loading %d minibatches (batch_size=%d) of training samples.\" % (train_num, batch_size))\n",
    "\n",
    "# load testing dataset\n",
    "testloader_small = list(load_dataset('./data/test/', img_size, num_per_class=50, batch_size=batch_size, zero_centered=True))\n",
    "test_num = len(testloader_small)\n",
    "print(\"Finish loading %d minibatches (batch_size=%d) of testing samples.\" % (test_num, batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clXaGdwaoIL7"
   },
   "outputs": [],
   "source": [
    "#--------------------------------------------------\n",
    "#       Define Network Architecture\n",
    "#--------------------------------------------------\n",
    "class TNet(nn.Module):\n",
    "    def __init__(self):\n",
    "      super(TNet,self).__init__()\n",
    "      # self.inners = nn.Sequential(\n",
    "      #       nn.Conv2d(1, 16, 3),\n",
    "      #       nn.ReLU(),\n",
    "      #       nn.MaxPool2d(2, stride=2),\n",
    "      #       nn.Conv2d(16, 32, 3),\n",
    "      #       nn.ReLU(),\n",
    "      #       nn.MaxPool2d(4, stride=2),\n",
    "      #       nn.Dropout(0.5),\n",
    "      #       nn.Conv2d(32, 64, 3),\n",
    "      #       nn.ReLU(),\n",
    "      #       nn.MaxPool2d(2, stride=2),\n",
    "      # )\n",
    "          \n",
    "      # self.classifier = nn.Sequential(\n",
    "      #       nn.Linear(1600, 400),\n",
    "      #       nn.ReLU(),\n",
    "      #       nn.Linear(400, 20)\n",
    "      # )\n",
    "      self.inners = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3),         \n",
    "            nn.ReLU(),         \n",
    "            nn.MaxPool2d(2, stride=2),         \n",
    "            nn.Conv2d(16, 32, 3),          \n",
    "            nn.ReLU(),          \n",
    "            nn.MaxPool2d(4, stride=2),      \n",
    "            nn.Dropout(0.5),      \n",
    "            nn.Conv2d(32, 16, 3),           \n",
    "            nn.ReLU(),    \n",
    "            nn.MaxPool2d(2, stride=2), \n",
    "      )\n",
    "          \n",
    "      self.classifier = nn.Sequential(\n",
    "        nn.Linear(400, 20)\n",
    "      )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "      x = self.inners(x)  \n",
    "      x = torch.flatten(x, 1)\n",
    "      x = self.classifier(x)\n",
    "      return x \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "-Tj73gVp8gN7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Iter:    0 | Time: 00:00:00 | Train Loss: 2.9543 | Average Loss: 2.9543 \n",
      "Epoch: 1 | Iter:   20 | Time: 00:00:00 | Train Loss: 2.9677 | Average Loss: 3.0037 \n",
      "Epoch: 1 | Iter:   40 | Time: 00:00:00 | Train Loss: 2.8118 | Average Loss: 2.9787 \n",
      "Epoch: 1 | Iter:   60 | Time: 00:00:00 | Train Loss: 2.9134 | Average Loss: 2.9691 \n",
      "Epoch: 1 | Iter:   80 | Time: 00:00:00 | Train Loss: 3.0035 | Average Loss: 2.9580 \n",
      "Epoch: 1 | Iter:  100 | Time: 00:00:00 | Train Loss: 2.7563 | Average Loss: 2.9295 \n",
      "Epoch: 1 | Iter:  120 | Time: 00:00:00 | Train Loss: 2.5360 | Average Loss: 2.9167 \n",
      "Epoch: 1 | Iter:  140 | Time: 00:00:00 | Train Loss: 2.3795 | Average Loss: 2.8877 \n",
      "Epoch: 1 | Iter:  160 | Time: 00:00:00 | Train Loss: 2.7442 | Average Loss: 2.8558 \n",
      "Epoch: 1 | Iter:  180 | Time: 00:00:00 | Train Loss: 2.4627 | Average Loss: 2.8307 \n",
      "Epoch: 1 | Iter:  200 | Time: 00:00:00 | Train Loss: 2.5640 | Average Loss: 2.8096 \n",
      "Epoch: 1 | Iter:  220 | Time: 00:00:00 | Train Loss: 2.7296 | Average Loss: 2.7885 \n",
      "Epoch: 1 | Iter:  240 | Time: 00:00:00 | Train Loss: 2.2973 | Average Loss: 2.7708 \n",
      "Epoch: 1 | Iter:  260 | Time: 00:00:00 | Train Loss: 2.1500 | Average Loss: 2.7483 \n",
      "Accuracy: 0.187500 | Time: 00:00:00\n",
      "Epoch: 2 | Iter:    0 | Time: 00:00:00 | Train Loss: 2.4965 | Average Loss: 2.7322 \n",
      "Epoch: 2 | Iter:   20 | Time: 00:00:00 | Train Loss: 2.3136 | Average Loss: 2.7165 \n",
      "Epoch: 2 | Iter:   40 | Time: 00:00:00 | Train Loss: 2.3282 | Average Loss: 2.6972 \n",
      "Epoch: 2 | Iter:   60 | Time: 00:00:00 | Train Loss: 2.4550 | Average Loss: 2.6818 \n",
      "Epoch: 2 | Iter:   80 | Time: 00:00:00 | Train Loss: 2.5205 | Average Loss: 2.6684 \n",
      "Epoch: 2 | Iter:  100 | Time: 00:00:00 | Train Loss: 2.3460 | Average Loss: 2.6499 \n",
      "Epoch: 2 | Iter:  120 | Time: 00:00:00 | Train Loss: 1.8376 | Average Loss: 2.6363 \n",
      "Epoch: 2 | Iter:  140 | Time: 00:00:00 | Train Loss: 2.2372 | Average Loss: 2.6227 \n",
      "Epoch: 2 | Iter:  160 | Time: 00:00:00 | Train Loss: 2.6163 | Average Loss: 2.6112 \n",
      "Epoch: 2 | Iter:  180 | Time: 00:00:01 | Train Loss: 2.1186 | Average Loss: 2.6016 \n",
      "Epoch: 2 | Iter:  200 | Time: 00:00:01 | Train Loss: 2.2672 | Average Loss: 2.5908 \n",
      "Epoch: 2 | Iter:  220 | Time: 00:00:01 | Train Loss: 2.4339 | Average Loss: 2.5788 \n",
      "Epoch: 2 | Iter:  240 | Time: 00:00:01 | Train Loss: 2.1136 | Average Loss: 2.5688 \n",
      "Epoch: 2 | Iter:  260 | Time: 00:00:01 | Train Loss: 1.7010 | Average Loss: 2.5561 \n",
      "Accuracy: 0.187500 | Time: 00:00:00\n",
      "Epoch: 3 | Iter:    0 | Time: 00:00:01 | Train Loss: 2.2229 | Average Loss: 2.5474 \n",
      "Epoch: 3 | Iter:   20 | Time: 00:00:01 | Train Loss: 2.1850 | Average Loss: 2.5372 \n",
      "Epoch: 3 | Iter:   40 | Time: 00:00:01 | Train Loss: 2.0202 | Average Loss: 2.5270 \n",
      "Epoch: 3 | Iter:   60 | Time: 00:00:01 | Train Loss: 2.5137 | Average Loss: 2.5192 \n",
      "Epoch: 3 | Iter:   80 | Time: 00:00:01 | Train Loss: 2.1747 | Average Loss: 2.5102 \n",
      "Epoch: 3 | Iter:  100 | Time: 00:00:01 | Train Loss: 2.0167 | Average Loss: 2.4987 \n",
      "Epoch: 3 | Iter:  120 | Time: 00:00:01 | Train Loss: 1.6109 | Average Loss: 2.4895 \n",
      "Epoch: 3 | Iter:  140 | Time: 00:00:01 | Train Loss: 2.1254 | Average Loss: 2.4802 \n",
      "Epoch: 3 | Iter:  160 | Time: 00:00:01 | Train Loss: 2.5332 | Average Loss: 2.4735 \n",
      "Epoch: 3 | Iter:  180 | Time: 00:00:01 | Train Loss: 1.9322 | Average Loss: 2.4669 \n",
      "Epoch: 3 | Iter:  200 | Time: 00:00:01 | Train Loss: 2.2506 | Average Loss: 2.4583 \n",
      "Epoch: 3 | Iter:  220 | Time: 00:00:01 | Train Loss: 2.1678 | Average Loss: 2.4498 \n",
      "Epoch: 3 | Iter:  240 | Time: 00:00:01 | Train Loss: 1.8453 | Average Loss: 2.4413 \n",
      "Epoch: 3 | Iter:  260 | Time: 00:00:01 | Train Loss: 1.4446 | Average Loss: 2.4312 \n",
      "Accuracy: 0.250000 | Time: 00:00:00\n",
      "Epoch: 4 | Iter:    0 | Time: 00:00:01 | Train Loss: 1.9238 | Average Loss: 2.4246 \n",
      "Epoch: 4 | Iter:   20 | Time: 00:00:01 | Train Loss: 2.1443 | Average Loss: 2.4165 \n",
      "Epoch: 4 | Iter:   40 | Time: 00:00:01 | Train Loss: 1.8599 | Average Loss: 2.4097 \n",
      "Epoch: 4 | Iter:   60 | Time: 00:00:01 | Train Loss: 2.5765 | Average Loss: 2.4047 \n",
      "Epoch: 4 | Iter:   80 | Time: 00:00:01 | Train Loss: 1.7726 | Average Loss: 2.3972 \n",
      "Epoch: 4 | Iter:  100 | Time: 00:00:01 | Train Loss: 2.0172 | Average Loss: 2.3885 \n",
      "Epoch: 4 | Iter:  120 | Time: 00:00:02 | Train Loss: 1.3543 | Average Loss: 2.3805 \n",
      "Epoch: 4 | Iter:  140 | Time: 00:00:02 | Train Loss: 2.0566 | Average Loss: 2.3733 \n",
      "Epoch: 4 | Iter:  160 | Time: 00:00:02 | Train Loss: 2.3551 | Average Loss: 2.3671 \n",
      "Epoch: 4 | Iter:  180 | Time: 00:00:02 | Train Loss: 1.8589 | Average Loss: 2.3608 \n",
      "Epoch: 4 | Iter:  200 | Time: 00:00:02 | Train Loss: 2.0947 | Average Loss: 2.3536 \n",
      "Epoch: 4 | Iter:  220 | Time: 00:00:02 | Train Loss: 1.8458 | Average Loss: 2.3461 \n",
      "Epoch: 4 | Iter:  240 | Time: 00:00:02 | Train Loss: 1.7368 | Average Loss: 2.3392 \n",
      "Epoch: 4 | Iter:  260 | Time: 00:00:02 | Train Loss: 1.3925 | Average Loss: 2.3312 \n",
      "Accuracy: 0.291667 | Time: 00:00:00\n",
      "Epoch: 5 | Iter:    0 | Time: 00:00:02 | Train Loss: 1.5519 | Average Loss: 2.3264 \n",
      "Epoch: 5 | Iter:   20 | Time: 00:00:02 | Train Loss: 2.2220 | Average Loss: 2.3199 \n",
      "Epoch: 5 | Iter:   40 | Time: 00:00:02 | Train Loss: 1.6638 | Average Loss: 2.3134 \n",
      "Epoch: 5 | Iter:   60 | Time: 00:00:02 | Train Loss: 2.3718 | Average Loss: 2.3090 \n",
      "Epoch: 5 | Iter:   80 | Time: 00:00:02 | Train Loss: 1.6912 | Average Loss: 2.3033 \n",
      "Epoch: 5 | Iter:  100 | Time: 00:00:02 | Train Loss: 1.9033 | Average Loss: 2.2963 \n",
      "Epoch: 5 | Iter:  120 | Time: 00:00:02 | Train Loss: 1.2677 | Average Loss: 2.2906 \n",
      "Epoch: 5 | Iter:  140 | Time: 00:00:02 | Train Loss: 2.0563 | Average Loss: 2.2844 \n",
      "Epoch: 5 | Iter:  160 | Time: 00:00:02 | Train Loss: 2.2312 | Average Loss: 2.2785 \n",
      "Epoch: 5 | Iter:  180 | Time: 00:00:02 | Train Loss: 1.5886 | Average Loss: 2.2731 \n",
      "Epoch: 5 | Iter:  200 | Time: 00:00:02 | Train Loss: 2.1072 | Average Loss: 2.2676 \n",
      "Epoch: 5 | Iter:  220 | Time: 00:00:02 | Train Loss: 1.6989 | Average Loss: 2.2619 \n",
      "Epoch: 5 | Iter:  240 | Time: 00:00:02 | Train Loss: 1.4813 | Average Loss: 2.2557 \n",
      "Epoch: 5 | Iter:  260 | Time: 00:00:02 | Train Loss: 1.2047 | Average Loss: 2.2489 \n",
      "Accuracy: 0.354167 | Time: 00:00:00\n",
      "Epoch: 6 | Iter:    0 | Time: 00:00:02 | Train Loss: 1.6028 | Average Loss: 2.2449 \n",
      "Epoch: 6 | Iter:   20 | Time: 00:00:02 | Train Loss: 2.1094 | Average Loss: 2.2392 \n",
      "Epoch: 6 | Iter:   40 | Time: 00:00:03 | Train Loss: 1.6702 | Average Loss: 2.2337 \n",
      "Epoch: 6 | Iter:   60 | Time: 00:00:03 | Train Loss: 2.3555 | Average Loss: 2.2290 \n",
      "Epoch: 6 | Iter:   80 | Time: 00:00:03 | Train Loss: 1.5349 | Average Loss: 2.2242 \n",
      "Epoch: 6 | Iter:  100 | Time: 00:00:03 | Train Loss: 1.8277 | Average Loss: 2.2183 \n",
      "Epoch: 6 | Iter:  120 | Time: 00:00:03 | Train Loss: 1.2201 | Average Loss: 2.2131 \n",
      "Epoch: 6 | Iter:  140 | Time: 00:00:03 | Train Loss: 2.1197 | Average Loss: 2.2080 \n",
      "Epoch: 6 | Iter:  160 | Time: 00:00:03 | Train Loss: 1.9838 | Average Loss: 2.2036 \n",
      "Epoch: 6 | Iter:  180 | Time: 00:00:03 | Train Loss: 1.4703 | Average Loss: 2.1983 \n",
      "Epoch: 6 | Iter:  200 | Time: 00:00:03 | Train Loss: 1.8451 | Average Loss: 2.1927 \n",
      "Epoch: 6 | Iter:  220 | Time: 00:00:03 | Train Loss: 1.4609 | Average Loss: 2.1873 \n",
      "Epoch: 6 | Iter:  240 | Time: 00:00:03 | Train Loss: 1.4764 | Average Loss: 2.1821 \n",
      "Epoch: 6 | Iter:  260 | Time: 00:00:03 | Train Loss: 1.1657 | Average Loss: 2.1761 \n",
      "Accuracy: 0.333333 | Time: 00:00:00\n",
      "Epoch: 7 | Iter:    0 | Time: 00:00:03 | Train Loss: 1.6062 | Average Loss: 2.1729 \n",
      "Epoch: 7 | Iter:   20 | Time: 00:00:03 | Train Loss: 2.0547 | Average Loss: 2.1685 \n",
      "Epoch: 7 | Iter:   40 | Time: 00:00:03 | Train Loss: 1.5789 | Average Loss: 2.1642 \n",
      "Epoch: 7 | Iter:   60 | Time: 00:00:03 | Train Loss: 2.1548 | Average Loss: 2.1597 \n",
      "Epoch: 7 | Iter:   80 | Time: 00:00:03 | Train Loss: 1.2953 | Average Loss: 2.1551 \n",
      "Epoch: 7 | Iter:  100 | Time: 00:00:03 | Train Loss: 1.9743 | Average Loss: 2.1501 \n",
      "Epoch: 7 | Iter:  120 | Time: 00:00:03 | Train Loss: 1.2018 | Average Loss: 2.1459 \n",
      "Epoch: 7 | Iter:  140 | Time: 00:00:03 | Train Loss: 2.0948 | Average Loss: 2.1413 \n",
      "Epoch: 7 | Iter:  160 | Time: 00:00:03 | Train Loss: 1.9414 | Average Loss: 2.1371 \n",
      "Epoch: 7 | Iter:  180 | Time: 00:00:03 | Train Loss: 1.3980 | Average Loss: 2.1320 \n",
      "Epoch: 7 | Iter:  200 | Time: 00:00:03 | Train Loss: 1.7794 | Average Loss: 2.1268 \n",
      "Epoch: 7 | Iter:  220 | Time: 00:00:03 | Train Loss: 1.2467 | Average Loss: 2.1216 \n",
      "Epoch: 7 | Iter:  240 | Time: 00:00:03 | Train Loss: 1.2193 | Average Loss: 2.1170 \n",
      "Epoch: 7 | Iter:  260 | Time: 00:00:04 | Train Loss: 1.0862 | Average Loss: 2.1122 \n",
      "Accuracy: 0.354167 | Time: 00:00:00\n",
      "Epoch: 8 | Iter:    0 | Time: 00:00:04 | Train Loss: 1.4010 | Average Loss: 2.1087 \n",
      "Epoch: 8 | Iter:   20 | Time: 00:00:04 | Train Loss: 1.8468 | Average Loss: 2.1050 \n",
      "Epoch: 8 | Iter:   40 | Time: 00:00:04 | Train Loss: 1.5309 | Average Loss: 2.1007 \n",
      "Epoch: 8 | Iter:   60 | Time: 00:00:04 | Train Loss: 2.2653 | Average Loss: 2.0963 \n",
      "Epoch: 8 | Iter:   80 | Time: 00:00:04 | Train Loss: 1.1783 | Average Loss: 2.0925 \n",
      "Epoch: 8 | Iter:  100 | Time: 00:00:04 | Train Loss: 1.5478 | Average Loss: 2.0881 \n",
      "Epoch: 8 | Iter:  120 | Time: 00:00:04 | Train Loss: 1.1628 | Average Loss: 2.0836 \n",
      "Epoch: 8 | Iter:  140 | Time: 00:00:04 | Train Loss: 1.6005 | Average Loss: 2.0795 \n",
      "Epoch: 8 | Iter:  160 | Time: 00:00:04 | Train Loss: 1.7604 | Average Loss: 2.0760 \n",
      "Epoch: 8 | Iter:  180 | Time: 00:00:04 | Train Loss: 1.3086 | Average Loss: 2.0716 \n",
      "Epoch: 8 | Iter:  200 | Time: 00:00:04 | Train Loss: 1.4308 | Average Loss: 2.0676 \n",
      "Epoch: 8 | Iter:  220 | Time: 00:00:04 | Train Loss: 1.4500 | Average Loss: 2.0637 \n",
      "Epoch: 8 | Iter:  240 | Time: 00:00:04 | Train Loss: 1.3098 | Average Loss: 2.0585 \n",
      "Epoch: 8 | Iter:  260 | Time: 00:00:04 | Train Loss: 1.1104 | Average Loss: 2.0539 \n",
      "Accuracy: 0.416667 | Time: 00:00:00\n",
      "Epoch: 9 | Iter:    0 | Time: 00:00:04 | Train Loss: 1.3976 | Average Loss: 2.0513 \n",
      "Epoch: 9 | Iter:   20 | Time: 00:00:04 | Train Loss: 2.2569 | Average Loss: 2.0478 \n",
      "Epoch: 9 | Iter:   40 | Time: 00:00:04 | Train Loss: 1.2141 | Average Loss: 2.0434 \n",
      "Epoch: 9 | Iter:   60 | Time: 00:00:04 | Train Loss: 1.8739 | Average Loss: 2.0392 \n",
      "Epoch: 9 | Iter:   80 | Time: 00:00:04 | Train Loss: 1.0599 | Average Loss: 2.0349 \n",
      "Epoch: 9 | Iter:  100 | Time: 00:00:04 | Train Loss: 2.0098 | Average Loss: 2.0313 \n",
      "Epoch: 9 | Iter:  120 | Time: 00:00:04 | Train Loss: 1.2721 | Average Loss: 2.0276 \n",
      "Epoch: 9 | Iter:  140 | Time: 00:00:04 | Train Loss: 1.9679 | Average Loss: 2.0239 \n",
      "Epoch: 9 | Iter:  160 | Time: 00:00:05 | Train Loss: 1.7546 | Average Loss: 2.0207 \n",
      "Epoch: 9 | Iter:  180 | Time: 00:00:05 | Train Loss: 1.2832 | Average Loss: 2.0163 \n",
      "Epoch: 9 | Iter:  200 | Time: 00:00:05 | Train Loss: 1.7139 | Average Loss: 2.0123 \n",
      "Epoch: 9 | Iter:  220 | Time: 00:00:05 | Train Loss: 1.3639 | Average Loss: 2.0085 \n",
      "Epoch: 9 | Iter:  240 | Time: 00:00:05 | Train Loss: 0.9932 | Average Loss: 2.0047 \n",
      "Epoch: 9 | Iter:  260 | Time: 00:00:05 | Train Loss: 1.0977 | Average Loss: 2.0004 \n",
      "Accuracy: 0.333333 | Time: 00:00:00\n",
      "Epoch:10 | Iter:    0 | Time: 00:00:05 | Train Loss: 1.4233 | Average Loss: 1.9980 \n",
      "Epoch:10 | Iter:   20 | Time: 00:00:05 | Train Loss: 1.9628 | Average Loss: 1.9947 \n",
      "Epoch:10 | Iter:   40 | Time: 00:00:05 | Train Loss: 1.2496 | Average Loss: 1.9905 \n",
      "Epoch:10 | Iter:   60 | Time: 00:00:05 | Train Loss: 1.8169 | Average Loss: 1.9866 \n",
      "Epoch:10 | Iter:   80 | Time: 00:00:05 | Train Loss: 1.0276 | Average Loss: 1.9831 \n",
      "Epoch:10 | Iter:  100 | Time: 00:00:05 | Train Loss: 1.6770 | Average Loss: 1.9792 \n",
      "Epoch:10 | Iter:  120 | Time: 00:00:05 | Train Loss: 1.1684 | Average Loss: 1.9756 \n",
      "Epoch:10 | Iter:  140 | Time: 00:00:05 | Train Loss: 2.2892 | Average Loss: 1.9726 \n",
      "Epoch:10 | Iter:  160 | Time: 00:00:05 | Train Loss: 1.6923 | Average Loss: 1.9693 \n",
      "Epoch:10 | Iter:  180 | Time: 00:00:05 | Train Loss: 1.5165 | Average Loss: 1.9662 \n",
      "Epoch:10 | Iter:  200 | Time: 00:00:05 | Train Loss: 1.6835 | Average Loss: 1.9630 \n",
      "Epoch:10 | Iter:  220 | Time: 00:00:05 | Train Loss: 1.0800 | Average Loss: 1.9596 \n",
      "Epoch:10 | Iter:  240 | Time: 00:00:05 | Train Loss: 1.1077 | Average Loss: 1.9561 \n",
      "Epoch:10 | Iter:  260 | Time: 00:00:05 | Train Loss: 1.0562 | Average Loss: 1.9526 \n",
      "Accuracy: 0.395833 | Time: 00:00:00\n",
      "Epoch:11 | Iter:    0 | Time: 00:00:05 | Train Loss: 1.4093 | Average Loss: 1.9501 \n",
      "Epoch:11 | Iter:   20 | Time: 00:00:05 | Train Loss: 1.9690 | Average Loss: 1.9473 \n",
      "Epoch:11 | Iter:   40 | Time: 00:00:05 | Train Loss: 1.4282 | Average Loss: 1.9439 \n",
      "Epoch:11 | Iter:   60 | Time: 00:00:05 | Train Loss: 1.9310 | Average Loss: 1.9399 \n",
      "Epoch:11 | Iter:   80 | Time: 00:00:05 | Train Loss: 0.9718 | Average Loss: 1.9366 \n",
      "Epoch:11 | Iter:  100 | Time: 00:00:06 | Train Loss: 1.5462 | Average Loss: 1.9333 \n",
      "Epoch:11 | Iter:  120 | Time: 00:00:06 | Train Loss: 0.9706 | Average Loss: 1.9302 \n",
      "Epoch:11 | Iter:  140 | Time: 00:00:06 | Train Loss: 2.1200 | Average Loss: 1.9274 \n",
      "Epoch:11 | Iter:  160 | Time: 00:00:06 | Train Loss: 1.6182 | Average Loss: 1.9246 \n",
      "Epoch:11 | Iter:  180 | Time: 00:00:06 | Train Loss: 1.5041 | Average Loss: 1.9212 \n",
      "Epoch:11 | Iter:  200 | Time: 00:00:06 | Train Loss: 1.5282 | Average Loss: 1.9178 \n",
      "Epoch:11 | Iter:  220 | Time: 00:00:06 | Train Loss: 1.3191 | Average Loss: 1.9146 \n",
      "Epoch:11 | Iter:  240 | Time: 00:00:06 | Train Loss: 1.0899 | Average Loss: 1.9112 \n",
      "Epoch:11 | Iter:  260 | Time: 00:00:06 | Train Loss: 1.0651 | Average Loss: 1.9079 \n",
      "Accuracy: 0.354167 | Time: 00:00:00\n",
      "Epoch:12 | Iter:    0 | Time: 00:00:06 | Train Loss: 1.2089 | Average Loss: 1.9054 \n",
      "Epoch:12 | Iter:   20 | Time: 00:00:06 | Train Loss: 1.7944 | Average Loss: 1.9021 \n",
      "Epoch:12 | Iter:   40 | Time: 00:00:06 | Train Loss: 1.6031 | Average Loss: 1.8991 \n",
      "Epoch:12 | Iter:   60 | Time: 00:00:06 | Train Loss: 1.6095 | Average Loss: 1.8956 \n",
      "Epoch:12 | Iter:   80 | Time: 00:00:06 | Train Loss: 1.2297 | Average Loss: 1.8929 \n",
      "Epoch:12 | Iter:  100 | Time: 00:00:06 | Train Loss: 1.3421 | Average Loss: 1.8894 \n",
      "Epoch:12 | Iter:  120 | Time: 00:00:06 | Train Loss: 1.1049 | Average Loss: 1.8864 \n",
      "Epoch:12 | Iter:  140 | Time: 00:00:06 | Train Loss: 1.4778 | Average Loss: 1.8839 \n",
      "Epoch:12 | Iter:  160 | Time: 00:00:06 | Train Loss: 1.2947 | Average Loss: 1.8813 \n",
      "Epoch:12 | Iter:  180 | Time: 00:00:06 | Train Loss: 1.2920 | Average Loss: 1.8780 \n",
      "Epoch:12 | Iter:  200 | Time: 00:00:06 | Train Loss: 1.4893 | Average Loss: 1.8745 \n",
      "Epoch:12 | Iter:  220 | Time: 00:00:06 | Train Loss: 1.3433 | Average Loss: 1.8715 \n",
      "Epoch:12 | Iter:  240 | Time: 00:00:06 | Train Loss: 0.9431 | Average Loss: 1.8687 \n",
      "Epoch:12 | Iter:  260 | Time: 00:00:06 | Train Loss: 0.9840 | Average Loss: 1.8651 \n",
      "Accuracy: 0.375000 | Time: 00:00:00\n",
      "Epoch:13 | Iter:    0 | Time: 00:00:07 | Train Loss: 1.0674 | Average Loss: 1.8626 \n",
      "Epoch:13 | Iter:   20 | Time: 00:00:07 | Train Loss: 1.5785 | Average Loss: 1.8595 \n",
      "Epoch:13 | Iter:   40 | Time: 00:00:07 | Train Loss: 1.2512 | Average Loss: 1.8564 \n",
      "Epoch:13 | Iter:   60 | Time: 00:00:07 | Train Loss: 1.5725 | Average Loss: 1.8527 \n",
      "Epoch:13 | Iter:   80 | Time: 00:00:07 | Train Loss: 0.9426 | Average Loss: 1.8502 \n",
      "Epoch:13 | Iter:  100 | Time: 00:00:07 | Train Loss: 1.3506 | Average Loss: 1.8472 \n",
      "Epoch:13 | Iter:  120 | Time: 00:00:07 | Train Loss: 0.9533 | Average Loss: 1.8442 \n",
      "Epoch:13 | Iter:  140 | Time: 00:00:07 | Train Loss: 1.6412 | Average Loss: 1.8419 \n",
      "Epoch:13 | Iter:  160 | Time: 00:00:07 | Train Loss: 1.5160 | Average Loss: 1.8395 \n",
      "Epoch:13 | Iter:  180 | Time: 00:00:07 | Train Loss: 1.2539 | Average Loss: 1.8366 \n",
      "Epoch:13 | Iter:  200 | Time: 00:00:07 | Train Loss: 1.7964 | Average Loss: 1.8342 \n",
      "Epoch:13 | Iter:  220 | Time: 00:00:07 | Train Loss: 0.8958 | Average Loss: 1.8312 \n",
      "Epoch:13 | Iter:  240 | Time: 00:00:07 | Train Loss: 0.8172 | Average Loss: 1.8282 \n",
      "Epoch:13 | Iter:  260 | Time: 00:00:07 | Train Loss: 1.0180 | Average Loss: 1.8252 \n",
      "Accuracy: 0.416667 | Time: 00:00:00\n",
      "Epoch:14 | Iter:    0 | Time: 00:00:07 | Train Loss: 1.2098 | Average Loss: 1.8231 \n",
      "Epoch:14 | Iter:   20 | Time: 00:00:07 | Train Loss: 1.7089 | Average Loss: 1.8204 \n",
      "Epoch:14 | Iter:   40 | Time: 00:00:07 | Train Loss: 1.5475 | Average Loss: 1.8178 \n",
      "Epoch:14 | Iter:   60 | Time: 00:00:07 | Train Loss: 1.6753 | Average Loss: 1.8146 \n",
      "Epoch:14 | Iter:   80 | Time: 00:00:07 | Train Loss: 0.8030 | Average Loss: 1.8122 \n",
      "Epoch:14 | Iter:  100 | Time: 00:00:07 | Train Loss: 0.7981 | Average Loss: 1.8092 \n",
      "Epoch:14 | Iter:  120 | Time: 00:00:07 | Train Loss: 0.6945 | Average Loss: 1.8065 \n",
      "Epoch:14 | Iter:  140 | Time: 00:00:07 | Train Loss: 2.0434 | Average Loss: 1.8042 \n",
      "Epoch:14 | Iter:  160 | Time: 00:00:07 | Train Loss: 1.4122 | Average Loss: 1.8019 \n",
      "Epoch:14 | Iter:  180 | Time: 00:00:07 | Train Loss: 1.2555 | Average Loss: 1.7990 \n",
      "Epoch:14 | Iter:  200 | Time: 00:00:07 | Train Loss: 1.4272 | Average Loss: 1.7959 \n",
      "Epoch:14 | Iter:  220 | Time: 00:00:07 | Train Loss: 1.1072 | Average Loss: 1.7928 \n",
      "Epoch:14 | Iter:  240 | Time: 00:00:08 | Train Loss: 0.9603 | Average Loss: 1.7899 \n",
      "Epoch:14 | Iter:  260 | Time: 00:00:08 | Train Loss: 1.1055 | Average Loss: 1.7863 \n",
      "Accuracy: 0.437500 | Time: 00:00:00\n",
      "Epoch:15 | Iter:    0 | Time: 00:00:08 | Train Loss: 1.3106 | Average Loss: 1.7842 \n",
      "Epoch:15 | Iter:   20 | Time: 00:00:08 | Train Loss: 1.9788 | Average Loss: 1.7819 \n",
      "Epoch:15 | Iter:   40 | Time: 00:00:08 | Train Loss: 1.5130 | Average Loss: 1.7791 \n",
      "Epoch:15 | Iter:   60 | Time: 00:00:08 | Train Loss: 1.2734 | Average Loss: 1.7760 \n",
      "Epoch:15 | Iter:   80 | Time: 00:00:08 | Train Loss: 0.7928 | Average Loss: 1.7736 \n",
      "Epoch:15 | Iter:  100 | Time: 00:00:08 | Train Loss: 1.0376 | Average Loss: 1.7711 \n",
      "Epoch:15 | Iter:  120 | Time: 00:00:08 | Train Loss: 0.8697 | Average Loss: 1.7687 \n",
      "Epoch:15 | Iter:  140 | Time: 00:00:08 | Train Loss: 1.3819 | Average Loss: 1.7663 \n",
      "Epoch:15 | Iter:  160 | Time: 00:00:08 | Train Loss: 1.0084 | Average Loss: 1.7639 \n",
      "Epoch:15 | Iter:  180 | Time: 00:00:08 | Train Loss: 1.0680 | Average Loss: 1.7610 \n",
      "Epoch:15 | Iter:  200 | Time: 00:00:08 | Train Loss: 1.3890 | Average Loss: 1.7581 \n",
      "Epoch:15 | Iter:  220 | Time: 00:00:08 | Train Loss: 1.1235 | Average Loss: 1.7552 \n",
      "Epoch:15 | Iter:  240 | Time: 00:00:08 | Train Loss: 0.8095 | Average Loss: 1.7524 \n",
      "Epoch:15 | Iter:  260 | Time: 00:00:08 | Train Loss: 0.7180 | Average Loss: 1.7497 \n",
      "Accuracy: 0.395833 | Time: 00:00:00\n",
      "Epoch:16 | Iter:    0 | Time: 00:00:08 | Train Loss: 1.3021 | Average Loss: 1.7479 \n",
      "Epoch:16 | Iter:   20 | Time: 00:00:08 | Train Loss: 1.7957 | Average Loss: 1.7455 \n",
      "Epoch:16 | Iter:   40 | Time: 00:00:08 | Train Loss: 1.4767 | Average Loss: 1.7427 \n",
      "Epoch:16 | Iter:   60 | Time: 00:00:08 | Train Loss: 1.5841 | Average Loss: 1.7400 \n",
      "Epoch:16 | Iter:   80 | Time: 00:00:08 | Train Loss: 0.7936 | Average Loss: 1.7378 \n",
      "Epoch:16 | Iter:  100 | Time: 00:00:08 | Train Loss: 1.3484 | Average Loss: 1.7355 \n",
      "Epoch:16 | Iter:  120 | Time: 00:00:08 | Train Loss: 0.9586 | Average Loss: 1.7333 \n",
      "Epoch:16 | Iter:  140 | Time: 00:00:08 | Train Loss: 1.5724 | Average Loss: 1.7307 \n",
      "Epoch:16 | Iter:  160 | Time: 00:00:09 | Train Loss: 1.2977 | Average Loss: 1.7288 \n",
      "Epoch:16 | Iter:  180 | Time: 00:00:09 | Train Loss: 1.6968 | Average Loss: 1.7264 \n",
      "Epoch:16 | Iter:  200 | Time: 00:00:09 | Train Loss: 1.2640 | Average Loss: 1.7241 \n",
      "Epoch:16 | Iter:  220 | Time: 00:00:09 | Train Loss: 0.9556 | Average Loss: 1.7213 \n",
      "Epoch:16 | Iter:  240 | Time: 00:00:09 | Train Loss: 0.8669 | Average Loss: 1.7190 \n",
      "Epoch:16 | Iter:  260 | Time: 00:00:09 | Train Loss: 0.7149 | Average Loss: 1.7161 \n",
      "Accuracy: 0.437500 | Time: 00:00:00\n",
      "Epoch:17 | Iter:    0 | Time: 00:00:09 | Train Loss: 1.0614 | Average Loss: 1.7143 \n",
      "Epoch:17 | Iter:   20 | Time: 00:00:09 | Train Loss: 1.6105 | Average Loss: 1.7126 \n",
      "Epoch:17 | Iter:   40 | Time: 00:00:09 | Train Loss: 1.2859 | Average Loss: 1.7101 \n",
      "Epoch:17 | Iter:   60 | Time: 00:00:09 | Train Loss: 1.6942 | Average Loss: 1.7080 \n",
      "Epoch:17 | Iter:   80 | Time: 00:00:09 | Train Loss: 0.7663 | Average Loss: 1.7057 \n",
      "Epoch:17 | Iter:  100 | Time: 00:00:09 | Train Loss: 1.2034 | Average Loss: 1.7040 \n",
      "Epoch:17 | Iter:  120 | Time: 00:00:09 | Train Loss: 0.8697 | Average Loss: 1.7017 \n",
      "Epoch:17 | Iter:  140 | Time: 00:00:09 | Train Loss: 1.3683 | Average Loss: 1.6997 \n",
      "Epoch:17 | Iter:  160 | Time: 00:00:09 | Train Loss: 1.1562 | Average Loss: 1.6978 \n",
      "Epoch:17 | Iter:  180 | Time: 00:00:09 | Train Loss: 1.4573 | Average Loss: 1.6956 \n",
      "Epoch:17 | Iter:  200 | Time: 00:00:09 | Train Loss: 1.1626 | Average Loss: 1.6931 \n",
      "Epoch:17 | Iter:  220 | Time: 00:00:09 | Train Loss: 0.9977 | Average Loss: 1.6909 \n",
      "Epoch:17 | Iter:  240 | Time: 00:00:09 | Train Loss: 0.6357 | Average Loss: 1.6883 \n",
      "Epoch:17 | Iter:  260 | Time: 00:00:09 | Train Loss: 0.9210 | Average Loss: 1.6858 \n",
      "Accuracy: 0.458333 | Time: 00:00:00\n",
      "Epoch:18 | Iter:    0 | Time: 00:00:09 | Train Loss: 1.0870 | Average Loss: 1.6842 \n",
      "Epoch:18 | Iter:   20 | Time: 00:00:09 | Train Loss: 1.5309 | Average Loss: 1.6822 \n",
      "Epoch:18 | Iter:   40 | Time: 00:00:09 | Train Loss: 0.8334 | Average Loss: 1.6796 \n",
      "Epoch:18 | Iter:   60 | Time: 00:00:09 | Train Loss: 1.1473 | Average Loss: 1.6772 \n",
      "Epoch:18 | Iter:   80 | Time: 00:00:10 | Train Loss: 0.7277 | Average Loss: 1.6755 \n",
      "Epoch:18 | Iter:  100 | Time: 00:00:10 | Train Loss: 1.0261 | Average Loss: 1.6732 \n",
      "Epoch:18 | Iter:  120 | Time: 00:00:10 | Train Loss: 1.1580 | Average Loss: 1.6715 \n",
      "Epoch:18 | Iter:  140 | Time: 00:00:10 | Train Loss: 1.2480 | Average Loss: 1.6700 \n",
      "Epoch:18 | Iter:  160 | Time: 00:00:10 | Train Loss: 1.1100 | Average Loss: 1.6681 \n",
      "Epoch:18 | Iter:  180 | Time: 00:00:10 | Train Loss: 1.3311 | Average Loss: 1.6658 \n",
      "Epoch:18 | Iter:  200 | Time: 00:00:10 | Train Loss: 1.3782 | Average Loss: 1.6638 \n",
      "Epoch:18 | Iter:  220 | Time: 00:00:10 | Train Loss: 0.9442 | Average Loss: 1.6616 \n",
      "Epoch:18 | Iter:  240 | Time: 00:00:10 | Train Loss: 0.7063 | Average Loss: 1.6596 \n",
      "Epoch:18 | Iter:  260 | Time: 00:00:10 | Train Loss: 0.5300 | Average Loss: 1.6575 \n",
      "Accuracy: 0.437500 | Time: 00:00:00\n",
      "Epoch:19 | Iter:    0 | Time: 00:00:10 | Train Loss: 1.0729 | Average Loss: 1.6564 \n",
      "Epoch:19 | Iter:   20 | Time: 00:00:10 | Train Loss: 1.4341 | Average Loss: 1.6545 \n",
      "Epoch:19 | Iter:   40 | Time: 00:00:10 | Train Loss: 1.5019 | Average Loss: 1.6522 \n",
      "Epoch:19 | Iter:   60 | Time: 00:00:10 | Train Loss: 1.3104 | Average Loss: 1.6499 \n",
      "Epoch:19 | Iter:   80 | Time: 00:00:10 | Train Loss: 0.6250 | Average Loss: 1.6478 \n",
      "Epoch:19 | Iter:  100 | Time: 00:00:10 | Train Loss: 1.0128 | Average Loss: 1.6457 \n",
      "Epoch:19 | Iter:  120 | Time: 00:00:10 | Train Loss: 0.7888 | Average Loss: 1.6438 \n",
      "Epoch:19 | Iter:  140 | Time: 00:00:10 | Train Loss: 1.5366 | Average Loss: 1.6423 \n",
      "Epoch:19 | Iter:  160 | Time: 00:00:10 | Train Loss: 1.2864 | Average Loss: 1.6403 \n",
      "Epoch:19 | Iter:  180 | Time: 00:00:10 | Train Loss: 1.2172 | Average Loss: 1.6385 \n",
      "Epoch:19 | Iter:  200 | Time: 00:00:10 | Train Loss: 1.1768 | Average Loss: 1.6364 \n",
      "Epoch:19 | Iter:  220 | Time: 00:00:10 | Train Loss: 1.0511 | Average Loss: 1.6346 \n",
      "Epoch:19 | Iter:  240 | Time: 00:00:10 | Train Loss: 0.8295 | Average Loss: 1.6329 \n",
      "Epoch:19 | Iter:  260 | Time: 00:00:10 | Train Loss: 0.7428 | Average Loss: 1.6307 \n",
      "Accuracy: 0.437500 | Time: 00:00:00\n",
      "Epoch:20 | Iter:    0 | Time: 00:00:11 | Train Loss: 1.1017 | Average Loss: 1.6292 \n",
      "Epoch:20 | Iter:   20 | Time: 00:00:11 | Train Loss: 1.3106 | Average Loss: 1.6275 \n",
      "Epoch:20 | Iter:   40 | Time: 00:00:11 | Train Loss: 0.9743 | Average Loss: 1.6255 \n",
      "Epoch:20 | Iter:   60 | Time: 00:00:11 | Train Loss: 0.9406 | Average Loss: 1.6231 \n",
      "Epoch:20 | Iter:   80 | Time: 00:00:11 | Train Loss: 0.8208 | Average Loss: 1.6213 \n",
      "Epoch:20 | Iter:  100 | Time: 00:00:11 | Train Loss: 1.5365 | Average Loss: 1.6193 \n",
      "Epoch:20 | Iter:  120 | Time: 00:00:11 | Train Loss: 1.0585 | Average Loss: 1.6174 \n",
      "Epoch:20 | Iter:  140 | Time: 00:00:11 | Train Loss: 1.4832 | Average Loss: 1.6160 \n",
      "Epoch:20 | Iter:  160 | Time: 00:00:11 | Train Loss: 1.2192 | Average Loss: 1.6145 \n",
      "Epoch:20 | Iter:  180 | Time: 00:00:11 | Train Loss: 0.8392 | Average Loss: 1.6126 \n",
      "Epoch:20 | Iter:  200 | Time: 00:00:11 | Train Loss: 1.3429 | Average Loss: 1.6110 \n",
      "Epoch:20 | Iter:  220 | Time: 00:00:11 | Train Loss: 0.8161 | Average Loss: 1.6092 \n",
      "Epoch:20 | Iter:  240 | Time: 00:00:11 | Train Loss: 0.8290 | Average Loss: 1.6073 \n",
      "Epoch:20 | Iter:  260 | Time: 00:00:11 | Train Loss: 0.5726 | Average Loss: 1.6050 \n",
      "Accuracy: 0.437500 | Time: 00:00:00\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------------\n",
    "#       Start Training & Evaluation\n",
    "#--------------------------------------------------\n",
    "net = TNet()\n",
    "train_option = {}\n",
    "train_option['lr'] = 0.002\n",
    "train_option['epoch'] = 20\n",
    "train_option['device'] = 'gpu'\n",
    "trainModel(net, trainloader_small, train_option, testloader_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WwDpNw03ELy9"
   },
   "source": [
    "Data augmentation: rotation\n",
    "\n",
    "Data normalization: subtract the mean from every image\n",
    "\n",
    "Layer 1: Convolution: nn.Conv2d(1, 16, 3)\n",
    "\n",
    "Layer 2: ReLU:\n",
    "\n",
    "Layer 3: maxPooling: nn.MaxPool2d(3, stride=2)\n",
    "\n",
    "Layer 4: Convolution: nn.Conv2d(16, 16, 3)\n",
    "\n",
    "Layer 5: ReLU:\n",
    "\n",
    "Layer 6: maxPooling: nn.MaxPool2d(3, stride=2)\n",
    "\n",
    "Layer 6: Dropout: nn.Dropout(0.5)\n",
    "\n",
    "Layer 7: Convolution: nn.Conv2d(16, 8, 3)\n",
    "\n",
    "Layer 8: ReLU:\n",
    "\n",
    "Layer 9: maxPooling: nn.MaxPool2d(3, stride=2)\n",
    "\n",
    "Layer 10: FC: nn.Linear(200, 22)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6RG-72WSzVEp"
   },
   "source": [
    "## Problem 1.2 [15 points]\n",
    "\n",
    " We tried **two techniques** to increase the accuracy of the model: e.g. increasing the training data by flipping the training images, adding batch normalization, different activation functions (e.g., sigmoid) and model architecture modifications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fMi0CYppMh_K"
   },
   "source": [
    "### Technique 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "qhdvKxQoSpKw"
   },
   "outputs": [],
   "source": [
    "#--------------------------------------------------\n",
    "#       Define Network Architecture\n",
    "#--------------------------------------------------\n",
    "class TNet(nn.Module):\n",
    "    def __init__(self):\n",
    "      super(TNet,self).__init__()\n",
    "      # self.inners = nn.Sequential(\n",
    "      #       nn.Conv2d(1, 16, 3),\n",
    "      #       nn.ReLU(),\n",
    "      #       nn.BatchNorm2d(16),\n",
    "      #       nn.MaxPool2d(2, stride=2),\n",
    "      #       nn.Conv2d(16, 32, 3),\n",
    "      #       nn.ReLU(),\n",
    "      #       nn.BatchNorm2d(32),\n",
    "      #       nn.MaxPool2d(4, stride=2),\n",
    "      #       nn.Dropout(0.5),\n",
    "      #       nn.Conv2d(32, 32, 3),\n",
    "      #       nn.ReLU(),\n",
    "      #       nn.BatchNorm2d(32),\n",
    "      #       nn.MaxPool2d(2, stride=2),\n",
    "      # )\n",
    "      # self.classifier = nn.Sequential(\n",
    "      #     nn.Linear(800, 200),\n",
    "      #     nn.ReLU(),\n",
    "      #     nn.Linear(200, 20)\n",
    "      # )\n",
    "      self.inners = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3),\n",
    "            nn.BatchNorm2d(16),  # Add Batch Normalization\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3, stride=2),\n",
    "            nn.Conv2d(16, 16, 3),\n",
    "            nn.BatchNorm2d(16),  # Add Batch Normalization\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3, stride=2),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Conv2d(16, 8, 3),\n",
    "            # nn.BatchNorm2d(8),  # Add Batch Normalization\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3, stride=2),\n",
    "      )\n",
    "          \n",
    "      self.classifier = nn.Sequential(\n",
    "        nn.Linear(200, 20)\n",
    "      )\n",
    "          \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "      x = self.inners(x)  \n",
    "      x = torch.flatten(x, 1)\n",
    "      x = self.classifier(x)\n",
    "      return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "_T2UG90ATZ3K"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Iter:    0 | Time: 00:00:00 | Train Loss: 3.6574 | Average Loss: 3.6574 \n",
      "Epoch: 1 | Iter:   20 | Time: 00:00:00 | Train Loss: 3.1188 | Average Loss: 3.0921 \n",
      "Epoch: 1 | Iter:   40 | Time: 00:00:00 | Train Loss: 2.8189 | Average Loss: 3.0125 \n",
      "Epoch: 1 | Iter:   60 | Time: 00:00:00 | Train Loss: 2.9680 | Average Loss: 2.9860 \n",
      "Epoch: 1 | Iter:   80 | Time: 00:00:00 | Train Loss: 3.0589 | Average Loss: 2.9641 \n",
      "Epoch: 1 | Iter:  100 | Time: 00:00:00 | Train Loss: 2.7288 | Average Loss: 2.9453 \n",
      "Epoch: 1 | Iter:  120 | Time: 00:00:00 | Train Loss: 2.4892 | Average Loss: 2.9268 \n",
      "Epoch: 1 | Iter:  140 | Time: 00:00:00 | Train Loss: 2.8430 | Average Loss: 2.9044 \n",
      "Epoch: 1 | Iter:  160 | Time: 00:00:00 | Train Loss: 2.8089 | Average Loss: 2.8773 \n",
      "Epoch: 1 | Iter:  180 | Time: 00:00:00 | Train Loss: 2.7430 | Average Loss: 2.8551 \n",
      "Epoch: 1 | Iter:  200 | Time: 00:00:00 | Train Loss: 2.5249 | Average Loss: 2.8388 \n",
      "Epoch: 1 | Iter:  220 | Time: 00:00:00 | Train Loss: 2.8541 | Average Loss: 2.8211 \n",
      "Epoch: 1 | Iter:  240 | Time: 00:00:00 | Train Loss: 2.4240 | Average Loss: 2.8060 \n",
      "Epoch: 1 | Iter:  260 | Time: 00:00:00 | Train Loss: 2.1019 | Average Loss: 2.7898 \n",
      "Accuracy: 0.187500 | Time: 00:00:00\n",
      "Epoch: 2 | Iter:    0 | Time: 00:00:00 | Train Loss: 2.3857 | Average Loss: 2.7714 \n",
      "Epoch: 2 | Iter:   20 | Time: 00:00:00 | Train Loss: 2.4658 | Average Loss: 2.7581 \n",
      "Epoch: 2 | Iter:   40 | Time: 00:00:00 | Train Loss: 2.3503 | Average Loss: 2.7414 \n",
      "Epoch: 2 | Iter:   60 | Time: 00:00:00 | Train Loss: 2.4950 | Average Loss: 2.7315 \n",
      "Epoch: 2 | Iter:   80 | Time: 00:00:00 | Train Loss: 2.5419 | Average Loss: 2.7187 \n",
      "Epoch: 2 | Iter:  100 | Time: 00:00:00 | Train Loss: 2.3912 | Average Loss: 2.7055 \n",
      "Epoch: 2 | Iter:  120 | Time: 00:00:00 | Train Loss: 1.9518 | Average Loss: 2.6949 \n",
      "Epoch: 2 | Iter:  140 | Time: 00:00:00 | Train Loss: 2.4501 | Average Loss: 2.6826 \n",
      "Epoch: 2 | Iter:  160 | Time: 00:00:00 | Train Loss: 2.5558 | Average Loss: 2.6688 \n",
      "Epoch: 2 | Iter:  180 | Time: 00:00:01 | Train Loss: 2.3923 | Average Loss: 2.6603 \n",
      "Epoch: 2 | Iter:  200 | Time: 00:00:01 | Train Loss: 2.2718 | Average Loss: 2.6517 \n",
      "Epoch: 2 | Iter:  220 | Time: 00:00:01 | Train Loss: 2.4866 | Average Loss: 2.6405 \n",
      "Epoch: 2 | Iter:  240 | Time: 00:00:01 | Train Loss: 2.2891 | Average Loss: 2.6320 \n",
      "Epoch: 2 | Iter:  260 | Time: 00:00:01 | Train Loss: 1.8078 | Average Loss: 2.6221 \n",
      "Accuracy: 0.208333 | Time: 00:00:00\n",
      "Epoch: 3 | Iter:    0 | Time: 00:00:01 | Train Loss: 2.1581 | Average Loss: 2.6133 \n",
      "Epoch: 3 | Iter:   20 | Time: 00:00:01 | Train Loss: 2.3024 | Average Loss: 2.6054 \n",
      "Epoch: 3 | Iter:   40 | Time: 00:00:01 | Train Loss: 2.3030 | Average Loss: 2.5961 \n",
      "Epoch: 3 | Iter:   60 | Time: 00:00:01 | Train Loss: 2.4862 | Average Loss: 2.5911 \n",
      "Epoch: 3 | Iter:   80 | Time: 00:00:01 | Train Loss: 2.1647 | Average Loss: 2.5850 \n",
      "Epoch: 3 | Iter:  100 | Time: 00:00:01 | Train Loss: 2.2038 | Average Loss: 2.5750 \n",
      "Epoch: 3 | Iter:  120 | Time: 00:00:01 | Train Loss: 1.7693 | Average Loss: 2.5677 \n",
      "Epoch: 3 | Iter:  140 | Time: 00:00:01 | Train Loss: 2.2622 | Average Loss: 2.5605 \n",
      "Epoch: 3 | Iter:  160 | Time: 00:00:01 | Train Loss: 2.5653 | Average Loss: 2.5538 \n",
      "Epoch: 3 | Iter:  180 | Time: 00:00:01 | Train Loss: 2.2974 | Average Loss: 2.5467 \n",
      "Epoch: 3 | Iter:  200 | Time: 00:00:01 | Train Loss: 2.3138 | Average Loss: 2.5402 \n",
      "Epoch: 3 | Iter:  220 | Time: 00:00:01 | Train Loss: 2.0934 | Average Loss: 2.5313 \n",
      "Epoch: 3 | Iter:  240 | Time: 00:00:01 | Train Loss: 2.0556 | Average Loss: 2.5250 \n",
      "Epoch: 3 | Iter:  260 | Time: 00:00:01 | Train Loss: 1.6706 | Average Loss: 2.5180 \n",
      "Accuracy: 0.250000 | Time: 00:00:00\n",
      "Epoch: 4 | Iter:    0 | Time: 00:00:01 | Train Loss: 1.9809 | Average Loss: 2.5121 \n",
      "Epoch: 4 | Iter:   20 | Time: 00:00:01 | Train Loss: 2.2645 | Average Loss: 2.5068 \n",
      "Epoch: 4 | Iter:   40 | Time: 00:00:01 | Train Loss: 2.0541 | Average Loss: 2.4998 \n",
      "Epoch: 4 | Iter:   60 | Time: 00:00:02 | Train Loss: 2.2255 | Average Loss: 2.4952 \n",
      "Epoch: 4 | Iter:   80 | Time: 00:00:02 | Train Loss: 2.0991 | Average Loss: 2.4904 \n",
      "Epoch: 4 | Iter:  100 | Time: 00:00:02 | Train Loss: 2.0465 | Average Loss: 2.4835 \n",
      "Epoch: 4 | Iter:  120 | Time: 00:00:02 | Train Loss: 1.6024 | Average Loss: 2.4773 \n",
      "Epoch: 4 | Iter:  140 | Time: 00:00:02 | Train Loss: 2.2482 | Average Loss: 2.4719 \n",
      "Epoch: 4 | Iter:  160 | Time: 00:00:02 | Train Loss: 2.5720 | Average Loss: 2.4664 \n",
      "Epoch: 4 | Iter:  180 | Time: 00:00:02 | Train Loss: 2.1286 | Average Loss: 2.4614 \n",
      "Epoch: 4 | Iter:  200 | Time: 00:00:02 | Train Loss: 2.0331 | Average Loss: 2.4562 \n",
      "Epoch: 4 | Iter:  220 | Time: 00:00:02 | Train Loss: 2.0313 | Average Loss: 2.4488 \n",
      "Epoch: 4 | Iter:  240 | Time: 00:00:02 | Train Loss: 2.1285 | Average Loss: 2.4435 \n",
      "Epoch: 4 | Iter:  260 | Time: 00:00:02 | Train Loss: 1.6433 | Average Loss: 2.4380 \n",
      "Accuracy: 0.229167 | Time: 00:00:00\n",
      "Epoch: 5 | Iter:    0 | Time: 00:00:02 | Train Loss: 1.9163 | Average Loss: 2.4351 \n",
      "Epoch: 5 | Iter:   20 | Time: 00:00:02 | Train Loss: 2.1695 | Average Loss: 2.4311 \n",
      "Epoch: 5 | Iter:   40 | Time: 00:00:02 | Train Loss: 2.1638 | Average Loss: 2.4258 \n",
      "Epoch: 5 | Iter:   60 | Time: 00:00:02 | Train Loss: 2.1758 | Average Loss: 2.4227 \n",
      "Epoch: 5 | Iter:   80 | Time: 00:00:02 | Train Loss: 2.0649 | Average Loss: 2.4190 \n",
      "Epoch: 5 | Iter:  100 | Time: 00:00:02 | Train Loss: 1.8804 | Average Loss: 2.4131 \n",
      "Epoch: 5 | Iter:  120 | Time: 00:00:02 | Train Loss: 1.6366 | Average Loss: 2.4088 \n",
      "Epoch: 5 | Iter:  140 | Time: 00:00:02 | Train Loss: 2.2012 | Average Loss: 2.4045 \n",
      "Epoch: 5 | Iter:  160 | Time: 00:00:02 | Train Loss: 2.5156 | Average Loss: 2.4006 \n",
      "Epoch: 5 | Iter:  180 | Time: 00:00:02 | Train Loss: 2.2420 | Average Loss: 2.3973 \n",
      "Epoch: 5 | Iter:  200 | Time: 00:00:02 | Train Loss: 1.8711 | Average Loss: 2.3931 \n",
      "Epoch: 5 | Iter:  220 | Time: 00:00:03 | Train Loss: 1.9805 | Average Loss: 2.3873 \n",
      "Epoch: 5 | Iter:  240 | Time: 00:00:03 | Train Loss: 2.0105 | Average Loss: 2.3831 \n",
      "Epoch: 5 | Iter:  260 | Time: 00:00:03 | Train Loss: 1.6132 | Average Loss: 2.3785 \n",
      "Accuracy: 0.250000 | Time: 00:00:00\n",
      "Epoch: 6 | Iter:    0 | Time: 00:00:03 | Train Loss: 1.9124 | Average Loss: 2.3751 \n",
      "Epoch: 6 | Iter:   20 | Time: 00:00:03 | Train Loss: 2.1711 | Average Loss: 2.3715 \n",
      "Epoch: 6 | Iter:   40 | Time: 00:00:03 | Train Loss: 2.0628 | Average Loss: 2.3674 \n",
      "Epoch: 6 | Iter:   60 | Time: 00:00:03 | Train Loss: 2.0731 | Average Loss: 2.3639 \n",
      "Epoch: 6 | Iter:   80 | Time: 00:00:03 | Train Loss: 1.9329 | Average Loss: 2.3609 \n",
      "Epoch: 6 | Iter:  100 | Time: 00:00:03 | Train Loss: 1.9523 | Average Loss: 2.3566 \n",
      "Epoch: 6 | Iter:  120 | Time: 00:00:03 | Train Loss: 1.3646 | Average Loss: 2.3533 \n",
      "Epoch: 6 | Iter:  140 | Time: 00:00:03 | Train Loss: 2.2953 | Average Loss: 2.3496 \n",
      "Epoch: 6 | Iter:  160 | Time: 00:00:03 | Train Loss: 2.4614 | Average Loss: 2.3464 \n",
      "Epoch: 6 | Iter:  180 | Time: 00:00:03 | Train Loss: 2.1598 | Average Loss: 2.3430 \n",
      "Epoch: 6 | Iter:  200 | Time: 00:00:03 | Train Loss: 1.8748 | Average Loss: 2.3394 \n",
      "Epoch: 6 | Iter:  220 | Time: 00:00:03 | Train Loss: 1.9760 | Average Loss: 2.3353 \n",
      "Epoch: 6 | Iter:  240 | Time: 00:00:03 | Train Loss: 1.8889 | Average Loss: 2.3318 \n",
      "Epoch: 6 | Iter:  260 | Time: 00:00:03 | Train Loss: 1.4294 | Average Loss: 2.3288 \n",
      "Accuracy: 0.250000 | Time: 00:00:00\n",
      "Epoch: 7 | Iter:    0 | Time: 00:00:03 | Train Loss: 1.8482 | Average Loss: 2.3260 \n",
      "Epoch: 7 | Iter:   20 | Time: 00:00:03 | Train Loss: 2.2821 | Average Loss: 2.3232 \n",
      "Epoch: 7 | Iter:   40 | Time: 00:00:03 | Train Loss: 2.1748 | Average Loss: 2.3197 \n",
      "Epoch: 7 | Iter:   60 | Time: 00:00:03 | Train Loss: 2.0425 | Average Loss: 2.3164 \n",
      "Epoch: 7 | Iter:   80 | Time: 00:00:03 | Train Loss: 1.9983 | Average Loss: 2.3142 \n",
      "Epoch: 7 | Iter:  100 | Time: 00:00:04 | Train Loss: 1.8888 | Average Loss: 2.3106 \n",
      "Epoch: 7 | Iter:  120 | Time: 00:00:04 | Train Loss: 1.2738 | Average Loss: 2.3077 \n",
      "Epoch: 7 | Iter:  140 | Time: 00:00:04 | Train Loss: 2.5012 | Average Loss: 2.3049 \n",
      "Epoch: 7 | Iter:  160 | Time: 00:00:04 | Train Loss: 2.5470 | Average Loss: 2.3022 \n",
      "Epoch: 7 | Iter:  180 | Time: 00:00:04 | Train Loss: 2.0879 | Average Loss: 2.2991 \n",
      "Epoch: 7 | Iter:  200 | Time: 00:00:04 | Train Loss: 2.0640 | Average Loss: 2.2958 \n",
      "Epoch: 7 | Iter:  220 | Time: 00:00:04 | Train Loss: 1.7495 | Average Loss: 2.2923 \n",
      "Epoch: 7 | Iter:  240 | Time: 00:00:04 | Train Loss: 1.9121 | Average Loss: 2.2894 \n",
      "Epoch: 7 | Iter:  260 | Time: 00:00:04 | Train Loss: 1.2246 | Average Loss: 2.2859 \n",
      "Accuracy: 0.250000 | Time: 00:00:00\n",
      "Epoch: 8 | Iter:    0 | Time: 00:00:04 | Train Loss: 1.4624 | Average Loss: 2.2828 \n",
      "Epoch: 8 | Iter:   20 | Time: 00:00:04 | Train Loss: 2.1265 | Average Loss: 2.2800 \n",
      "Epoch: 8 | Iter:   40 | Time: 00:00:04 | Train Loss: 1.8788 | Average Loss: 2.2771 \n",
      "Epoch: 8 | Iter:   60 | Time: 00:00:04 | Train Loss: 1.9971 | Average Loss: 2.2749 \n",
      "Epoch: 8 | Iter:   80 | Time: 00:00:04 | Train Loss: 1.7525 | Average Loss: 2.2722 \n",
      "Epoch: 8 | Iter:  100 | Time: 00:00:04 | Train Loss: 2.0100 | Average Loss: 2.2688 \n",
      "Epoch: 8 | Iter:  120 | Time: 00:00:04 | Train Loss: 1.1781 | Average Loss: 2.2665 \n",
      "Epoch: 8 | Iter:  140 | Time: 00:00:04 | Train Loss: 2.2025 | Average Loss: 2.2636 \n",
      "Epoch: 8 | Iter:  160 | Time: 00:00:04 | Train Loss: 2.5118 | Average Loss: 2.2612 \n",
      "Epoch: 8 | Iter:  180 | Time: 00:00:04 | Train Loss: 1.9107 | Average Loss: 2.2581 \n",
      "Epoch: 8 | Iter:  200 | Time: 00:00:04 | Train Loss: 1.8929 | Average Loss: 2.2551 \n",
      "Epoch: 8 | Iter:  220 | Time: 00:00:04 | Train Loss: 1.6723 | Average Loss: 2.2519 \n",
      "Epoch: 8 | Iter:  240 | Time: 00:00:05 | Train Loss: 1.7337 | Average Loss: 2.2488 \n",
      "Epoch: 8 | Iter:  260 | Time: 00:00:05 | Train Loss: 1.4430 | Average Loss: 2.2461 \n",
      "Accuracy: 0.270833 | Time: 00:00:00\n",
      "Epoch: 9 | Iter:    0 | Time: 00:00:05 | Train Loss: 1.5005 | Average Loss: 2.2439 \n",
      "Epoch: 9 | Iter:   20 | Time: 00:00:05 | Train Loss: 2.0196 | Average Loss: 2.2413 \n",
      "Epoch: 9 | Iter:   40 | Time: 00:00:05 | Train Loss: 1.9103 | Average Loss: 2.2386 \n",
      "Epoch: 9 | Iter:   60 | Time: 00:00:05 | Train Loss: 2.1725 | Average Loss: 2.2363 \n",
      "Epoch: 9 | Iter:   80 | Time: 00:00:05 | Train Loss: 1.4084 | Average Loss: 2.2343 \n",
      "Epoch: 9 | Iter:  100 | Time: 00:00:05 | Train Loss: 2.0596 | Average Loss: 2.2318 \n",
      "Epoch: 9 | Iter:  120 | Time: 00:00:05 | Train Loss: 1.2675 | Average Loss: 2.2294 \n",
      "Epoch: 9 | Iter:  140 | Time: 00:00:05 | Train Loss: 2.2704 | Average Loss: 2.2271 \n",
      "Epoch: 9 | Iter:  160 | Time: 00:00:05 | Train Loss: 2.5576 | Average Loss: 2.2249 \n",
      "Epoch: 9 | Iter:  180 | Time: 00:00:05 | Train Loss: 1.8937 | Average Loss: 2.2223 \n",
      "Epoch: 9 | Iter:  200 | Time: 00:00:05 | Train Loss: 1.7255 | Average Loss: 2.2195 \n",
      "Epoch: 9 | Iter:  220 | Time: 00:00:05 | Train Loss: 1.3809 | Average Loss: 2.2164 \n",
      "Epoch: 9 | Iter:  240 | Time: 00:00:05 | Train Loss: 2.0085 | Average Loss: 2.2137 \n",
      "Epoch: 9 | Iter:  260 | Time: 00:00:05 | Train Loss: 1.2504 | Average Loss: 2.2111 \n",
      "Accuracy: 0.270833 | Time: 00:00:00\n",
      "Epoch:10 | Iter:    0 | Time: 00:00:05 | Train Loss: 1.5097 | Average Loss: 2.2092 \n",
      "Epoch:10 | Iter:   20 | Time: 00:00:05 | Train Loss: 1.8010 | Average Loss: 2.2067 \n",
      "Epoch:10 | Iter:   40 | Time: 00:00:05 | Train Loss: 1.9416 | Average Loss: 2.2044 \n",
      "Epoch:10 | Iter:   60 | Time: 00:00:05 | Train Loss: 1.9071 | Average Loss: 2.2025 \n",
      "Epoch:10 | Iter:   80 | Time: 00:00:05 | Train Loss: 1.6655 | Average Loss: 2.2009 \n",
      "Epoch:10 | Iter:  100 | Time: 00:00:06 | Train Loss: 1.7923 | Average Loss: 2.1984 \n",
      "Epoch:10 | Iter:  120 | Time: 00:00:06 | Train Loss: 1.2109 | Average Loss: 2.1961 \n",
      "Epoch:10 | Iter:  140 | Time: 00:00:06 | Train Loss: 2.2115 | Average Loss: 2.1942 \n",
      "Epoch:10 | Iter:  160 | Time: 00:00:06 | Train Loss: 2.1046 | Average Loss: 2.1923 \n",
      "Epoch:10 | Iter:  180 | Time: 00:00:06 | Train Loss: 1.9177 | Average Loss: 2.1897 \n",
      "Epoch:10 | Iter:  200 | Time: 00:00:06 | Train Loss: 1.7038 | Average Loss: 2.1874 \n",
      "Epoch:10 | Iter:  220 | Time: 00:00:06 | Train Loss: 1.5543 | Average Loss: 2.1849 \n",
      "Epoch:10 | Iter:  240 | Time: 00:00:06 | Train Loss: 1.7046 | Average Loss: 2.1825 \n",
      "Epoch:10 | Iter:  260 | Time: 00:00:06 | Train Loss: 1.3608 | Average Loss: 2.1799 \n",
      "Accuracy: 0.270833 | Time: 00:00:00\n",
      "Epoch:11 | Iter:    0 | Time: 00:00:06 | Train Loss: 1.5221 | Average Loss: 2.1779 \n",
      "Epoch:11 | Iter:   20 | Time: 00:00:06 | Train Loss: 1.9620 | Average Loss: 2.1759 \n",
      "Epoch:11 | Iter:   40 | Time: 00:00:06 | Train Loss: 1.9286 | Average Loss: 2.1739 \n",
      "Epoch:11 | Iter:   60 | Time: 00:00:06 | Train Loss: 2.0628 | Average Loss: 2.1716 \n",
      "Epoch:11 | Iter:   80 | Time: 00:00:06 | Train Loss: 1.5284 | Average Loss: 2.1702 \n",
      "Epoch:11 | Iter:  100 | Time: 00:00:06 | Train Loss: 1.7296 | Average Loss: 2.1679 \n",
      "Epoch:11 | Iter:  120 | Time: 00:00:06 | Train Loss: 1.0821 | Average Loss: 2.1666 \n",
      "Epoch:11 | Iter:  140 | Time: 00:00:06 | Train Loss: 2.1039 | Average Loss: 2.1643 \n",
      "Epoch:11 | Iter:  160 | Time: 00:00:06 | Train Loss: 2.2212 | Average Loss: 2.1625 \n",
      "Epoch:11 | Iter:  180 | Time: 00:00:06 | Train Loss: 1.8742 | Average Loss: 2.1602 \n",
      "Epoch:11 | Iter:  200 | Time: 00:00:06 | Train Loss: 1.9901 | Average Loss: 2.1578 \n",
      "Epoch:11 | Iter:  220 | Time: 00:00:07 | Train Loss: 1.4501 | Average Loss: 2.1558 \n",
      "Epoch:11 | Iter:  240 | Time: 00:00:07 | Train Loss: 1.6622 | Average Loss: 2.1538 \n",
      "Epoch:11 | Iter:  260 | Time: 00:00:07 | Train Loss: 1.1038 | Average Loss: 2.1514 \n",
      "Accuracy: 0.291667 | Time: 00:00:00\n",
      "Epoch:12 | Iter:    0 | Time: 00:00:07 | Train Loss: 1.4949 | Average Loss: 2.1498 \n",
      "Epoch:12 | Iter:   20 | Time: 00:00:07 | Train Loss: 2.0086 | Average Loss: 2.1483 \n",
      "Epoch:12 | Iter:   40 | Time: 00:00:07 | Train Loss: 1.8792 | Average Loss: 2.1463 \n",
      "Epoch:12 | Iter:   60 | Time: 00:00:07 | Train Loss: 1.9538 | Average Loss: 2.1444 \n",
      "Epoch:12 | Iter:   80 | Time: 00:00:07 | Train Loss: 1.5850 | Average Loss: 2.1430 \n",
      "Epoch:12 | Iter:  100 | Time: 00:00:07 | Train Loss: 1.6876 | Average Loss: 2.1410 \n",
      "Epoch:12 | Iter:  120 | Time: 00:00:07 | Train Loss: 1.2830 | Average Loss: 2.1393 \n",
      "Epoch:12 | Iter:  140 | Time: 00:00:07 | Train Loss: 2.1681 | Average Loss: 2.1377 \n",
      "Epoch:12 | Iter:  160 | Time: 00:00:07 | Train Loss: 2.2556 | Average Loss: 2.1359 \n",
      "Epoch:12 | Iter:  180 | Time: 00:00:07 | Train Loss: 1.8027 | Average Loss: 2.1335 \n",
      "Epoch:12 | Iter:  200 | Time: 00:00:07 | Train Loss: 1.9682 | Average Loss: 2.1317 \n",
      "Epoch:12 | Iter:  220 | Time: 00:00:07 | Train Loss: 1.4033 | Average Loss: 2.1296 \n",
      "Epoch:12 | Iter:  240 | Time: 00:00:07 | Train Loss: 1.7771 | Average Loss: 2.1278 \n",
      "Epoch:12 | Iter:  260 | Time: 00:00:07 | Train Loss: 1.1669 | Average Loss: 2.1255 \n",
      "Accuracy: 0.270833 | Time: 00:00:00\n",
      "Epoch:13 | Iter:    0 | Time: 00:00:07 | Train Loss: 1.8527 | Average Loss: 2.1240 \n",
      "Epoch:13 | Iter:   20 | Time: 00:00:07 | Train Loss: 1.7935 | Average Loss: 2.1223 \n",
      "Epoch:13 | Iter:   40 | Time: 00:00:08 | Train Loss: 1.7703 | Average Loss: 2.1204 \n",
      "Epoch:13 | Iter:   60 | Time: 00:00:08 | Train Loss: 1.8633 | Average Loss: 2.1185 \n",
      "Epoch:13 | Iter:   80 | Time: 00:00:08 | Train Loss: 1.6407 | Average Loss: 2.1169 \n",
      "Epoch:13 | Iter:  100 | Time: 00:00:08 | Train Loss: 1.8464 | Average Loss: 2.1149 \n",
      "Epoch:13 | Iter:  120 | Time: 00:00:08 | Train Loss: 1.2274 | Average Loss: 2.1134 \n",
      "Epoch:13 | Iter:  140 | Time: 00:00:08 | Train Loss: 2.3167 | Average Loss: 2.1119 \n",
      "Epoch:13 | Iter:  160 | Time: 00:00:08 | Train Loss: 2.1692 | Average Loss: 2.1104 \n",
      "Epoch:13 | Iter:  180 | Time: 00:00:08 | Train Loss: 2.0042 | Average Loss: 2.1088 \n",
      "Epoch:13 | Iter:  200 | Time: 00:00:08 | Train Loss: 1.8707 | Average Loss: 2.1070 \n",
      "Epoch:13 | Iter:  220 | Time: 00:00:08 | Train Loss: 1.5148 | Average Loss: 2.1054 \n",
      "Epoch:13 | Iter:  240 | Time: 00:00:08 | Train Loss: 1.7850 | Average Loss: 2.1036 \n",
      "Epoch:13 | Iter:  260 | Time: 00:00:08 | Train Loss: 1.0431 | Average Loss: 2.1018 \n",
      "Accuracy: 0.312500 | Time: 00:00:00\n",
      "Epoch:14 | Iter:    0 | Time: 00:00:08 | Train Loss: 1.3894 | Average Loss: 2.1006 \n",
      "Epoch:14 | Iter:   20 | Time: 00:00:08 | Train Loss: 1.9120 | Average Loss: 2.0993 \n",
      "Epoch:14 | Iter:   40 | Time: 00:00:08 | Train Loss: 1.6936 | Average Loss: 2.0978 \n",
      "Epoch:14 | Iter:   60 | Time: 00:00:08 | Train Loss: 1.7507 | Average Loss: 2.0960 \n",
      "Epoch:14 | Iter:   80 | Time: 00:00:08 | Train Loss: 1.5213 | Average Loss: 2.0947 \n",
      "Epoch:14 | Iter:  100 | Time: 00:00:08 | Train Loss: 1.6859 | Average Loss: 2.0928 \n",
      "Epoch:14 | Iter:  120 | Time: 00:00:08 | Train Loss: 1.1205 | Average Loss: 2.0914 \n",
      "Epoch:14 | Iter:  140 | Time: 00:00:08 | Train Loss: 2.0134 | Average Loss: 2.0898 \n",
      "Epoch:14 | Iter:  160 | Time: 00:00:09 | Train Loss: 2.0926 | Average Loss: 2.0884 \n",
      "Epoch:14 | Iter:  180 | Time: 00:00:09 | Train Loss: 1.8214 | Average Loss: 2.0862 \n",
      "Epoch:14 | Iter:  200 | Time: 00:00:09 | Train Loss: 1.9974 | Average Loss: 2.0844 \n",
      "Epoch:14 | Iter:  220 | Time: 00:00:09 | Train Loss: 1.3435 | Average Loss: 2.0826 \n",
      "Epoch:14 | Iter:  240 | Time: 00:00:09 | Train Loss: 1.7682 | Average Loss: 2.0811 \n",
      "Epoch:14 | Iter:  260 | Time: 00:00:09 | Train Loss: 1.3004 | Average Loss: 2.0795 \n",
      "Accuracy: 0.312500 | Time: 00:00:00\n",
      "Epoch:15 | Iter:    0 | Time: 00:00:09 | Train Loss: 1.4215 | Average Loss: 2.0782 \n",
      "Epoch:15 | Iter:   20 | Time: 00:00:09 | Train Loss: 1.6424 | Average Loss: 2.0767 \n",
      "Epoch:15 | Iter:   40 | Time: 00:00:09 | Train Loss: 1.7232 | Average Loss: 2.0751 \n",
      "Epoch:15 | Iter:   60 | Time: 00:00:09 | Train Loss: 1.7803 | Average Loss: 2.0735 \n",
      "Epoch:15 | Iter:   80 | Time: 00:00:09 | Train Loss: 1.7237 | Average Loss: 2.0725 \n",
      "Epoch:15 | Iter:  100 | Time: 00:00:09 | Train Loss: 1.7689 | Average Loss: 2.0711 \n",
      "Epoch:15 | Iter:  120 | Time: 00:00:09 | Train Loss: 1.1137 | Average Loss: 2.0699 \n",
      "Epoch:15 | Iter:  140 | Time: 00:00:09 | Train Loss: 1.9762 | Average Loss: 2.0684 \n",
      "Epoch:15 | Iter:  160 | Time: 00:00:09 | Train Loss: 2.2138 | Average Loss: 2.0670 \n",
      "Epoch:15 | Iter:  180 | Time: 00:00:09 | Train Loss: 1.6313 | Average Loss: 2.0654 \n",
      "Epoch:15 | Iter:  200 | Time: 00:00:09 | Train Loss: 1.9442 | Average Loss: 2.0638 \n",
      "Epoch:15 | Iter:  220 | Time: 00:00:09 | Train Loss: 1.3681 | Average Loss: 2.0621 \n",
      "Epoch:15 | Iter:  240 | Time: 00:00:09 | Train Loss: 1.7835 | Average Loss: 2.0605 \n",
      "Epoch:15 | Iter:  260 | Time: 00:00:09 | Train Loss: 1.3062 | Average Loss: 2.0590 \n",
      "Accuracy: 0.354167 | Time: 00:00:00\n",
      "Epoch:16 | Iter:    0 | Time: 00:00:10 | Train Loss: 1.4241 | Average Loss: 2.0578 \n",
      "Epoch:16 | Iter:   20 | Time: 00:00:10 | Train Loss: 1.8266 | Average Loss: 2.0567 \n",
      "Epoch:16 | Iter:   40 | Time: 00:00:10 | Train Loss: 1.8074 | Average Loss: 2.0555 \n",
      "Epoch:16 | Iter:   60 | Time: 00:00:10 | Train Loss: 1.9879 | Average Loss: 2.0542 \n",
      "Epoch:16 | Iter:   80 | Time: 00:00:10 | Train Loss: 1.4021 | Average Loss: 2.0531 \n",
      "Epoch:16 | Iter:  100 | Time: 00:00:10 | Train Loss: 1.8039 | Average Loss: 2.0517 \n",
      "Epoch:16 | Iter:  120 | Time: 00:00:10 | Train Loss: 1.1547 | Average Loss: 2.0507 \n",
      "Epoch:16 | Iter:  140 | Time: 00:00:10 | Train Loss: 1.9243 | Average Loss: 2.0495 \n",
      "Epoch:16 | Iter:  160 | Time: 00:00:10 | Train Loss: 2.1735 | Average Loss: 2.0482 \n",
      "Epoch:16 | Iter:  180 | Time: 00:00:10 | Train Loss: 1.7128 | Average Loss: 2.0466 \n",
      "Epoch:16 | Iter:  200 | Time: 00:00:10 | Train Loss: 1.8289 | Average Loss: 2.0453 \n",
      "Epoch:16 | Iter:  220 | Time: 00:00:10 | Train Loss: 1.2263 | Average Loss: 2.0435 \n",
      "Epoch:16 | Iter:  240 | Time: 00:00:10 | Train Loss: 1.7375 | Average Loss: 2.0421 \n",
      "Epoch:16 | Iter:  260 | Time: 00:00:10 | Train Loss: 1.3017 | Average Loss: 2.0408 \n",
      "Accuracy: 0.395833 | Time: 00:00:00\n",
      "Epoch:17 | Iter:    0 | Time: 00:00:10 | Train Loss: 1.4040 | Average Loss: 2.0397 \n",
      "Epoch:17 | Iter:   20 | Time: 00:00:10 | Train Loss: 1.9454 | Average Loss: 2.0387 \n",
      "Epoch:17 | Iter:   40 | Time: 00:00:10 | Train Loss: 1.7297 | Average Loss: 2.0374 \n",
      "Epoch:17 | Iter:   60 | Time: 00:00:10 | Train Loss: 1.5004 | Average Loss: 2.0357 \n",
      "Epoch:17 | Iter:   80 | Time: 00:00:11 | Train Loss: 1.1815 | Average Loss: 2.0348 \n",
      "Epoch:17 | Iter:  100 | Time: 00:00:11 | Train Loss: 1.6623 | Average Loss: 2.0334 \n",
      "Epoch:17 | Iter:  120 | Time: 00:00:11 | Train Loss: 0.9601 | Average Loss: 2.0325 \n",
      "Epoch:17 | Iter:  140 | Time: 00:00:11 | Train Loss: 2.0693 | Average Loss: 2.0314 \n",
      "Epoch:17 | Iter:  160 | Time: 00:00:11 | Train Loss: 2.3401 | Average Loss: 2.0302 \n",
      "Epoch:17 | Iter:  180 | Time: 00:00:11 | Train Loss: 1.7052 | Average Loss: 2.0287 \n",
      "Epoch:17 | Iter:  200 | Time: 00:00:11 | Train Loss: 1.7662 | Average Loss: 2.0274 \n",
      "Epoch:17 | Iter:  220 | Time: 00:00:11 | Train Loss: 1.4395 | Average Loss: 2.0262 \n",
      "Epoch:17 | Iter:  240 | Time: 00:00:11 | Train Loss: 1.7317 | Average Loss: 2.0248 \n",
      "Epoch:17 | Iter:  260 | Time: 00:00:11 | Train Loss: 1.1518 | Average Loss: 2.0234 \n",
      "Accuracy: 0.375000 | Time: 00:00:00\n",
      "Epoch:18 | Iter:    0 | Time: 00:00:11 | Train Loss: 1.3608 | Average Loss: 2.0222 \n",
      "Epoch:18 | Iter:   20 | Time: 00:00:11 | Train Loss: 1.7673 | Average Loss: 2.0213 \n",
      "Epoch:18 | Iter:   40 | Time: 00:00:11 | Train Loss: 1.7181 | Average Loss: 2.0200 \n",
      "Epoch:18 | Iter:   60 | Time: 00:00:11 | Train Loss: 1.7030 | Average Loss: 2.0187 \n",
      "Epoch:18 | Iter:   80 | Time: 00:00:11 | Train Loss: 1.4855 | Average Loss: 2.0178 \n",
      "Epoch:18 | Iter:  100 | Time: 00:00:11 | Train Loss: 1.7026 | Average Loss: 2.0166 \n",
      "Epoch:18 | Iter:  120 | Time: 00:00:11 | Train Loss: 1.0813 | Average Loss: 2.0156 \n",
      "Epoch:18 | Iter:  140 | Time: 00:00:11 | Train Loss: 1.9634 | Average Loss: 2.0141 \n",
      "Epoch:18 | Iter:  160 | Time: 00:00:11 | Train Loss: 2.1477 | Average Loss: 2.0133 \n",
      "Epoch:18 | Iter:  180 | Time: 00:00:11 | Train Loss: 1.6223 | Average Loss: 2.0117 \n",
      "Epoch:18 | Iter:  200 | Time: 00:00:12 | Train Loss: 1.7448 | Average Loss: 2.0106 \n",
      "Epoch:18 | Iter:  220 | Time: 00:00:12 | Train Loss: 1.1029 | Average Loss: 2.0094 \n",
      "Epoch:18 | Iter:  240 | Time: 00:00:12 | Train Loss: 1.4905 | Average Loss: 2.0082 \n",
      "Epoch:18 | Iter:  260 | Time: 00:00:12 | Train Loss: 1.2490 | Average Loss: 2.0069 \n",
      "Accuracy: 0.375000 | Time: 00:00:00\n",
      "Epoch:19 | Iter:    0 | Time: 00:00:12 | Train Loss: 1.4445 | Average Loss: 2.0060 \n",
      "Epoch:19 | Iter:   20 | Time: 00:00:12 | Train Loss: 1.8162 | Average Loss: 2.0050 \n",
      "Epoch:19 | Iter:   40 | Time: 00:00:12 | Train Loss: 1.5816 | Average Loss: 2.0037 \n",
      "Epoch:19 | Iter:   60 | Time: 00:00:12 | Train Loss: 1.8026 | Average Loss: 2.0024 \n",
      "Epoch:19 | Iter:   80 | Time: 00:00:12 | Train Loss: 1.5016 | Average Loss: 2.0016 \n",
      "Epoch:19 | Iter:  100 | Time: 00:00:12 | Train Loss: 1.7987 | Average Loss: 2.0004 \n",
      "Epoch:19 | Iter:  120 | Time: 00:00:12 | Train Loss: 1.0413 | Average Loss: 1.9992 \n",
      "Epoch:19 | Iter:  140 | Time: 00:00:12 | Train Loss: 2.0517 | Average Loss: 1.9981 \n",
      "Epoch:19 | Iter:  160 | Time: 00:00:12 | Train Loss: 1.8929 | Average Loss: 1.9972 \n",
      "Epoch:19 | Iter:  180 | Time: 00:00:12 | Train Loss: 1.6968 | Average Loss: 1.9957 \n",
      "Epoch:19 | Iter:  200 | Time: 00:00:12 | Train Loss: 2.0125 | Average Loss: 1.9946 \n",
      "Epoch:19 | Iter:  220 | Time: 00:00:12 | Train Loss: 1.2256 | Average Loss: 1.9933 \n",
      "Epoch:19 | Iter:  240 | Time: 00:00:12 | Train Loss: 1.3675 | Average Loss: 1.9919 \n",
      "Epoch:19 | Iter:  260 | Time: 00:00:12 | Train Loss: 1.0553 | Average Loss: 1.9904 \n",
      "Accuracy: 0.312500 | Time: 00:00:00\n",
      "Epoch:20 | Iter:    0 | Time: 00:00:12 | Train Loss: 1.3096 | Average Loss: 1.9897 \n",
      "Epoch:20 | Iter:   20 | Time: 00:00:13 | Train Loss: 1.9927 | Average Loss: 1.9889 \n",
      "Epoch:20 | Iter:   40 | Time: 00:00:13 | Train Loss: 1.4175 | Average Loss: 1.9876 \n",
      "Epoch:20 | Iter:   60 | Time: 00:00:13 | Train Loss: 1.6750 | Average Loss: 1.9863 \n",
      "Epoch:20 | Iter:   80 | Time: 00:00:13 | Train Loss: 1.0723 | Average Loss: 1.9853 \n",
      "Epoch:20 | Iter:  100 | Time: 00:00:13 | Train Loss: 1.5801 | Average Loss: 1.9840 \n",
      "Epoch:20 | Iter:  120 | Time: 00:00:13 | Train Loss: 0.9027 | Average Loss: 1.9830 \n",
      "Epoch:20 | Iter:  140 | Time: 00:00:13 | Train Loss: 2.2885 | Average Loss: 1.9818 \n",
      "Epoch:20 | Iter:  160 | Time: 00:00:13 | Train Loss: 1.7383 | Average Loss: 1.9808 \n",
      "Epoch:20 | Iter:  180 | Time: 00:00:13 | Train Loss: 1.6501 | Average Loss: 1.9796 \n",
      "Epoch:20 | Iter:  200 | Time: 00:00:13 | Train Loss: 1.7527 | Average Loss: 1.9784 \n",
      "Epoch:20 | Iter:  220 | Time: 00:00:13 | Train Loss: 1.1834 | Average Loss: 1.9773 \n",
      "Epoch:20 | Iter:  240 | Time: 00:00:13 | Train Loss: 1.4432 | Average Loss: 1.9761 \n",
      "Epoch:20 | Iter:  260 | Time: 00:00:13 | Train Loss: 1.2118 | Average Loss: 1.9750 \n",
      "Accuracy: 0.354167 | Time: 00:00:00\n",
      "Epoch:21 | Iter:    0 | Time: 00:00:13 | Train Loss: 1.4071 | Average Loss: 1.9742 \n",
      "Epoch:21 | Iter:   20 | Time: 00:00:13 | Train Loss: 1.6129 | Average Loss: 1.9733 \n",
      "Epoch:21 | Iter:   40 | Time: 00:00:13 | Train Loss: 1.5721 | Average Loss: 1.9722 \n",
      "Epoch:21 | Iter:   60 | Time: 00:00:13 | Train Loss: 1.6123 | Average Loss: 1.9710 \n",
      "Epoch:21 | Iter:   80 | Time: 00:00:13 | Train Loss: 1.1278 | Average Loss: 1.9701 \n",
      "Epoch:21 | Iter:  100 | Time: 00:00:13 | Train Loss: 1.8458 | Average Loss: 1.9691 \n",
      "Epoch:21 | Iter:  120 | Time: 00:00:13 | Train Loss: 1.1650 | Average Loss: 1.9682 \n",
      "Epoch:21 | Iter:  140 | Time: 00:00:13 | Train Loss: 2.1028 | Average Loss: 1.9673 \n",
      "Epoch:21 | Iter:  160 | Time: 00:00:13 | Train Loss: 2.1175 | Average Loss: 1.9664 \n",
      "Epoch:21 | Iter:  180 | Time: 00:00:13 | Train Loss: 1.5754 | Average Loss: 1.9652 \n",
      "Epoch:21 | Iter:  200 | Time: 00:00:14 | Train Loss: 1.9433 | Average Loss: 1.9639 \n",
      "Epoch:21 | Iter:  220 | Time: 00:00:14 | Train Loss: 1.0271 | Average Loss: 1.9629 \n",
      "Epoch:21 | Iter:  240 | Time: 00:00:14 | Train Loss: 1.4545 | Average Loss: 1.9619 \n",
      "Epoch:21 | Iter:  260 | Time: 00:00:14 | Train Loss: 1.2753 | Average Loss: 1.9608 \n",
      "Accuracy: 0.312500 | Time: 00:00:00\n",
      "Epoch:22 | Iter:    0 | Time: 00:00:14 | Train Loss: 1.2607 | Average Loss: 1.9600 \n",
      "Epoch:22 | Iter:   20 | Time: 00:00:14 | Train Loss: 1.9476 | Average Loss: 1.9591 \n",
      "Epoch:22 | Iter:   40 | Time: 00:00:14 | Train Loss: 1.7114 | Average Loss: 1.9580 \n",
      "Epoch:22 | Iter:   60 | Time: 00:00:14 | Train Loss: 1.4904 | Average Loss: 1.9568 \n",
      "Epoch:22 | Iter:   80 | Time: 00:00:14 | Train Loss: 1.3464 | Average Loss: 1.9560 \n",
      "Epoch:22 | Iter:  100 | Time: 00:00:14 | Train Loss: 1.4364 | Average Loss: 1.9551 \n",
      "Epoch:22 | Iter:  120 | Time: 00:00:14 | Train Loss: 1.1133 | Average Loss: 1.9545 \n",
      "Epoch:22 | Iter:  140 | Time: 00:00:14 | Train Loss: 1.9036 | Average Loss: 1.9534 \n",
      "Epoch:22 | Iter:  160 | Time: 00:00:14 | Train Loss: 2.1820 | Average Loss: 1.9528 \n",
      "Epoch:22 | Iter:  180 | Time: 00:00:14 | Train Loss: 1.4847 | Average Loss: 1.9515 \n",
      "Epoch:22 | Iter:  200 | Time: 00:00:14 | Train Loss: 1.6023 | Average Loss: 1.9505 \n",
      "Epoch:22 | Iter:  220 | Time: 00:00:14 | Train Loss: 1.2041 | Average Loss: 1.9495 \n",
      "Epoch:22 | Iter:  240 | Time: 00:00:14 | Train Loss: 1.4744 | Average Loss: 1.9484 \n",
      "Epoch:22 | Iter:  260 | Time: 00:00:14 | Train Loss: 1.0504 | Average Loss: 1.9473 \n",
      "Accuracy: 0.395833 | Time: 00:00:00\n",
      "Epoch:23 | Iter:    0 | Time: 00:00:14 | Train Loss: 1.3389 | Average Loss: 1.9464 \n",
      "Epoch:23 | Iter:   20 | Time: 00:00:14 | Train Loss: 1.8958 | Average Loss: 1.9456 \n",
      "Epoch:23 | Iter:   40 | Time: 00:00:14 | Train Loss: 1.5925 | Average Loss: 1.9446 \n",
      "Epoch:23 | Iter:   60 | Time: 00:00:14 | Train Loss: 1.6354 | Average Loss: 1.9435 \n",
      "Epoch:23 | Iter:   80 | Time: 00:00:15 | Train Loss: 1.2948 | Average Loss: 1.9427 \n",
      "Epoch:23 | Iter:  100 | Time: 00:00:15 | Train Loss: 1.7033 | Average Loss: 1.9418 \n",
      "Epoch:23 | Iter:  120 | Time: 00:00:15 | Train Loss: 0.8737 | Average Loss: 1.9410 \n",
      "Epoch:23 | Iter:  140 | Time: 00:00:15 | Train Loss: 1.9513 | Average Loss: 1.9400 \n",
      "Epoch:23 | Iter:  160 | Time: 00:00:15 | Train Loss: 1.9063 | Average Loss: 1.9391 \n",
      "Epoch:23 | Iter:  180 | Time: 00:00:15 | Train Loss: 1.8481 | Average Loss: 1.9382 \n",
      "Epoch:23 | Iter:  200 | Time: 00:00:15 | Train Loss: 1.6684 | Average Loss: 1.9372 \n",
      "Epoch:23 | Iter:  220 | Time: 00:00:15 | Train Loss: 1.1592 | Average Loss: 1.9363 \n",
      "Epoch:23 | Iter:  240 | Time: 00:00:15 | Train Loss: 1.4518 | Average Loss: 1.9353 \n",
      "Epoch:23 | Iter:  260 | Time: 00:00:15 | Train Loss: 1.2051 | Average Loss: 1.9345 \n",
      "Accuracy: 0.354167 | Time: 00:00:00\n",
      "Epoch:24 | Iter:    0 | Time: 00:00:15 | Train Loss: 1.3304 | Average Loss: 1.9339 \n",
      "Epoch:24 | Iter:   20 | Time: 00:00:15 | Train Loss: 1.9150 | Average Loss: 1.9331 \n",
      "Epoch:24 | Iter:   40 | Time: 00:00:15 | Train Loss: 1.5991 | Average Loss: 1.9322 \n",
      "Epoch:24 | Iter:   60 | Time: 00:00:15 | Train Loss: 1.4739 | Average Loss: 1.9312 \n",
      "Epoch:24 | Iter:   80 | Time: 00:00:15 | Train Loss: 1.2651 | Average Loss: 1.9303 \n",
      "Epoch:24 | Iter:  100 | Time: 00:00:15 | Train Loss: 1.6493 | Average Loss: 1.9294 \n",
      "Epoch:24 | Iter:  120 | Time: 00:00:15 | Train Loss: 1.1902 | Average Loss: 1.9287 \n",
      "Epoch:24 | Iter:  140 | Time: 00:00:15 | Train Loss: 1.8542 | Average Loss: 1.9277 \n",
      "Epoch:24 | Iter:  160 | Time: 00:00:15 | Train Loss: 1.6774 | Average Loss: 1.9269 \n",
      "Epoch:24 | Iter:  180 | Time: 00:00:15 | Train Loss: 1.5276 | Average Loss: 1.9258 \n",
      "Epoch:24 | Iter:  200 | Time: 00:00:15 | Train Loss: 1.8828 | Average Loss: 1.9249 \n",
      "Epoch:24 | Iter:  220 | Time: 00:00:15 | Train Loss: 0.9019 | Average Loss: 1.9239 \n",
      "Epoch:24 | Iter:  240 | Time: 00:00:15 | Train Loss: 1.5308 | Average Loss: 1.9229 \n",
      "Epoch:24 | Iter:  260 | Time: 00:00:16 | Train Loss: 1.2846 | Average Loss: 1.9219 \n",
      "Accuracy: 0.375000 | Time: 00:00:00\n",
      "Epoch:25 | Iter:    0 | Time: 00:00:16 | Train Loss: 1.2592 | Average Loss: 1.9213 \n",
      "Epoch:25 | Iter:   20 | Time: 00:00:16 | Train Loss: 1.7755 | Average Loss: 1.9206 \n",
      "Epoch:25 | Iter:   40 | Time: 00:00:16 | Train Loss: 1.7034 | Average Loss: 1.9196 \n",
      "Epoch:25 | Iter:   60 | Time: 00:00:16 | Train Loss: 1.5928 | Average Loss: 1.9187 \n",
      "Epoch:25 | Iter:   80 | Time: 00:00:16 | Train Loss: 1.1558 | Average Loss: 1.9180 \n",
      "Epoch:25 | Iter:  100 | Time: 00:00:16 | Train Loss: 1.7297 | Average Loss: 1.9171 \n",
      "Epoch:25 | Iter:  120 | Time: 00:00:16 | Train Loss: 1.1001 | Average Loss: 1.9164 \n",
      "Epoch:25 | Iter:  140 | Time: 00:00:16 | Train Loss: 1.8400 | Average Loss: 1.9157 \n",
      "Epoch:25 | Iter:  160 | Time: 00:00:16 | Train Loss: 1.8943 | Average Loss: 1.9149 \n",
      "Epoch:25 | Iter:  180 | Time: 00:00:16 | Train Loss: 1.6046 | Average Loss: 1.9138 \n",
      "Epoch:25 | Iter:  200 | Time: 00:00:16 | Train Loss: 1.7725 | Average Loss: 1.9129 \n",
      "Epoch:25 | Iter:  220 | Time: 00:00:16 | Train Loss: 1.3512 | Average Loss: 1.9121 \n",
      "Epoch:25 | Iter:  240 | Time: 00:00:16 | Train Loss: 1.2121 | Average Loss: 1.9112 \n",
      "Epoch:25 | Iter:  260 | Time: 00:00:16 | Train Loss: 1.0829 | Average Loss: 1.9102 \n",
      "Accuracy: 0.354167 | Time: 00:00:00\n",
      "Epoch:26 | Iter:    0 | Time: 00:00:16 | Train Loss: 1.2637 | Average Loss: 1.9095 \n",
      "Epoch:26 | Iter:   20 | Time: 00:00:16 | Train Loss: 1.6357 | Average Loss: 1.9088 \n",
      "Epoch:26 | Iter:   40 | Time: 00:00:16 | Train Loss: 1.6557 | Average Loss: 1.9079 \n",
      "Epoch:26 | Iter:   60 | Time: 00:00:16 | Train Loss: 1.5219 | Average Loss: 1.9069 \n",
      "Epoch:26 | Iter:   80 | Time: 00:00:16 | Train Loss: 1.2807 | Average Loss: 1.9062 \n",
      "Epoch:26 | Iter:  100 | Time: 00:00:16 | Train Loss: 1.9086 | Average Loss: 1.9054 \n",
      "Epoch:26 | Iter:  120 | Time: 00:00:17 | Train Loss: 1.1190 | Average Loss: 1.9048 \n",
      "Epoch:26 | Iter:  140 | Time: 00:00:17 | Train Loss: 1.8084 | Average Loss: 1.9039 \n",
      "Epoch:26 | Iter:  160 | Time: 00:00:17 | Train Loss: 1.4708 | Average Loss: 1.9032 \n",
      "Epoch:26 | Iter:  180 | Time: 00:00:17 | Train Loss: 1.7729 | Average Loss: 1.9021 \n",
      "Epoch:26 | Iter:  200 | Time: 00:00:17 | Train Loss: 1.5807 | Average Loss: 1.9014 \n",
      "Epoch:26 | Iter:  220 | Time: 00:00:17 | Train Loss: 1.0984 | Average Loss: 1.9007 \n",
      "Epoch:26 | Iter:  240 | Time: 00:00:17 | Train Loss: 1.2908 | Average Loss: 1.8998 \n",
      "Epoch:26 | Iter:  260 | Time: 00:00:17 | Train Loss: 0.9966 | Average Loss: 1.8989 \n",
      "Accuracy: 0.333333 | Time: 00:00:00\n",
      "Epoch:27 | Iter:    0 | Time: 00:00:17 | Train Loss: 1.4209 | Average Loss: 1.8982 \n",
      "Epoch:27 | Iter:   20 | Time: 00:00:17 | Train Loss: 1.7125 | Average Loss: 1.8976 \n",
      "Epoch:27 | Iter:   40 | Time: 00:00:17 | Train Loss: 1.7684 | Average Loss: 1.8967 \n",
      "Epoch:27 | Iter:   60 | Time: 00:00:17 | Train Loss: 1.5097 | Average Loss: 1.8957 \n",
      "Epoch:27 | Iter:   80 | Time: 00:00:17 | Train Loss: 1.1618 | Average Loss: 1.8951 \n",
      "Epoch:27 | Iter:  100 | Time: 00:00:17 | Train Loss: 1.5728 | Average Loss: 1.8944 \n",
      "Epoch:27 | Iter:  120 | Time: 00:00:17 | Train Loss: 0.7948 | Average Loss: 1.8936 \n",
      "Epoch:27 | Iter:  140 | Time: 00:00:17 | Train Loss: 1.7976 | Average Loss: 1.8928 \n",
      "Epoch:27 | Iter:  160 | Time: 00:00:17 | Train Loss: 1.6777 | Average Loss: 1.8921 \n",
      "Epoch:27 | Iter:  180 | Time: 00:00:17 | Train Loss: 1.6607 | Average Loss: 1.8913 \n",
      "Epoch:27 | Iter:  200 | Time: 00:00:17 | Train Loss: 1.4738 | Average Loss: 1.8905 \n",
      "Epoch:27 | Iter:  220 | Time: 00:00:17 | Train Loss: 1.1488 | Average Loss: 1.8896 \n",
      "Epoch:27 | Iter:  240 | Time: 00:00:17 | Train Loss: 1.3885 | Average Loss: 1.8887 \n",
      "Epoch:27 | Iter:  260 | Time: 00:00:17 | Train Loss: 1.1528 | Average Loss: 1.8877 \n",
      "Accuracy: 0.395833 | Time: 00:00:00\n",
      "Epoch:28 | Iter:    0 | Time: 00:00:17 | Train Loss: 1.0611 | Average Loss: 1.8869 \n",
      "Epoch:28 | Iter:   20 | Time: 00:00:18 | Train Loss: 1.7766 | Average Loss: 1.8863 \n",
      "Epoch:28 | Iter:   40 | Time: 00:00:18 | Train Loss: 1.5491 | Average Loss: 1.8854 \n",
      "Epoch:28 | Iter:   60 | Time: 00:00:18 | Train Loss: 1.3786 | Average Loss: 1.8845 \n",
      "Epoch:28 | Iter:   80 | Time: 00:00:18 | Train Loss: 1.1461 | Average Loss: 1.8838 \n",
      "Epoch:28 | Iter:  100 | Time: 00:00:18 | Train Loss: 1.6954 | Average Loss: 1.8832 \n",
      "Epoch:28 | Iter:  120 | Time: 00:00:18 | Train Loss: 0.9295 | Average Loss: 1.8825 \n",
      "Epoch:28 | Iter:  140 | Time: 00:00:18 | Train Loss: 2.1335 | Average Loss: 1.8817 \n",
      "Epoch:28 | Iter:  160 | Time: 00:00:18 | Train Loss: 1.7341 | Average Loss: 1.8810 \n",
      "Epoch:28 | Iter:  180 | Time: 00:00:18 | Train Loss: 1.4868 | Average Loss: 1.8800 \n",
      "Epoch:28 | Iter:  200 | Time: 00:00:18 | Train Loss: 1.7308 | Average Loss: 1.8793 \n",
      "Epoch:28 | Iter:  220 | Time: 00:00:18 | Train Loss: 1.2208 | Average Loss: 1.8783 \n",
      "Epoch:28 | Iter:  240 | Time: 00:00:18 | Train Loss: 1.6556 | Average Loss: 1.8774 \n",
      "Epoch:28 | Iter:  260 | Time: 00:00:18 | Train Loss: 0.8591 | Average Loss: 1.8765 \n",
      "Accuracy: 0.333333 | Time: 00:00:00\n",
      "Epoch:29 | Iter:    0 | Time: 00:00:18 | Train Loss: 1.1638 | Average Loss: 1.8761 \n",
      "Epoch:29 | Iter:   20 | Time: 00:00:18 | Train Loss: 1.8673 | Average Loss: 1.8754 \n",
      "Epoch:29 | Iter:   40 | Time: 00:00:18 | Train Loss: 1.4887 | Average Loss: 1.8747 \n",
      "Epoch:29 | Iter:   60 | Time: 00:00:18 | Train Loss: 1.4927 | Average Loss: 1.8738 \n",
      "Epoch:29 | Iter:   80 | Time: 00:00:18 | Train Loss: 1.3499 | Average Loss: 1.8733 \n",
      "Epoch:29 | Iter:  100 | Time: 00:00:18 | Train Loss: 1.7822 | Average Loss: 1.8725 \n",
      "Epoch:29 | Iter:  120 | Time: 00:00:18 | Train Loss: 1.1708 | Average Loss: 1.8720 \n",
      "Epoch:29 | Iter:  140 | Time: 00:00:18 | Train Loss: 1.9177 | Average Loss: 1.8713 \n",
      "Epoch:29 | Iter:  160 | Time: 00:00:18 | Train Loss: 1.2928 | Average Loss: 1.8705 \n",
      "Epoch:29 | Iter:  180 | Time: 00:00:19 | Train Loss: 1.4647 | Average Loss: 1.8698 \n",
      "Epoch:29 | Iter:  200 | Time: 00:00:19 | Train Loss: 1.5744 | Average Loss: 1.8688 \n",
      "Epoch:29 | Iter:  220 | Time: 00:00:19 | Train Loss: 1.2421 | Average Loss: 1.8680 \n",
      "Epoch:29 | Iter:  240 | Time: 00:00:19 | Train Loss: 1.4066 | Average Loss: 1.8671 \n",
      "Epoch:29 | Iter:  260 | Time: 00:00:19 | Train Loss: 1.0282 | Average Loss: 1.8663 \n",
      "Accuracy: 0.375000 | Time: 00:00:00\n",
      "Epoch:30 | Iter:    0 | Time: 00:00:19 | Train Loss: 1.2260 | Average Loss: 1.8656 \n",
      "Epoch:30 | Iter:   20 | Time: 00:00:19 | Train Loss: 1.6500 | Average Loss: 1.8651 \n",
      "Epoch:30 | Iter:   40 | Time: 00:00:19 | Train Loss: 1.3522 | Average Loss: 1.8642 \n",
      "Epoch:30 | Iter:   60 | Time: 00:00:19 | Train Loss: 1.4015 | Average Loss: 1.8633 \n",
      "Epoch:30 | Iter:   80 | Time: 00:00:19 | Train Loss: 0.9997 | Average Loss: 1.8627 \n",
      "Epoch:30 | Iter:  100 | Time: 00:00:19 | Train Loss: 1.2869 | Average Loss: 1.8621 \n",
      "Epoch:30 | Iter:  120 | Time: 00:00:19 | Train Loss: 0.8758 | Average Loss: 1.8615 \n",
      "Epoch:30 | Iter:  140 | Time: 00:00:19 | Train Loss: 1.6771 | Average Loss: 1.8609 \n",
      "Epoch:30 | Iter:  160 | Time: 00:00:19 | Train Loss: 1.7334 | Average Loss: 1.8603 \n",
      "Epoch:30 | Iter:  180 | Time: 00:00:19 | Train Loss: 1.4265 | Average Loss: 1.8594 \n",
      "Epoch:30 | Iter:  200 | Time: 00:00:19 | Train Loss: 1.4168 | Average Loss: 1.8585 \n",
      "Epoch:30 | Iter:  220 | Time: 00:00:19 | Train Loss: 1.1495 | Average Loss: 1.8578 \n",
      "Epoch:30 | Iter:  240 | Time: 00:00:19 | Train Loss: 1.2943 | Average Loss: 1.8570 \n",
      "Epoch:30 | Iter:  260 | Time: 00:00:19 | Train Loss: 0.9764 | Average Loss: 1.8562 \n",
      "Accuracy: 0.354167 | Time: 00:00:00\n",
      "Epoch:31 | Iter:    0 | Time: 00:00:19 | Train Loss: 1.0355 | Average Loss: 1.8557 \n",
      "Epoch:31 | Iter:   20 | Time: 00:00:19 | Train Loss: 1.8308 | Average Loss: 1.8552 \n",
      "Epoch:31 | Iter:   40 | Time: 00:00:19 | Train Loss: 1.6195 | Average Loss: 1.8545 \n",
      "Epoch:31 | Iter:   60 | Time: 00:00:19 | Train Loss: 1.8381 | Average Loss: 1.8537 \n",
      "Epoch:31 | Iter:   80 | Time: 00:00:20 | Train Loss: 1.1343 | Average Loss: 1.8532 \n",
      "Epoch:31 | Iter:  100 | Time: 00:00:20 | Train Loss: 1.4630 | Average Loss: 1.8526 \n",
      "Epoch:31 | Iter:  120 | Time: 00:00:20 | Train Loss: 1.1467 | Average Loss: 1.8522 \n",
      "Epoch:31 | Iter:  140 | Time: 00:00:20 | Train Loss: 2.0368 | Average Loss: 1.8516 \n",
      "Epoch:31 | Iter:  160 | Time: 00:00:20 | Train Loss: 1.7520 | Average Loss: 1.8509 \n",
      "Epoch:31 | Iter:  180 | Time: 00:00:20 | Train Loss: 1.3653 | Average Loss: 1.8501 \n",
      "Epoch:31 | Iter:  200 | Time: 00:00:20 | Train Loss: 1.5562 | Average Loss: 1.8494 \n",
      "Epoch:31 | Iter:  220 | Time: 00:00:20 | Train Loss: 1.0686 | Average Loss: 1.8487 \n",
      "Epoch:31 | Iter:  240 | Time: 00:00:20 | Train Loss: 1.4300 | Average Loss: 1.8480 \n",
      "Epoch:31 | Iter:  260 | Time: 00:00:20 | Train Loss: 0.9196 | Average Loss: 1.8473 \n",
      "Accuracy: 0.354167 | Time: 00:00:00\n",
      "Epoch:32 | Iter:    0 | Time: 00:00:20 | Train Loss: 1.3761 | Average Loss: 1.8468 \n",
      "Epoch:32 | Iter:   20 | Time: 00:00:20 | Train Loss: 1.6174 | Average Loss: 1.8462 \n",
      "Epoch:32 | Iter:   40 | Time: 00:00:20 | Train Loss: 1.3999 | Average Loss: 1.8453 \n",
      "Epoch:32 | Iter:   60 | Time: 00:00:20 | Train Loss: 1.3731 | Average Loss: 1.8444 \n",
      "Epoch:32 | Iter:   80 | Time: 00:00:20 | Train Loss: 1.1454 | Average Loss: 1.8439 \n",
      "Epoch:32 | Iter:  100 | Time: 00:00:20 | Train Loss: 1.4516 | Average Loss: 1.8432 \n",
      "Epoch:32 | Iter:  120 | Time: 00:00:20 | Train Loss: 0.6856 | Average Loss: 1.8426 \n",
      "Epoch:32 | Iter:  140 | Time: 00:00:20 | Train Loss: 1.8669 | Average Loss: 1.8420 \n",
      "Epoch:32 | Iter:  160 | Time: 00:00:20 | Train Loss: 1.6801 | Average Loss: 1.8414 \n",
      "Epoch:32 | Iter:  180 | Time: 00:00:20 | Train Loss: 1.3727 | Average Loss: 1.8406 \n",
      "Epoch:32 | Iter:  200 | Time: 00:00:20 | Train Loss: 1.6417 | Average Loss: 1.8400 \n",
      "Epoch:32 | Iter:  220 | Time: 00:00:20 | Train Loss: 1.1350 | Average Loss: 1.8394 \n",
      "Epoch:32 | Iter:  240 | Time: 00:00:21 | Train Loss: 1.3134 | Average Loss: 1.8387 \n",
      "Epoch:32 | Iter:  260 | Time: 00:00:21 | Train Loss: 1.2137 | Average Loss: 1.8379 \n",
      "Accuracy: 0.354167 | Time: 00:00:00\n",
      "Epoch:33 | Iter:    0 | Time: 00:00:21 | Train Loss: 1.2481 | Average Loss: 1.8373 \n",
      "Epoch:33 | Iter:   20 | Time: 00:00:21 | Train Loss: 1.9788 | Average Loss: 1.8367 \n",
      "Epoch:33 | Iter:   40 | Time: 00:00:21 | Train Loss: 1.6242 | Average Loss: 1.8361 \n",
      "Epoch:33 | Iter:   60 | Time: 00:00:21 | Train Loss: 1.5225 | Average Loss: 1.8353 \n",
      "Epoch:33 | Iter:   80 | Time: 00:00:21 | Train Loss: 1.4835 | Average Loss: 1.8349 \n",
      "Epoch:33 | Iter:  100 | Time: 00:00:21 | Train Loss: 1.4371 | Average Loss: 1.8342 \n",
      "Epoch:33 | Iter:  120 | Time: 00:00:21 | Train Loss: 0.7250 | Average Loss: 1.8336 \n",
      "Epoch:33 | Iter:  140 | Time: 00:00:21 | Train Loss: 1.7242 | Average Loss: 1.8330 \n",
      "Epoch:33 | Iter:  160 | Time: 00:00:21 | Train Loss: 1.7639 | Average Loss: 1.8323 \n",
      "Epoch:33 | Iter:  180 | Time: 00:00:21 | Train Loss: 1.6038 | Average Loss: 1.8316 \n",
      "Epoch:33 | Iter:  200 | Time: 00:00:21 | Train Loss: 1.7015 | Average Loss: 1.8309 \n",
      "Epoch:33 | Iter:  220 | Time: 00:00:21 | Train Loss: 1.2271 | Average Loss: 1.8302 \n",
      "Epoch:33 | Iter:  240 | Time: 00:00:21 | Train Loss: 1.5520 | Average Loss: 1.8295 \n",
      "Epoch:33 | Iter:  260 | Time: 00:00:21 | Train Loss: 1.0727 | Average Loss: 1.8288 \n",
      "Accuracy: 0.333333 | Time: 00:00:00\n",
      "Epoch:34 | Iter:    0 | Time: 00:00:21 | Train Loss: 1.3416 | Average Loss: 1.8282 \n",
      "Epoch:34 | Iter:   20 | Time: 00:00:21 | Train Loss: 1.9350 | Average Loss: 1.8278 \n",
      "Epoch:34 | Iter:   40 | Time: 00:00:21 | Train Loss: 1.3714 | Average Loss: 1.8271 \n",
      "Epoch:34 | Iter:   60 | Time: 00:00:21 | Train Loss: 1.5945 | Average Loss: 1.8262 \n",
      "Epoch:34 | Iter:   80 | Time: 00:00:21 | Train Loss: 1.0731 | Average Loss: 1.8256 \n",
      "Epoch:34 | Iter:  100 | Time: 00:00:22 | Train Loss: 1.4108 | Average Loss: 1.8250 \n",
      "Epoch:34 | Iter:  120 | Time: 00:00:22 | Train Loss: 0.7785 | Average Loss: 1.8245 \n",
      "Epoch:34 | Iter:  140 | Time: 00:00:22 | Train Loss: 2.1122 | Average Loss: 1.8240 \n",
      "Epoch:34 | Iter:  160 | Time: 00:00:22 | Train Loss: 1.5339 | Average Loss: 1.8233 \n",
      "Epoch:34 | Iter:  180 | Time: 00:00:22 | Train Loss: 1.3081 | Average Loss: 1.8225 \n",
      "Epoch:34 | Iter:  200 | Time: 00:00:22 | Train Loss: 1.4603 | Average Loss: 1.8219 \n",
      "Epoch:34 | Iter:  220 | Time: 00:00:22 | Train Loss: 1.0144 | Average Loss: 1.8212 \n",
      "Epoch:34 | Iter:  240 | Time: 00:00:22 | Train Loss: 1.2962 | Average Loss: 1.8206 \n",
      "Epoch:34 | Iter:  260 | Time: 00:00:22 | Train Loss: 0.9859 | Average Loss: 1.8199 \n",
      "Accuracy: 0.291667 | Time: 00:00:00\n",
      "Epoch:35 | Iter:    0 | Time: 00:00:22 | Train Loss: 1.3416 | Average Loss: 1.8193 \n",
      "Epoch:35 | Iter:   20 | Time: 00:00:22 | Train Loss: 1.8735 | Average Loss: 1.8188 \n",
      "Epoch:35 | Iter:   40 | Time: 00:00:22 | Train Loss: 1.3227 | Average Loss: 1.8183 \n",
      "Epoch:35 | Iter:   60 | Time: 00:00:22 | Train Loss: 1.5130 | Average Loss: 1.8175 \n",
      "Epoch:35 | Iter:   80 | Time: 00:00:22 | Train Loss: 1.1778 | Average Loss: 1.8171 \n",
      "Epoch:35 | Iter:  100 | Time: 00:00:22 | Train Loss: 1.7423 | Average Loss: 1.8165 \n",
      "Epoch:35 | Iter:  120 | Time: 00:00:22 | Train Loss: 1.0192 | Average Loss: 1.8160 \n",
      "Epoch:35 | Iter:  140 | Time: 00:00:22 | Train Loss: 1.8695 | Average Loss: 1.8155 \n",
      "Epoch:35 | Iter:  160 | Time: 00:00:22 | Train Loss: 1.8244 | Average Loss: 1.8150 \n",
      "Epoch:35 | Iter:  180 | Time: 00:00:22 | Train Loss: 1.7278 | Average Loss: 1.8142 \n",
      "Epoch:35 | Iter:  200 | Time: 00:00:22 | Train Loss: 1.3950 | Average Loss: 1.8138 \n",
      "Epoch:35 | Iter:  220 | Time: 00:00:23 | Train Loss: 0.9534 | Average Loss: 1.8132 \n",
      "Epoch:35 | Iter:  240 | Time: 00:00:23 | Train Loss: 1.4282 | Average Loss: 1.8125 \n",
      "Epoch:35 | Iter:  260 | Time: 00:00:23 | Train Loss: 1.1019 | Average Loss: 1.8119 \n",
      "Accuracy: 0.354167 | Time: 00:00:00\n",
      "Epoch:36 | Iter:    0 | Time: 00:00:23 | Train Loss: 1.2808 | Average Loss: 1.8115 \n",
      "Epoch:36 | Iter:   20 | Time: 00:00:23 | Train Loss: 1.7417 | Average Loss: 1.8110 \n",
      "Epoch:36 | Iter:   40 | Time: 00:00:23 | Train Loss: 1.4348 | Average Loss: 1.8105 \n",
      "Epoch:36 | Iter:   60 | Time: 00:00:23 | Train Loss: 1.3876 | Average Loss: 1.8098 \n",
      "Epoch:36 | Iter:   80 | Time: 00:00:23 | Train Loss: 0.9007 | Average Loss: 1.8093 \n",
      "Epoch:36 | Iter:  100 | Time: 00:00:23 | Train Loss: 1.2401 | Average Loss: 1.8087 \n",
      "Epoch:36 | Iter:  120 | Time: 00:00:23 | Train Loss: 1.0180 | Average Loss: 1.8081 \n",
      "Epoch:36 | Iter:  140 | Time: 00:00:23 | Train Loss: 1.9623 | Average Loss: 1.8076 \n",
      "Epoch:36 | Iter:  160 | Time: 00:00:23 | Train Loss: 1.7381 | Average Loss: 1.8070 \n",
      "Epoch:36 | Iter:  180 | Time: 00:00:23 | Train Loss: 1.0372 | Average Loss: 1.8063 \n",
      "Epoch:36 | Iter:  200 | Time: 00:00:23 | Train Loss: 1.9718 | Average Loss: 1.8058 \n",
      "Epoch:36 | Iter:  220 | Time: 00:00:23 | Train Loss: 1.1273 | Average Loss: 1.8052 \n",
      "Epoch:36 | Iter:  240 | Time: 00:00:23 | Train Loss: 1.3213 | Average Loss: 1.8046 \n",
      "Epoch:36 | Iter:  260 | Time: 00:00:23 | Train Loss: 1.0325 | Average Loss: 1.8038 \n",
      "Accuracy: 0.312500 | Time: 00:00:00\n",
      "Epoch:37 | Iter:    0 | Time: 00:00:23 | Train Loss: 1.5029 | Average Loss: 1.8035 \n",
      "Epoch:37 | Iter:   20 | Time: 00:00:23 | Train Loss: 1.6070 | Average Loss: 1.8030 \n",
      "Epoch:37 | Iter:   40 | Time: 00:00:23 | Train Loss: 1.4525 | Average Loss: 1.8024 \n",
      "Epoch:37 | Iter:   60 | Time: 00:00:24 | Train Loss: 1.8750 | Average Loss: 1.8018 \n",
      "Epoch:37 | Iter:   80 | Time: 00:00:24 | Train Loss: 1.2648 | Average Loss: 1.8014 \n",
      "Epoch:37 | Iter:  100 | Time: 00:00:24 | Train Loss: 1.4190 | Average Loss: 1.8009 \n",
      "Epoch:37 | Iter:  120 | Time: 00:00:24 | Train Loss: 0.8915 | Average Loss: 1.8004 \n",
      "Epoch:37 | Iter:  140 | Time: 00:00:24 | Train Loss: 1.8824 | Average Loss: 1.7998 \n",
      "Epoch:37 | Iter:  160 | Time: 00:00:24 | Train Loss: 1.7112 | Average Loss: 1.7993 \n",
      "Epoch:37 | Iter:  180 | Time: 00:00:24 | Train Loss: 1.5967 | Average Loss: 1.7986 \n",
      "Epoch:37 | Iter:  200 | Time: 00:00:24 | Train Loss: 1.8030 | Average Loss: 1.7980 \n",
      "Epoch:37 | Iter:  220 | Time: 00:00:24 | Train Loss: 1.1497 | Average Loss: 1.7974 \n",
      "Epoch:37 | Iter:  240 | Time: 00:00:24 | Train Loss: 1.3263 | Average Loss: 1.7967 \n",
      "Epoch:37 | Iter:  260 | Time: 00:00:24 | Train Loss: 1.1886 | Average Loss: 1.7961 \n",
      "Accuracy: 0.291667 | Time: 00:00:00\n",
      "Epoch:38 | Iter:    0 | Time: 00:00:24 | Train Loss: 1.2088 | Average Loss: 1.7958 \n",
      "Epoch:38 | Iter:   20 | Time: 00:00:24 | Train Loss: 1.7872 | Average Loss: 1.7953 \n",
      "Epoch:38 | Iter:   40 | Time: 00:00:24 | Train Loss: 1.4648 | Average Loss: 1.7947 \n",
      "Epoch:38 | Iter:   60 | Time: 00:00:24 | Train Loss: 1.5343 | Average Loss: 1.7940 \n",
      "Epoch:38 | Iter:   80 | Time: 00:00:24 | Train Loss: 1.1401 | Average Loss: 1.7935 \n",
      "Epoch:38 | Iter:  100 | Time: 00:00:24 | Train Loss: 1.5316 | Average Loss: 1.7929 \n",
      "Epoch:38 | Iter:  120 | Time: 00:00:24 | Train Loss: 0.9618 | Average Loss: 1.7925 \n",
      "Epoch:38 | Iter:  140 | Time: 00:00:24 | Train Loss: 1.8612 | Average Loss: 1.7920 \n",
      "Epoch:38 | Iter:  160 | Time: 00:00:24 | Train Loss: 1.3410 | Average Loss: 1.7914 \n",
      "Epoch:38 | Iter:  180 | Time: 00:00:25 | Train Loss: 1.2884 | Average Loss: 1.7906 \n",
      "Epoch:38 | Iter:  200 | Time: 00:00:25 | Train Loss: 1.6181 | Average Loss: 1.7900 \n",
      "Epoch:38 | Iter:  220 | Time: 00:00:25 | Train Loss: 1.0799 | Average Loss: 1.7894 \n",
      "Epoch:38 | Iter:  240 | Time: 00:00:25 | Train Loss: 1.3862 | Average Loss: 1.7888 \n",
      "Epoch:38 | Iter:  260 | Time: 00:00:25 | Train Loss: 1.0517 | Average Loss: 1.7882 \n",
      "Accuracy: 0.333333 | Time: 00:00:00\n",
      "Epoch:39 | Iter:    0 | Time: 00:00:25 | Train Loss: 1.1720 | Average Loss: 1.7878 \n",
      "Epoch:39 | Iter:   20 | Time: 00:00:25 | Train Loss: 1.5388 | Average Loss: 1.7874 \n",
      "Epoch:39 | Iter:   40 | Time: 00:00:25 | Train Loss: 1.3415 | Average Loss: 1.7868 \n",
      "Epoch:39 | Iter:   60 | Time: 00:00:25 | Train Loss: 1.4894 | Average Loss: 1.7861 \n",
      "Epoch:39 | Iter:   80 | Time: 00:00:25 | Train Loss: 1.1355 | Average Loss: 1.7858 \n",
      "Epoch:39 | Iter:  100 | Time: 00:00:25 | Train Loss: 1.4904 | Average Loss: 1.7852 \n",
      "Epoch:39 | Iter:  120 | Time: 00:00:25 | Train Loss: 0.9681 | Average Loss: 1.7848 \n",
      "Epoch:39 | Iter:  140 | Time: 00:00:25 | Train Loss: 1.6960 | Average Loss: 1.7842 \n",
      "Epoch:39 | Iter:  160 | Time: 00:00:25 | Train Loss: 1.6129 | Average Loss: 1.7837 \n",
      "Epoch:39 | Iter:  180 | Time: 00:00:25 | Train Loss: 1.3459 | Average Loss: 1.7831 \n",
      "Epoch:39 | Iter:  200 | Time: 00:00:25 | Train Loss: 1.8085 | Average Loss: 1.7825 \n",
      "Epoch:39 | Iter:  220 | Time: 00:00:25 | Train Loss: 1.2794 | Average Loss: 1.7819 \n",
      "Epoch:39 | Iter:  240 | Time: 00:00:25 | Train Loss: 1.1186 | Average Loss: 1.7812 \n",
      "Epoch:39 | Iter:  260 | Time: 00:00:25 | Train Loss: 1.2657 | Average Loss: 1.7806 \n",
      "Accuracy: 0.333333 | Time: 00:00:00\n",
      "Epoch:40 | Iter:    0 | Time: 00:00:25 | Train Loss: 1.0929 | Average Loss: 1.7802 \n",
      "Epoch:40 | Iter:   20 | Time: 00:00:25 | Train Loss: 1.8073 | Average Loss: 1.7798 \n",
      "Epoch:40 | Iter:   40 | Time: 00:00:26 | Train Loss: 1.2429 | Average Loss: 1.7793 \n",
      "Epoch:40 | Iter:   60 | Time: 00:00:26 | Train Loss: 1.3290 | Average Loss: 1.7787 \n",
      "Epoch:40 | Iter:   80 | Time: 00:00:26 | Train Loss: 1.0590 | Average Loss: 1.7782 \n",
      "Epoch:40 | Iter:  100 | Time: 00:00:26 | Train Loss: 1.4508 | Average Loss: 1.7777 \n",
      "Epoch:40 | Iter:  120 | Time: 00:00:26 | Train Loss: 0.8160 | Average Loss: 1.7772 \n",
      "Epoch:40 | Iter:  140 | Time: 00:00:26 | Train Loss: 1.5320 | Average Loss: 1.7766 \n",
      "Epoch:40 | Iter:  160 | Time: 00:00:26 | Train Loss: 1.5864 | Average Loss: 1.7760 \n",
      "Epoch:40 | Iter:  180 | Time: 00:00:26 | Train Loss: 1.3338 | Average Loss: 1.7754 \n",
      "Epoch:40 | Iter:  200 | Time: 00:00:26 | Train Loss: 1.6081 | Average Loss: 1.7749 \n",
      "Epoch:40 | Iter:  220 | Time: 00:00:26 | Train Loss: 1.0936 | Average Loss: 1.7744 \n",
      "Epoch:40 | Iter:  240 | Time: 00:00:26 | Train Loss: 1.8203 | Average Loss: 1.7739 \n",
      "Epoch:40 | Iter:  260 | Time: 00:00:26 | Train Loss: 1.1254 | Average Loss: 1.7733 \n",
      "Accuracy: 0.354167 | Time: 00:00:00\n",
      "Epoch:41 | Iter:    0 | Time: 00:00:26 | Train Loss: 1.0218 | Average Loss: 1.7728 \n",
      "Epoch:41 | Iter:   20 | Time: 00:00:26 | Train Loss: 1.5374 | Average Loss: 1.7724 \n",
      "Epoch:41 | Iter:   40 | Time: 00:00:26 | Train Loss: 1.2819 | Average Loss: 1.7717 \n",
      "Epoch:41 | Iter:   60 | Time: 00:00:26 | Train Loss: 1.3515 | Average Loss: 1.7711 \n",
      "Epoch:41 | Iter:   80 | Time: 00:00:26 | Train Loss: 1.0917 | Average Loss: 1.7706 \n",
      "Epoch:41 | Iter:  100 | Time: 00:00:26 | Train Loss: 1.7864 | Average Loss: 1.7702 \n",
      "Epoch:41 | Iter:  120 | Time: 00:00:26 | Train Loss: 1.0041 | Average Loss: 1.7697 \n",
      "Epoch:41 | Iter:  140 | Time: 00:00:27 | Train Loss: 1.9874 | Average Loss: 1.7693 \n",
      "Epoch:41 | Iter:  160 | Time: 00:00:27 | Train Loss: 1.4745 | Average Loss: 1.7687 \n",
      "Epoch:41 | Iter:  180 | Time: 00:00:27 | Train Loss: 1.4160 | Average Loss: 1.7682 \n",
      "Epoch:41 | Iter:  200 | Time: 00:00:27 | Train Loss: 1.5079 | Average Loss: 1.7676 \n",
      "Epoch:41 | Iter:  220 | Time: 00:00:27 | Train Loss: 1.0581 | Average Loss: 1.7671 \n",
      "Epoch:41 | Iter:  240 | Time: 00:00:27 | Train Loss: 1.4841 | Average Loss: 1.7666 \n",
      "Epoch:41 | Iter:  260 | Time: 00:00:27 | Train Loss: 0.9854 | Average Loss: 1.7662 \n",
      "Accuracy: 0.270833 | Time: 00:00:00\n",
      "Epoch:42 | Iter:    0 | Time: 00:00:27 | Train Loss: 0.8530 | Average Loss: 1.7657 \n",
      "Epoch:42 | Iter:   20 | Time: 00:00:27 | Train Loss: 1.7473 | Average Loss: 1.7653 \n",
      "Epoch:42 | Iter:   40 | Time: 00:00:27 | Train Loss: 1.2110 | Average Loss: 1.7648 \n",
      "Epoch:42 | Iter:   60 | Time: 00:00:27 | Train Loss: 1.3515 | Average Loss: 1.7642 \n",
      "Epoch:42 | Iter:   80 | Time: 00:00:27 | Train Loss: 1.0431 | Average Loss: 1.7639 \n",
      "Epoch:42 | Iter:  100 | Time: 00:00:27 | Train Loss: 1.2972 | Average Loss: 1.7632 \n",
      "Epoch:42 | Iter:  120 | Time: 00:00:27 | Train Loss: 0.8954 | Average Loss: 1.7629 \n",
      "Epoch:42 | Iter:  140 | Time: 00:00:27 | Train Loss: 1.8289 | Average Loss: 1.7624 \n",
      "Epoch:42 | Iter:  160 | Time: 00:00:27 | Train Loss: 1.5495 | Average Loss: 1.7619 \n",
      "Epoch:42 | Iter:  180 | Time: 00:00:27 | Train Loss: 1.3995 | Average Loss: 1.7612 \n",
      "Epoch:42 | Iter:  200 | Time: 00:00:27 | Train Loss: 1.8592 | Average Loss: 1.7608 \n",
      "Epoch:42 | Iter:  220 | Time: 00:00:27 | Train Loss: 1.3534 | Average Loss: 1.7603 \n",
      "Epoch:42 | Iter:  240 | Time: 00:00:27 | Train Loss: 1.3217 | Average Loss: 1.7598 \n",
      "Epoch:42 | Iter:  260 | Time: 00:00:27 | Train Loss: 1.0558 | Average Loss: 1.7591 \n",
      "Accuracy: 0.312500 | Time: 00:00:00\n",
      "Epoch:43 | Iter:    0 | Time: 00:00:28 | Train Loss: 1.1457 | Average Loss: 1.7588 \n",
      "Epoch:43 | Iter:   20 | Time: 00:00:28 | Train Loss: 1.6005 | Average Loss: 1.7583 \n",
      "Epoch:43 | Iter:   40 | Time: 00:00:28 | Train Loss: 1.3799 | Average Loss: 1.7578 \n",
      "Epoch:43 | Iter:   60 | Time: 00:00:28 | Train Loss: 1.4004 | Average Loss: 1.7571 \n",
      "Epoch:43 | Iter:   80 | Time: 00:00:28 | Train Loss: 1.1125 | Average Loss: 1.7567 \n",
      "Epoch:43 | Iter:  100 | Time: 00:00:28 | Train Loss: 1.4071 | Average Loss: 1.7563 \n",
      "Epoch:43 | Iter:  120 | Time: 00:00:28 | Train Loss: 0.6993 | Average Loss: 1.7559 \n",
      "Epoch:43 | Iter:  140 | Time: 00:00:28 | Train Loss: 1.7733 | Average Loss: 1.7555 \n",
      "Epoch:43 | Iter:  160 | Time: 00:00:28 | Train Loss: 1.7330 | Average Loss: 1.7551 \n",
      "Epoch:43 | Iter:  180 | Time: 00:00:28 | Train Loss: 1.5160 | Average Loss: 1.7544 \n",
      "Epoch:43 | Iter:  200 | Time: 00:00:28 | Train Loss: 1.5993 | Average Loss: 1.7539 \n",
      "Epoch:43 | Iter:  220 | Time: 00:00:28 | Train Loss: 1.0951 | Average Loss: 1.7533 \n",
      "Epoch:43 | Iter:  240 | Time: 00:00:28 | Train Loss: 1.1027 | Average Loss: 1.7527 \n",
      "Epoch:43 | Iter:  260 | Time: 00:00:28 | Train Loss: 1.1022 | Average Loss: 1.7522 \n",
      "Accuracy: 0.312500 | Time: 00:00:00\n",
      "Epoch:44 | Iter:    0 | Time: 00:00:28 | Train Loss: 0.9702 | Average Loss: 1.7517 \n",
      "Epoch:44 | Iter:   20 | Time: 00:00:28 | Train Loss: 1.3367 | Average Loss: 1.7512 \n",
      "Epoch:44 | Iter:   40 | Time: 00:00:28 | Train Loss: 1.1104 | Average Loss: 1.7507 \n",
      "Epoch:44 | Iter:   60 | Time: 00:00:28 | Train Loss: 1.1664 | Average Loss: 1.7501 \n",
      "Epoch:44 | Iter:   80 | Time: 00:00:28 | Train Loss: 0.9057 | Average Loss: 1.7498 \n",
      "Epoch:44 | Iter:  100 | Time: 00:00:29 | Train Loss: 1.4924 | Average Loss: 1.7495 \n",
      "Epoch:44 | Iter:  120 | Time: 00:00:29 | Train Loss: 0.7649 | Average Loss: 1.7490 \n",
      "Epoch:44 | Iter:  140 | Time: 00:00:29 | Train Loss: 1.8508 | Average Loss: 1.7485 \n",
      "Epoch:44 | Iter:  160 | Time: 00:00:29 | Train Loss: 1.6717 | Average Loss: 1.7482 \n",
      "Epoch:44 | Iter:  180 | Time: 00:00:29 | Train Loss: 1.2770 | Average Loss: 1.7477 \n",
      "Epoch:44 | Iter:  200 | Time: 00:00:29 | Train Loss: 1.4792 | Average Loss: 1.7471 \n",
      "Epoch:44 | Iter:  220 | Time: 00:00:29 | Train Loss: 1.1082 | Average Loss: 1.7467 \n",
      "Epoch:44 | Iter:  240 | Time: 00:00:29 | Train Loss: 1.0692 | Average Loss: 1.7461 \n",
      "Epoch:44 | Iter:  260 | Time: 00:00:29 | Train Loss: 1.1513 | Average Loss: 1.7456 \n",
      "Accuracy: 0.312500 | Time: 00:00:00\n",
      "Epoch:45 | Iter:    0 | Time: 00:00:29 | Train Loss: 1.1563 | Average Loss: 1.7451 \n",
      "Epoch:45 | Iter:   20 | Time: 00:00:29 | Train Loss: 1.7234 | Average Loss: 1.7448 \n",
      "Epoch:45 | Iter:   40 | Time: 00:00:29 | Train Loss: 1.2608 | Average Loss: 1.7442 \n",
      "Epoch:45 | Iter:   60 | Time: 00:00:29 | Train Loss: 1.5094 | Average Loss: 1.7436 \n",
      "Epoch:45 | Iter:   80 | Time: 00:00:29 | Train Loss: 1.4267 | Average Loss: 1.7432 \n",
      "Epoch:45 | Iter:  100 | Time: 00:00:29 | Train Loss: 1.6459 | Average Loss: 1.7428 \n",
      "Epoch:45 | Iter:  120 | Time: 00:00:29 | Train Loss: 0.8636 | Average Loss: 1.7425 \n",
      "Epoch:45 | Iter:  140 | Time: 00:00:29 | Train Loss: 1.6909 | Average Loss: 1.7421 \n",
      "Epoch:45 | Iter:  160 | Time: 00:00:29 | Train Loss: 1.4773 | Average Loss: 1.7416 \n",
      "Epoch:45 | Iter:  180 | Time: 00:00:29 | Train Loss: 1.3617 | Average Loss: 1.7411 \n",
      "Epoch:45 | Iter:  200 | Time: 00:00:29 | Train Loss: 1.5370 | Average Loss: 1.7406 \n",
      "Epoch:45 | Iter:  220 | Time: 00:00:29 | Train Loss: 1.0015 | Average Loss: 1.7401 \n",
      "Epoch:45 | Iter:  240 | Time: 00:00:30 | Train Loss: 1.0409 | Average Loss: 1.7394 \n",
      "Epoch:45 | Iter:  260 | Time: 00:00:30 | Train Loss: 0.9469 | Average Loss: 1.7388 \n",
      "Accuracy: 0.312500 | Time: 00:00:00\n",
      "Epoch:46 | Iter:    0 | Time: 00:00:30 | Train Loss: 1.0426 | Average Loss: 1.7384 \n",
      "Epoch:46 | Iter:   20 | Time: 00:00:30 | Train Loss: 1.6418 | Average Loss: 1.7380 \n",
      "Epoch:46 | Iter:   40 | Time: 00:00:30 | Train Loss: 1.4075 | Average Loss: 1.7375 \n",
      "Epoch:46 | Iter:   60 | Time: 00:00:30 | Train Loss: 1.2056 | Average Loss: 1.7368 \n",
      "Epoch:46 | Iter:   80 | Time: 00:00:30 | Train Loss: 1.1676 | Average Loss: 1.7365 \n",
      "Epoch:46 | Iter:  100 | Time: 00:00:30 | Train Loss: 1.3699 | Average Loss: 1.7360 \n",
      "Epoch:46 | Iter:  120 | Time: 00:00:30 | Train Loss: 0.8738 | Average Loss: 1.7356 \n",
      "Epoch:46 | Iter:  140 | Time: 00:00:30 | Train Loss: 1.7953 | Average Loss: 1.7352 \n",
      "Epoch:46 | Iter:  160 | Time: 00:00:30 | Train Loss: 1.6850 | Average Loss: 1.7348 \n",
      "Epoch:46 | Iter:  180 | Time: 00:00:30 | Train Loss: 1.3464 | Average Loss: 1.7343 \n",
      "Epoch:46 | Iter:  200 | Time: 00:00:30 | Train Loss: 1.7433 | Average Loss: 1.7339 \n",
      "Epoch:46 | Iter:  220 | Time: 00:00:30 | Train Loss: 0.9684 | Average Loss: 1.7335 \n",
      "Epoch:46 | Iter:  240 | Time: 00:00:30 | Train Loss: 1.1248 | Average Loss: 1.7330 \n",
      "Epoch:46 | Iter:  260 | Time: 00:00:30 | Train Loss: 0.8620 | Average Loss: 1.7325 \n",
      "Accuracy: 0.354167 | Time: 00:00:00\n",
      "Epoch:47 | Iter:    0 | Time: 00:00:30 | Train Loss: 1.0139 | Average Loss: 1.7322 \n",
      "Epoch:47 | Iter:   20 | Time: 00:00:30 | Train Loss: 1.8494 | Average Loss: 1.7319 \n",
      "Epoch:47 | Iter:   40 | Time: 00:00:30 | Train Loss: 1.3205 | Average Loss: 1.7315 \n",
      "Epoch:47 | Iter:   60 | Time: 00:00:30 | Train Loss: 1.5205 | Average Loss: 1.7309 \n",
      "Epoch:47 | Iter:   80 | Time: 00:00:31 | Train Loss: 1.0479 | Average Loss: 1.7306 \n",
      "Epoch:47 | Iter:  100 | Time: 00:00:31 | Train Loss: 1.8930 | Average Loss: 1.7302 \n",
      "Epoch:47 | Iter:  120 | Time: 00:00:31 | Train Loss: 0.7878 | Average Loss: 1.7298 \n",
      "Epoch:47 | Iter:  140 | Time: 00:00:31 | Train Loss: 1.5117 | Average Loss: 1.7294 \n",
      "Epoch:47 | Iter:  160 | Time: 00:00:31 | Train Loss: 1.5929 | Average Loss: 1.7291 \n",
      "Epoch:47 | Iter:  180 | Time: 00:00:31 | Train Loss: 1.1077 | Average Loss: 1.7287 \n",
      "Epoch:47 | Iter:  200 | Time: 00:00:31 | Train Loss: 1.8627 | Average Loss: 1.7281 \n",
      "Epoch:47 | Iter:  220 | Time: 00:00:31 | Train Loss: 1.2492 | Average Loss: 1.7277 \n",
      "Epoch:47 | Iter:  240 | Time: 00:00:31 | Train Loss: 1.0921 | Average Loss: 1.7272 \n",
      "Epoch:47 | Iter:  260 | Time: 00:00:31 | Train Loss: 1.1064 | Average Loss: 1.7267 \n",
      "Accuracy: 0.333333 | Time: 00:00:00\n",
      "Epoch:48 | Iter:    0 | Time: 00:00:31 | Train Loss: 0.9864 | Average Loss: 1.7264 \n",
      "Epoch:48 | Iter:   20 | Time: 00:00:31 | Train Loss: 1.7120 | Average Loss: 1.7261 \n",
      "Epoch:48 | Iter:   40 | Time: 00:00:31 | Train Loss: 1.4424 | Average Loss: 1.7257 \n",
      "Epoch:48 | Iter:   60 | Time: 00:00:31 | Train Loss: 1.4194 | Average Loss: 1.7251 \n",
      "Epoch:48 | Iter:   80 | Time: 00:00:31 | Train Loss: 0.8587 | Average Loss: 1.7249 \n",
      "Epoch:48 | Iter:  100 | Time: 00:00:31 | Train Loss: 1.4755 | Average Loss: 1.7246 \n",
      "Epoch:48 | Iter:  120 | Time: 00:00:31 | Train Loss: 0.8418 | Average Loss: 1.7242 \n",
      "Epoch:48 | Iter:  140 | Time: 00:00:31 | Train Loss: 1.6386 | Average Loss: 1.7238 \n",
      "Epoch:48 | Iter:  160 | Time: 00:00:31 | Train Loss: 1.5638 | Average Loss: 1.7234 \n",
      "Epoch:48 | Iter:  180 | Time: 00:00:31 | Train Loss: 1.2467 | Average Loss: 1.7229 \n",
      "Epoch:48 | Iter:  200 | Time: 00:00:32 | Train Loss: 1.7983 | Average Loss: 1.7225 \n",
      "Epoch:48 | Iter:  220 | Time: 00:00:32 | Train Loss: 1.2164 | Average Loss: 1.7221 \n",
      "Epoch:48 | Iter:  240 | Time: 00:00:32 | Train Loss: 1.2929 | Average Loss: 1.7216 \n",
      "Epoch:48 | Iter:  260 | Time: 00:00:32 | Train Loss: 0.9884 | Average Loss: 1.7211 \n",
      "Accuracy: 0.312500 | Time: 00:00:00\n",
      "Epoch:49 | Iter:    0 | Time: 00:00:32 | Train Loss: 1.0739 | Average Loss: 1.7208 \n",
      "Epoch:49 | Iter:   20 | Time: 00:00:32 | Train Loss: 1.7135 | Average Loss: 1.7204 \n",
      "Epoch:49 | Iter:   40 | Time: 00:00:32 | Train Loss: 1.3602 | Average Loss: 1.7200 \n",
      "Epoch:49 | Iter:   60 | Time: 00:00:32 | Train Loss: 1.5247 | Average Loss: 1.7195 \n",
      "Epoch:49 | Iter:   80 | Time: 00:00:32 | Train Loss: 1.3216 | Average Loss: 1.7192 \n",
      "Epoch:49 | Iter:  100 | Time: 00:00:32 | Train Loss: 1.6977 | Average Loss: 1.7189 \n",
      "Epoch:49 | Iter:  120 | Time: 00:00:32 | Train Loss: 1.3428 | Average Loss: 1.7186 \n",
      "Epoch:49 | Iter:  140 | Time: 00:00:32 | Train Loss: 1.9741 | Average Loss: 1.7182 \n",
      "Epoch:49 | Iter:  160 | Time: 00:00:32 | Train Loss: 1.9163 | Average Loss: 1.7178 \n",
      "Epoch:49 | Iter:  180 | Time: 00:00:32 | Train Loss: 1.1297 | Average Loss: 1.7173 \n",
      "Epoch:49 | Iter:  200 | Time: 00:00:32 | Train Loss: 1.3035 | Average Loss: 1.7168 \n",
      "Epoch:49 | Iter:  220 | Time: 00:00:32 | Train Loss: 0.9097 | Average Loss: 1.7163 \n",
      "Epoch:49 | Iter:  240 | Time: 00:00:32 | Train Loss: 1.2732 | Average Loss: 1.7159 \n",
      "Epoch:49 | Iter:  260 | Time: 00:00:32 | Train Loss: 0.9682 | Average Loss: 1.7154 \n",
      "Accuracy: 0.312500 | Time: 00:00:00\n",
      "Epoch:50 | Iter:    0 | Time: 00:00:32 | Train Loss: 1.0959 | Average Loss: 1.7150 \n",
      "Epoch:50 | Iter:   20 | Time: 00:00:32 | Train Loss: 1.2593 | Average Loss: 1.7147 \n",
      "Epoch:50 | Iter:   40 | Time: 00:00:33 | Train Loss: 1.3353 | Average Loss: 1.7142 \n",
      "Epoch:50 | Iter:   60 | Time: 00:00:33 | Train Loss: 1.5419 | Average Loss: 1.7138 \n",
      "Epoch:50 | Iter:   80 | Time: 00:00:33 | Train Loss: 0.9761 | Average Loss: 1.7135 \n",
      "Epoch:50 | Iter:  100 | Time: 00:00:33 | Train Loss: 1.2023 | Average Loss: 1.7130 \n",
      "Epoch:50 | Iter:  120 | Time: 00:00:33 | Train Loss: 0.7608 | Average Loss: 1.7127 \n",
      "Epoch:50 | Iter:  140 | Time: 00:00:33 | Train Loss: 1.8242 | Average Loss: 1.7124 \n",
      "Epoch:50 | Iter:  160 | Time: 00:00:33 | Train Loss: 1.9361 | Average Loss: 1.7120 \n",
      "Epoch:50 | Iter:  180 | Time: 00:00:33 | Train Loss: 1.3041 | Average Loss: 1.7116 \n",
      "Epoch:50 | Iter:  200 | Time: 00:00:33 | Train Loss: 1.5338 | Average Loss: 1.7111 \n",
      "Epoch:50 | Iter:  220 | Time: 00:00:33 | Train Loss: 1.0065 | Average Loss: 1.7109 \n",
      "Epoch:50 | Iter:  240 | Time: 00:00:33 | Train Loss: 1.0575 | Average Loss: 1.7103 \n",
      "Epoch:50 | Iter:  260 | Time: 00:00:33 | Train Loss: 0.9526 | Average Loss: 1.7099 \n",
      "Accuracy: 0.291667 | Time: 00:00:00\n",
      "Epoch:51 | Iter:    0 | Time: 00:00:33 | Train Loss: 0.9984 | Average Loss: 1.7097 \n",
      "Epoch:51 | Iter:   20 | Time: 00:00:33 | Train Loss: 1.5044 | Average Loss: 1.7093 \n",
      "Epoch:51 | Iter:   40 | Time: 00:00:33 | Train Loss: 1.2876 | Average Loss: 1.7088 \n",
      "Epoch:51 | Iter:   60 | Time: 00:00:33 | Train Loss: 1.2543 | Average Loss: 1.7083 \n",
      "Epoch:51 | Iter:   80 | Time: 00:00:33 | Train Loss: 0.9533 | Average Loss: 1.7079 \n",
      "Epoch:51 | Iter:  100 | Time: 00:00:33 | Train Loss: 1.1775 | Average Loss: 1.7076 \n",
      "Epoch:51 | Iter:  120 | Time: 00:00:33 | Train Loss: 0.8487 | Average Loss: 1.7072 \n",
      "Epoch:51 | Iter:  140 | Time: 00:00:33 | Train Loss: 1.8078 | Average Loss: 1.7068 \n",
      "Epoch:51 | Iter:  160 | Time: 00:00:33 | Train Loss: 1.1074 | Average Loss: 1.7064 \n",
      "Epoch:51 | Iter:  180 | Time: 00:00:34 | Train Loss: 1.1524 | Average Loss: 1.7059 \n",
      "Epoch:51 | Iter:  200 | Time: 00:00:34 | Train Loss: 1.5669 | Average Loss: 1.7055 \n",
      "Epoch:51 | Iter:  220 | Time: 00:00:34 | Train Loss: 0.9704 | Average Loss: 1.7050 \n",
      "Epoch:51 | Iter:  240 | Time: 00:00:34 | Train Loss: 1.1690 | Average Loss: 1.7045 \n",
      "Epoch:51 | Iter:  260 | Time: 00:00:34 | Train Loss: 0.7534 | Average Loss: 1.7041 \n",
      "Accuracy: 0.291667 | Time: 00:00:00\n",
      "Epoch:52 | Iter:    0 | Time: 00:00:34 | Train Loss: 1.1895 | Average Loss: 1.7038 \n",
      "Epoch:52 | Iter:   20 | Time: 00:00:34 | Train Loss: 1.4348 | Average Loss: 1.7034 \n",
      "Epoch:52 | Iter:   40 | Time: 00:00:34 | Train Loss: 1.3782 | Average Loss: 1.7030 \n",
      "Epoch:52 | Iter:   60 | Time: 00:00:34 | Train Loss: 1.6256 | Average Loss: 1.7025 \n",
      "Epoch:52 | Iter:   80 | Time: 00:00:34 | Train Loss: 0.8032 | Average Loss: 1.7020 \n",
      "Epoch:52 | Iter:  100 | Time: 00:00:34 | Train Loss: 1.6345 | Average Loss: 1.7017 \n",
      "Epoch:52 | Iter:  120 | Time: 00:00:34 | Train Loss: 0.9700 | Average Loss: 1.7013 \n",
      "Epoch:52 | Iter:  140 | Time: 00:00:34 | Train Loss: 1.6779 | Average Loss: 1.7009 \n",
      "Epoch:52 | Iter:  160 | Time: 00:00:34 | Train Loss: 1.7329 | Average Loss: 1.7005 \n",
      "Epoch:52 | Iter:  180 | Time: 00:00:34 | Train Loss: 1.1017 | Average Loss: 1.7001 \n",
      "Epoch:52 | Iter:  200 | Time: 00:00:34 | Train Loss: 1.4580 | Average Loss: 1.6997 \n",
      "Epoch:52 | Iter:  220 | Time: 00:00:34 | Train Loss: 0.9490 | Average Loss: 1.6992 \n",
      "Epoch:52 | Iter:  240 | Time: 00:00:34 | Train Loss: 1.1558 | Average Loss: 1.6987 \n",
      "Epoch:52 | Iter:  260 | Time: 00:00:34 | Train Loss: 0.8214 | Average Loss: 1.6982 \n",
      "Accuracy: 0.333333 | Time: 00:00:00\n",
      "Epoch:53 | Iter:    0 | Time: 00:00:34 | Train Loss: 0.9303 | Average Loss: 1.6980 \n",
      "Epoch:53 | Iter:   20 | Time: 00:00:34 | Train Loss: 1.7011 | Average Loss: 1.6976 \n",
      "Epoch:53 | Iter:   40 | Time: 00:00:35 | Train Loss: 1.1951 | Average Loss: 1.6972 \n",
      "Epoch:53 | Iter:   60 | Time: 00:00:35 | Train Loss: 1.4546 | Average Loss: 1.6968 \n",
      "Epoch:53 | Iter:   80 | Time: 00:00:35 | Train Loss: 0.8924 | Average Loss: 1.6964 \n",
      "Epoch:53 | Iter:  100 | Time: 00:00:35 | Train Loss: 1.4418 | Average Loss: 1.6960 \n",
      "Epoch:53 | Iter:  120 | Time: 00:00:35 | Train Loss: 0.8015 | Average Loss: 1.6956 \n",
      "Epoch:53 | Iter:  140 | Time: 00:00:35 | Train Loss: 1.7640 | Average Loss: 1.6953 \n",
      "Epoch:53 | Iter:  160 | Time: 00:00:35 | Train Loss: 1.3831 | Average Loss: 1.6949 \n",
      "Epoch:53 | Iter:  180 | Time: 00:00:35 | Train Loss: 1.1302 | Average Loss: 1.6945 \n",
      "Epoch:53 | Iter:  200 | Time: 00:00:35 | Train Loss: 1.5135 | Average Loss: 1.6942 \n",
      "Epoch:53 | Iter:  220 | Time: 00:00:35 | Train Loss: 1.0533 | Average Loss: 1.6936 \n",
      "Epoch:53 | Iter:  240 | Time: 00:00:35 | Train Loss: 1.1402 | Average Loss: 1.6932 \n",
      "Epoch:53 | Iter:  260 | Time: 00:00:35 | Train Loss: 0.8997 | Average Loss: 1.6926 \n",
      "Accuracy: 0.312500 | Time: 00:00:00\n",
      "Epoch:54 | Iter:    0 | Time: 00:00:35 | Train Loss: 0.9813 | Average Loss: 1.6923 \n",
      "Epoch:54 | Iter:   20 | Time: 00:00:35 | Train Loss: 1.6040 | Average Loss: 1.6920 \n",
      "Epoch:54 | Iter:   40 | Time: 00:00:35 | Train Loss: 1.4603 | Average Loss: 1.6914 \n",
      "Epoch:54 | Iter:   60 | Time: 00:00:35 | Train Loss: 1.5194 | Average Loss: 1.6910 \n",
      "Epoch:54 | Iter:   80 | Time: 00:00:35 | Train Loss: 0.9196 | Average Loss: 1.6908 \n",
      "Epoch:54 | Iter:  100 | Time: 00:00:35 | Train Loss: 1.5215 | Average Loss: 1.6905 \n",
      "Epoch:54 | Iter:  120 | Time: 00:00:35 | Train Loss: 0.9660 | Average Loss: 1.6903 \n",
      "Epoch:54 | Iter:  140 | Time: 00:00:35 | Train Loss: 1.6488 | Average Loss: 1.6899 \n",
      "Epoch:54 | Iter:  160 | Time: 00:00:36 | Train Loss: 1.3462 | Average Loss: 1.6895 \n",
      "Epoch:54 | Iter:  180 | Time: 00:00:36 | Train Loss: 1.1169 | Average Loss: 1.6891 \n",
      "Epoch:54 | Iter:  200 | Time: 00:00:36 | Train Loss: 1.3264 | Average Loss: 1.6887 \n",
      "Epoch:54 | Iter:  220 | Time: 00:00:36 | Train Loss: 1.1090 | Average Loss: 1.6883 \n",
      "Epoch:54 | Iter:  240 | Time: 00:00:36 | Train Loss: 1.0485 | Average Loss: 1.6878 \n",
      "Epoch:54 | Iter:  260 | Time: 00:00:36 | Train Loss: 0.9360 | Average Loss: 1.6873 \n",
      "Accuracy: 0.354167 | Time: 00:00:00\n",
      "Epoch:55 | Iter:    0 | Time: 00:00:36 | Train Loss: 0.9569 | Average Loss: 1.6870 \n",
      "Epoch:55 | Iter:   20 | Time: 00:00:36 | Train Loss: 1.5445 | Average Loss: 1.6867 \n",
      "Epoch:55 | Iter:   40 | Time: 00:00:36 | Train Loss: 1.1553 | Average Loss: 1.6862 \n",
      "Epoch:55 | Iter:   60 | Time: 00:00:36 | Train Loss: 1.3291 | Average Loss: 1.6858 \n",
      "Epoch:55 | Iter:   80 | Time: 00:00:36 | Train Loss: 0.9302 | Average Loss: 1.6855 \n",
      "Epoch:55 | Iter:  100 | Time: 00:00:36 | Train Loss: 1.5451 | Average Loss: 1.6851 \n",
      "Epoch:55 | Iter:  120 | Time: 00:00:36 | Train Loss: 0.6875 | Average Loss: 1.6849 \n",
      "Epoch:55 | Iter:  140 | Time: 00:00:36 | Train Loss: 1.3982 | Average Loss: 1.6845 \n",
      "Epoch:55 | Iter:  160 | Time: 00:00:36 | Train Loss: 1.6232 | Average Loss: 1.6842 \n",
      "Epoch:55 | Iter:  180 | Time: 00:00:36 | Train Loss: 1.3386 | Average Loss: 1.6838 \n",
      "Epoch:55 | Iter:  200 | Time: 00:00:36 | Train Loss: 1.1701 | Average Loss: 1.6833 \n",
      "Epoch:55 | Iter:  220 | Time: 00:00:36 | Train Loss: 1.2043 | Average Loss: 1.6829 \n",
      "Epoch:55 | Iter:  240 | Time: 00:00:36 | Train Loss: 1.0252 | Average Loss: 1.6824 \n",
      "Epoch:55 | Iter:  260 | Time: 00:00:36 | Train Loss: 1.1090 | Average Loss: 1.6821 \n",
      "Accuracy: 0.312500 | Time: 00:00:00\n",
      "Epoch:56 | Iter:    0 | Time: 00:00:36 | Train Loss: 1.0766 | Average Loss: 1.6818 \n",
      "Epoch:56 | Iter:   20 | Time: 00:00:36 | Train Loss: 1.8156 | Average Loss: 1.6814 \n",
      "Epoch:56 | Iter:   40 | Time: 00:00:37 | Train Loss: 1.3713 | Average Loss: 1.6810 \n",
      "Epoch:56 | Iter:   60 | Time: 00:00:37 | Train Loss: 1.3479 | Average Loss: 1.6806 \n",
      "Epoch:56 | Iter:   80 | Time: 00:00:37 | Train Loss: 1.2126 | Average Loss: 1.6803 \n",
      "Epoch:56 | Iter:  100 | Time: 00:00:37 | Train Loss: 1.3263 | Average Loss: 1.6799 \n",
      "Epoch:56 | Iter:  120 | Time: 00:00:37 | Train Loss: 1.2175 | Average Loss: 1.6797 \n",
      "Epoch:56 | Iter:  140 | Time: 00:00:37 | Train Loss: 1.5793 | Average Loss: 1.6795 \n",
      "Epoch:56 | Iter:  160 | Time: 00:00:37 | Train Loss: 1.6240 | Average Loss: 1.6791 \n",
      "Epoch:56 | Iter:  180 | Time: 00:00:37 | Train Loss: 1.1475 | Average Loss: 1.6786 \n",
      "Epoch:56 | Iter:  200 | Time: 00:00:37 | Train Loss: 1.3669 | Average Loss: 1.6782 \n",
      "Epoch:56 | Iter:  220 | Time: 00:00:37 | Train Loss: 1.1144 | Average Loss: 1.6778 \n",
      "Epoch:56 | Iter:  240 | Time: 00:00:37 | Train Loss: 1.1918 | Average Loss: 1.6773 \n",
      "Epoch:56 | Iter:  260 | Time: 00:00:37 | Train Loss: 0.8949 | Average Loss: 1.6770 \n",
      "Accuracy: 0.312500 | Time: 00:00:00\n",
      "Epoch:57 | Iter:    0 | Time: 00:00:37 | Train Loss: 1.0704 | Average Loss: 1.6767 \n",
      "Epoch:57 | Iter:   20 | Time: 00:00:37 | Train Loss: 1.6303 | Average Loss: 1.6764 \n",
      "Epoch:57 | Iter:   40 | Time: 00:00:37 | Train Loss: 1.2960 | Average Loss: 1.6760 \n",
      "Epoch:57 | Iter:   60 | Time: 00:00:37 | Train Loss: 1.5849 | Average Loss: 1.6756 \n",
      "Epoch:57 | Iter:   80 | Time: 00:00:37 | Train Loss: 0.8514 | Average Loss: 1.6753 \n",
      "Epoch:57 | Iter:  100 | Time: 00:00:37 | Train Loss: 1.2949 | Average Loss: 1.6750 \n",
      "Epoch:57 | Iter:  120 | Time: 00:00:37 | Train Loss: 0.6585 | Average Loss: 1.6748 \n",
      "Epoch:57 | Iter:  140 | Time: 00:00:37 | Train Loss: 1.7118 | Average Loss: 1.6744 \n",
      "Epoch:57 | Iter:  160 | Time: 00:00:37 | Train Loss: 1.7597 | Average Loss: 1.6741 \n",
      "Epoch:57 | Iter:  180 | Time: 00:00:38 | Train Loss: 1.1605 | Average Loss: 1.6737 \n",
      "Epoch:57 | Iter:  200 | Time: 00:00:38 | Train Loss: 1.5911 | Average Loss: 1.6734 \n",
      "Epoch:57 | Iter:  220 | Time: 00:00:38 | Train Loss: 1.0198 | Average Loss: 1.6730 \n",
      "Epoch:57 | Iter:  240 | Time: 00:00:38 | Train Loss: 1.1192 | Average Loss: 1.6726 \n",
      "Epoch:57 | Iter:  260 | Time: 00:00:38 | Train Loss: 0.8473 | Average Loss: 1.6722 \n",
      "Accuracy: 0.312500 | Time: 00:00:00\n",
      "Epoch:58 | Iter:    0 | Time: 00:00:38 | Train Loss: 1.1160 | Average Loss: 1.6720 \n",
      "Epoch:58 | Iter:   20 | Time: 00:00:38 | Train Loss: 1.7157 | Average Loss: 1.6716 \n",
      "Epoch:58 | Iter:   40 | Time: 00:00:38 | Train Loss: 1.0929 | Average Loss: 1.6712 \n",
      "Epoch:58 | Iter:   60 | Time: 00:00:38 | Train Loss: 1.2132 | Average Loss: 1.6708 \n",
      "Epoch:58 | Iter:   80 | Time: 00:00:38 | Train Loss: 1.0438 | Average Loss: 1.6706 \n",
      "Epoch:58 | Iter:  100 | Time: 00:00:38 | Train Loss: 1.0980 | Average Loss: 1.6701 \n",
      "Epoch:58 | Iter:  120 | Time: 00:00:38 | Train Loss: 0.7748 | Average Loss: 1.6698 \n",
      "Epoch:58 | Iter:  140 | Time: 00:00:38 | Train Loss: 1.7417 | Average Loss: 1.6694 \n",
      "Epoch:58 | Iter:  160 | Time: 00:00:38 | Train Loss: 1.5714 | Average Loss: 1.6690 \n",
      "Epoch:58 | Iter:  180 | Time: 00:00:38 | Train Loss: 1.3076 | Average Loss: 1.6686 \n",
      "Epoch:58 | Iter:  200 | Time: 00:00:38 | Train Loss: 1.4828 | Average Loss: 1.6683 \n",
      "Epoch:58 | Iter:  220 | Time: 00:00:38 | Train Loss: 0.9654 | Average Loss: 1.6678 \n",
      "Epoch:58 | Iter:  240 | Time: 00:00:38 | Train Loss: 0.9443 | Average Loss: 1.6674 \n",
      "Epoch:58 | Iter:  260 | Time: 00:00:38 | Train Loss: 0.8769 | Average Loss: 1.6670 \n",
      "Accuracy: 0.312500 | Time: 00:00:00\n",
      "Epoch:59 | Iter:    0 | Time: 00:00:38 | Train Loss: 1.1689 | Average Loss: 1.6668 \n",
      "Epoch:59 | Iter:   20 | Time: 00:00:38 | Train Loss: 1.6646 | Average Loss: 1.6665 \n",
      "Epoch:59 | Iter:   40 | Time: 00:00:38 | Train Loss: 1.3410 | Average Loss: 1.6661 \n",
      "Epoch:59 | Iter:   60 | Time: 00:00:39 | Train Loss: 1.3414 | Average Loss: 1.6657 \n",
      "Epoch:59 | Iter:   80 | Time: 00:00:39 | Train Loss: 0.7210 | Average Loss: 1.6654 \n",
      "Epoch:59 | Iter:  100 | Time: 00:00:39 | Train Loss: 1.3844 | Average Loss: 1.6651 \n",
      "Epoch:59 | Iter:  120 | Time: 00:00:39 | Train Loss: 0.6450 | Average Loss: 1.6648 \n",
      "Epoch:59 | Iter:  140 | Time: 00:00:39 | Train Loss: 1.7096 | Average Loss: 1.6645 \n",
      "Epoch:59 | Iter:  160 | Time: 00:00:39 | Train Loss: 1.6307 | Average Loss: 1.6642 \n",
      "Epoch:59 | Iter:  180 | Time: 00:00:39 | Train Loss: 1.1287 | Average Loss: 1.6637 \n",
      "Epoch:59 | Iter:  200 | Time: 00:00:39 | Train Loss: 1.6112 | Average Loss: 1.6632 \n",
      "Epoch:59 | Iter:  220 | Time: 00:00:39 | Train Loss: 0.9659 | Average Loss: 1.6629 \n",
      "Epoch:59 | Iter:  240 | Time: 00:00:39 | Train Loss: 0.9494 | Average Loss: 1.6625 \n",
      "Epoch:59 | Iter:  260 | Time: 00:00:39 | Train Loss: 0.8770 | Average Loss: 1.6621 \n",
      "Accuracy: 0.312500 | Time: 00:00:00\n",
      "Epoch:60 | Iter:    0 | Time: 00:00:39 | Train Loss: 1.1301 | Average Loss: 1.6619 \n",
      "Epoch:60 | Iter:   20 | Time: 00:00:39 | Train Loss: 1.9034 | Average Loss: 1.6617 \n",
      "Epoch:60 | Iter:   40 | Time: 00:00:39 | Train Loss: 1.4194 | Average Loss: 1.6613 \n",
      "Epoch:60 | Iter:   60 | Time: 00:00:39 | Train Loss: 1.2243 | Average Loss: 1.6609 \n",
      "Epoch:60 | Iter:   80 | Time: 00:00:39 | Train Loss: 1.0644 | Average Loss: 1.6606 \n",
      "Epoch:60 | Iter:  100 | Time: 00:00:39 | Train Loss: 1.1997 | Average Loss: 1.6602 \n",
      "Epoch:60 | Iter:  120 | Time: 00:00:39 | Train Loss: 1.0300 | Average Loss: 1.6600 \n",
      "Epoch:60 | Iter:  140 | Time: 00:00:39 | Train Loss: 1.5484 | Average Loss: 1.6597 \n",
      "Epoch:60 | Iter:  160 | Time: 00:00:39 | Train Loss: 1.6371 | Average Loss: 1.6593 \n",
      "Epoch:60 | Iter:  180 | Time: 00:00:39 | Train Loss: 1.0744 | Average Loss: 1.6590 \n",
      "Epoch:60 | Iter:  200 | Time: 00:00:39 | Train Loss: 1.3888 | Average Loss: 1.6586 \n",
      "Epoch:60 | Iter:  220 | Time: 00:00:39 | Train Loss: 1.1822 | Average Loss: 1.6583 \n",
      "Epoch:60 | Iter:  240 | Time: 00:00:40 | Train Loss: 1.1707 | Average Loss: 1.6579 \n",
      "Epoch:60 | Iter:  260 | Time: 00:00:40 | Train Loss: 0.9926 | Average Loss: 1.6575 \n",
      "Accuracy: 0.291667 | Time: 00:00:00\n",
      "Epoch:61 | Iter:    0 | Time: 00:00:40 | Train Loss: 1.0217 | Average Loss: 1.6573 \n",
      "Epoch:61 | Iter:   20 | Time: 00:00:40 | Train Loss: 1.5093 | Average Loss: 1.6571 \n",
      "Epoch:61 | Iter:   40 | Time: 00:00:40 | Train Loss: 1.4122 | Average Loss: 1.6566 \n",
      "Epoch:61 | Iter:   60 | Time: 00:00:40 | Train Loss: 1.5196 | Average Loss: 1.6563 \n",
      "Epoch:61 | Iter:   80 | Time: 00:00:40 | Train Loss: 0.9949 | Average Loss: 1.6560 \n",
      "Epoch:61 | Iter:  100 | Time: 00:00:40 | Train Loss: 1.3727 | Average Loss: 1.6558 \n",
      "Epoch:61 | Iter:  120 | Time: 00:00:40 | Train Loss: 1.0497 | Average Loss: 1.6555 \n",
      "Epoch:61 | Iter:  140 | Time: 00:00:40 | Train Loss: 1.6595 | Average Loss: 1.6553 \n",
      "Epoch:61 | Iter:  160 | Time: 00:00:40 | Train Loss: 1.3103 | Average Loss: 1.6549 \n",
      "Epoch:61 | Iter:  180 | Time: 00:00:40 | Train Loss: 1.1949 | Average Loss: 1.6545 \n",
      "Epoch:61 | Iter:  200 | Time: 00:00:40 | Train Loss: 1.4241 | Average Loss: 1.6542 \n",
      "Epoch:61 | Iter:  220 | Time: 00:00:40 | Train Loss: 1.2483 | Average Loss: 1.6538 \n",
      "Epoch:61 | Iter:  240 | Time: 00:00:40 | Train Loss: 1.2076 | Average Loss: 1.6534 \n",
      "Epoch:61 | Iter:  260 | Time: 00:00:40 | Train Loss: 1.2427 | Average Loss: 1.6530 \n",
      "Accuracy: 0.333333 | Time: 00:00:00\n",
      "Epoch:62 | Iter:    0 | Time: 00:00:40 | Train Loss: 1.0999 | Average Loss: 1.6528 \n",
      "Epoch:62 | Iter:   20 | Time: 00:00:40 | Train Loss: 1.8846 | Average Loss: 1.6525 \n",
      "Epoch:62 | Iter:   40 | Time: 00:00:40 | Train Loss: 1.2602 | Average Loss: 1.6521 \n",
      "Epoch:62 | Iter:   60 | Time: 00:00:40 | Train Loss: 1.4478 | Average Loss: 1.6517 \n",
      "Epoch:62 | Iter:   80 | Time: 00:00:40 | Train Loss: 0.9863 | Average Loss: 1.6515 \n",
      "Epoch:62 | Iter:  100 | Time: 00:00:40 | Train Loss: 1.5654 | Average Loss: 1.6512 \n",
      "Epoch:62 | Iter:  120 | Time: 00:00:40 | Train Loss: 0.8857 | Average Loss: 1.6510 \n",
      "Epoch:62 | Iter:  140 | Time: 00:00:40 | Train Loss: 1.5195 | Average Loss: 1.6506 \n",
      "Epoch:62 | Iter:  160 | Time: 00:00:41 | Train Loss: 1.3167 | Average Loss: 1.6503 \n",
      "Epoch:62 | Iter:  180 | Time: 00:00:41 | Train Loss: 1.2892 | Average Loss: 1.6499 \n",
      "Epoch:62 | Iter:  200 | Time: 00:00:41 | Train Loss: 1.5138 | Average Loss: 1.6494 \n",
      "Epoch:62 | Iter:  220 | Time: 00:00:41 | Train Loss: 1.0830 | Average Loss: 1.6491 \n",
      "Epoch:62 | Iter:  240 | Time: 00:00:41 | Train Loss: 1.1324 | Average Loss: 1.6486 \n",
      "Epoch:62 | Iter:  260 | Time: 00:00:41 | Train Loss: 0.8300 | Average Loss: 1.6483 \n",
      "Accuracy: 0.312500 | Time: 00:00:00\n",
      "Epoch:63 | Iter:    0 | Time: 00:00:41 | Train Loss: 1.1043 | Average Loss: 1.6481 \n",
      "Epoch:63 | Iter:   20 | Time: 00:00:41 | Train Loss: 1.6639 | Average Loss: 1.6478 \n",
      "Epoch:63 | Iter:   40 | Time: 00:00:41 | Train Loss: 1.1467 | Average Loss: 1.6474 \n",
      "Epoch:63 | Iter:   60 | Time: 00:00:41 | Train Loss: 1.3136 | Average Loss: 1.6470 \n",
      "Epoch:63 | Iter:   80 | Time: 00:00:41 | Train Loss: 0.9585 | Average Loss: 1.6468 \n",
      "Epoch:63 | Iter:  100 | Time: 00:00:41 | Train Loss: 1.3353 | Average Loss: 1.6464 \n",
      "Epoch:63 | Iter:  120 | Time: 00:00:41 | Train Loss: 0.6080 | Average Loss: 1.6461 \n",
      "Epoch:63 | Iter:  140 | Time: 00:00:41 | Train Loss: 1.5859 | Average Loss: 1.6459 \n",
      "Epoch:63 | Iter:  160 | Time: 00:00:41 | Train Loss: 1.4053 | Average Loss: 1.6456 \n",
      "Epoch:63 | Iter:  180 | Time: 00:00:41 | Train Loss: 1.1764 | Average Loss: 1.6452 \n",
      "Epoch:63 | Iter:  200 | Time: 00:00:41 | Train Loss: 1.3216 | Average Loss: 1.6448 \n",
      "Epoch:63 | Iter:  220 | Time: 00:00:41 | Train Loss: 1.4426 | Average Loss: 1.6445 \n",
      "Epoch:63 | Iter:  240 | Time: 00:00:41 | Train Loss: 1.0599 | Average Loss: 1.6441 \n",
      "Epoch:63 | Iter:  260 | Time: 00:00:41 | Train Loss: 1.0349 | Average Loss: 1.6437 \n",
      "Accuracy: 0.312500 | Time: 00:00:00\n",
      "Epoch:64 | Iter:    0 | Time: 00:00:41 | Train Loss: 0.9266 | Average Loss: 1.6435 \n",
      "Epoch:64 | Iter:   20 | Time: 00:00:41 | Train Loss: 1.2873 | Average Loss: 1.6432 \n",
      "Epoch:64 | Iter:   40 | Time: 00:00:42 | Train Loss: 1.2392 | Average Loss: 1.6429 \n",
      "Epoch:64 | Iter:   60 | Time: 00:00:42 | Train Loss: 1.4134 | Average Loss: 1.6426 \n",
      "Epoch:64 | Iter:   80 | Time: 00:00:42 | Train Loss: 1.0221 | Average Loss: 1.6424 \n",
      "Epoch:64 | Iter:  100 | Time: 00:00:42 | Train Loss: 1.7594 | Average Loss: 1.6421 \n",
      "Epoch:64 | Iter:  120 | Time: 00:00:42 | Train Loss: 0.6474 | Average Loss: 1.6418 \n",
      "Epoch:64 | Iter:  140 | Time: 00:00:42 | Train Loss: 1.8042 | Average Loss: 1.6415 \n",
      "Epoch:64 | Iter:  160 | Time: 00:00:42 | Train Loss: 1.4939 | Average Loss: 1.6412 \n",
      "Epoch:64 | Iter:  180 | Time: 00:00:42 | Train Loss: 1.3041 | Average Loss: 1.6408 \n",
      "Epoch:64 | Iter:  200 | Time: 00:00:42 | Train Loss: 1.1217 | Average Loss: 1.6404 \n",
      "Epoch:64 | Iter:  220 | Time: 00:00:42 | Train Loss: 1.0686 | Average Loss: 1.6401 \n",
      "Epoch:64 | Iter:  240 | Time: 00:00:42 | Train Loss: 0.9487 | Average Loss: 1.6397 \n",
      "Epoch:64 | Iter:  260 | Time: 00:00:42 | Train Loss: 1.0574 | Average Loss: 1.6394 \n",
      "Accuracy: 0.333333 | Time: 00:00:00\n",
      "Epoch:65 | Iter:    0 | Time: 00:00:42 | Train Loss: 1.0792 | Average Loss: 1.6391 \n",
      "Epoch:65 | Iter:   20 | Time: 00:00:42 | Train Loss: 1.7844 | Average Loss: 1.6389 \n",
      "Epoch:65 | Iter:   40 | Time: 00:00:42 | Train Loss: 1.4357 | Average Loss: 1.6386 \n",
      "Epoch:65 | Iter:   60 | Time: 00:00:42 | Train Loss: 1.2076 | Average Loss: 1.6382 \n",
      "Epoch:65 | Iter:   80 | Time: 00:00:42 | Train Loss: 0.8872 | Average Loss: 1.6380 \n",
      "Epoch:65 | Iter:  100 | Time: 00:00:42 | Train Loss: 0.9441 | Average Loss: 1.6375 \n",
      "Epoch:65 | Iter:  120 | Time: 00:00:42 | Train Loss: 0.6774 | Average Loss: 1.6371 \n",
      "Epoch:65 | Iter:  140 | Time: 00:00:42 | Train Loss: 1.8205 | Average Loss: 1.6369 \n",
      "Epoch:65 | Iter:  160 | Time: 00:00:42 | Train Loss: 1.5252 | Average Loss: 1.6365 \n",
      "Epoch:65 | Iter:  180 | Time: 00:00:42 | Train Loss: 1.0592 | Average Loss: 1.6361 \n",
      "Epoch:65 | Iter:  200 | Time: 00:00:42 | Train Loss: 1.1499 | Average Loss: 1.6358 \n",
      "Epoch:65 | Iter:  220 | Time: 00:00:42 | Train Loss: 1.0230 | Average Loss: 1.6355 \n",
      "Epoch:65 | Iter:  240 | Time: 00:00:42 | Train Loss: 0.9764 | Average Loss: 1.6352 \n",
      "Epoch:65 | Iter:  260 | Time: 00:00:43 | Train Loss: 0.9472 | Average Loss: 1.6349 \n",
      "Accuracy: 0.354167 | Time: 00:00:00\n",
      "Epoch:66 | Iter:    0 | Time: 00:00:43 | Train Loss: 1.1845 | Average Loss: 1.6347 \n",
      "Epoch:66 | Iter:   20 | Time: 00:00:43 | Train Loss: 1.4244 | Average Loss: 1.6344 \n",
      "Epoch:66 | Iter:   40 | Time: 00:00:43 | Train Loss: 1.1444 | Average Loss: 1.6340 \n",
      "Epoch:66 | Iter:   60 | Time: 00:00:43 | Train Loss: 1.4507 | Average Loss: 1.6337 \n",
      "Epoch:66 | Iter:   80 | Time: 00:00:43 | Train Loss: 0.9202 | Average Loss: 1.6334 \n",
      "Epoch:66 | Iter:  100 | Time: 00:00:43 | Train Loss: 1.3086 | Average Loss: 1.6331 \n",
      "Epoch:66 | Iter:  120 | Time: 00:00:43 | Train Loss: 0.7517 | Average Loss: 1.6329 \n",
      "Epoch:66 | Iter:  140 | Time: 00:00:43 | Train Loss: 1.5708 | Average Loss: 1.6327 \n",
      "Epoch:66 | Iter:  160 | Time: 00:00:43 | Train Loss: 1.7447 | Average Loss: 1.6323 \n",
      "Epoch:66 | Iter:  180 | Time: 00:00:43 | Train Loss: 0.8134 | Average Loss: 1.6319 \n",
      "Epoch:66 | Iter:  200 | Time: 00:00:43 | Train Loss: 1.4501 | Average Loss: 1.6315 \n",
      "Epoch:66 | Iter:  220 | Time: 00:00:43 | Train Loss: 1.1029 | Average Loss: 1.6312 \n",
      "Epoch:66 | Iter:  240 | Time: 00:00:43 | Train Loss: 1.3126 | Average Loss: 1.6309 \n",
      "Epoch:66 | Iter:  260 | Time: 00:00:43 | Train Loss: 1.0298 | Average Loss: 1.6306 \n",
      "Accuracy: 0.312500 | Time: 00:00:00\n",
      "Epoch:67 | Iter:    0 | Time: 00:00:43 | Train Loss: 1.1235 | Average Loss: 1.6304 \n",
      "Epoch:67 | Iter:   20 | Time: 00:00:43 | Train Loss: 1.7154 | Average Loss: 1.6301 \n",
      "Epoch:67 | Iter:   40 | Time: 00:00:43 | Train Loss: 1.2881 | Average Loss: 1.6299 \n",
      "Epoch:67 | Iter:   60 | Time: 00:00:43 | Train Loss: 1.5485 | Average Loss: 1.6295 \n",
      "Epoch:67 | Iter:   80 | Time: 00:00:43 | Train Loss: 1.0170 | Average Loss: 1.6293 \n",
      "Epoch:67 | Iter:  100 | Time: 00:00:43 | Train Loss: 1.4488 | Average Loss: 1.6290 \n",
      "Epoch:67 | Iter:  120 | Time: 00:00:43 | Train Loss: 1.0177 | Average Loss: 1.6288 \n",
      "Epoch:67 | Iter:  140 | Time: 00:00:44 | Train Loss: 1.5242 | Average Loss: 1.6286 \n",
      "Epoch:67 | Iter:  160 | Time: 00:00:44 | Train Loss: 1.5455 | Average Loss: 1.6282 \n",
      "Epoch:67 | Iter:  180 | Time: 00:00:44 | Train Loss: 1.2728 | Average Loss: 1.6279 \n",
      "Epoch:67 | Iter:  200 | Time: 00:00:44 | Train Loss: 1.3862 | Average Loss: 1.6275 \n",
      "Epoch:67 | Iter:  220 | Time: 00:00:44 | Train Loss: 0.9265 | Average Loss: 1.6272 \n",
      "Epoch:67 | Iter:  240 | Time: 00:00:44 | Train Loss: 1.2434 | Average Loss: 1.6268 \n",
      "Epoch:67 | Iter:  260 | Time: 00:00:44 | Train Loss: 0.9001 | Average Loss: 1.6264 \n",
      "Accuracy: 0.333333 | Time: 00:00:00\n",
      "Epoch:68 | Iter:    0 | Time: 00:00:44 | Train Loss: 1.0640 | Average Loss: 1.6262 \n",
      "Epoch:68 | Iter:   20 | Time: 00:00:44 | Train Loss: 1.4331 | Average Loss: 1.6260 \n",
      "Epoch:68 | Iter:   40 | Time: 00:00:44 | Train Loss: 1.2568 | Average Loss: 1.6257 \n",
      "Epoch:68 | Iter:   60 | Time: 00:00:44 | Train Loss: 1.0335 | Average Loss: 1.6254 \n",
      "Epoch:68 | Iter:   80 | Time: 00:00:44 | Train Loss: 0.7995 | Average Loss: 1.6251 \n",
      "Epoch:68 | Iter:  100 | Time: 00:00:44 | Train Loss: 1.4601 | Average Loss: 1.6248 \n",
      "Epoch:68 | Iter:  120 | Time: 00:00:44 | Train Loss: 0.6899 | Average Loss: 1.6246 \n",
      "Epoch:68 | Iter:  140 | Time: 00:00:44 | Train Loss: 1.7113 | Average Loss: 1.6243 \n",
      "Epoch:68 | Iter:  160 | Time: 00:00:44 | Train Loss: 1.2526 | Average Loss: 1.6240 \n",
      "Epoch:68 | Iter:  180 | Time: 00:00:44 | Train Loss: 0.8218 | Average Loss: 1.6237 \n",
      "Epoch:68 | Iter:  200 | Time: 00:00:44 | Train Loss: 1.5032 | Average Loss: 1.6234 \n",
      "Epoch:68 | Iter:  220 | Time: 00:00:44 | Train Loss: 1.1328 | Average Loss: 1.6231 \n",
      "Epoch:68 | Iter:  240 | Time: 00:00:44 | Train Loss: 1.0271 | Average Loss: 1.6227 \n",
      "Epoch:68 | Iter:  260 | Time: 00:00:44 | Train Loss: 1.0919 | Average Loss: 1.6224 \n",
      "Accuracy: 0.333333 | Time: 00:00:00\n",
      "Epoch:69 | Iter:    0 | Time: 00:00:44 | Train Loss: 0.9915 | Average Loss: 1.6223 \n",
      "Epoch:69 | Iter:   20 | Time: 00:00:44 | Train Loss: 1.6095 | Average Loss: 1.6220 \n",
      "Epoch:69 | Iter:   40 | Time: 00:00:45 | Train Loss: 1.0694 | Average Loss: 1.6217 \n",
      "Epoch:69 | Iter:   60 | Time: 00:00:45 | Train Loss: 0.9350 | Average Loss: 1.6213 \n",
      "Epoch:69 | Iter:   80 | Time: 00:00:45 | Train Loss: 0.7598 | Average Loss: 1.6210 \n",
      "Epoch:69 | Iter:  100 | Time: 00:00:45 | Train Loss: 1.2144 | Average Loss: 1.6208 \n",
      "Epoch:69 | Iter:  120 | Time: 00:00:45 | Train Loss: 0.6247 | Average Loss: 1.6205 \n",
      "Epoch:69 | Iter:  140 | Time: 00:00:45 | Train Loss: 1.8438 | Average Loss: 1.6202 \n",
      "Epoch:69 | Iter:  160 | Time: 00:00:45 | Train Loss: 1.5144 | Average Loss: 1.6200 \n",
      "Epoch:69 | Iter:  180 | Time: 00:00:45 | Train Loss: 1.2650 | Average Loss: 1.6196 \n",
      "Epoch:69 | Iter:  200 | Time: 00:00:45 | Train Loss: 1.6677 | Average Loss: 1.6193 \n",
      "Epoch:69 | Iter:  220 | Time: 00:00:45 | Train Loss: 0.8939 | Average Loss: 1.6189 \n",
      "Epoch:69 | Iter:  240 | Time: 00:00:45 | Train Loss: 0.9569 | Average Loss: 1.6186 \n",
      "Epoch:69 | Iter:  260 | Time: 00:00:45 | Train Loss: 1.0138 | Average Loss: 1.6183 \n",
      "Accuracy: 0.333333 | Time: 00:00:00\n",
      "Epoch:70 | Iter:    0 | Time: 00:00:45 | Train Loss: 0.8186 | Average Loss: 1.6181 \n",
      "Epoch:70 | Iter:   20 | Time: 00:00:45 | Train Loss: 1.4773 | Average Loss: 1.6178 \n",
      "Epoch:70 | Iter:   40 | Time: 00:00:45 | Train Loss: 1.4303 | Average Loss: 1.6175 \n",
      "Epoch:70 | Iter:   60 | Time: 00:00:45 | Train Loss: 1.3683 | Average Loss: 1.6172 \n",
      "Epoch:70 | Iter:   80 | Time: 00:00:45 | Train Loss: 0.8650 | Average Loss: 1.6169 \n",
      "Epoch:70 | Iter:  100 | Time: 00:00:45 | Train Loss: 1.6958 | Average Loss: 1.6167 \n",
      "Epoch:70 | Iter:  120 | Time: 00:00:45 | Train Loss: 0.8650 | Average Loss: 1.6165 \n",
      "Epoch:70 | Iter:  140 | Time: 00:00:45 | Train Loss: 1.4560 | Average Loss: 1.6163 \n",
      "Epoch:70 | Iter:  160 | Time: 00:00:45 | Train Loss: 1.5005 | Average Loss: 1.6160 \n",
      "Epoch:70 | Iter:  180 | Time: 00:00:45 | Train Loss: 0.9893 | Average Loss: 1.6157 \n",
      "Epoch:70 | Iter:  200 | Time: 00:00:45 | Train Loss: 1.4013 | Average Loss: 1.6153 \n",
      "Epoch:70 | Iter:  220 | Time: 00:00:46 | Train Loss: 1.0267 | Average Loss: 1.6151 \n",
      "Epoch:70 | Iter:  240 | Time: 00:00:46 | Train Loss: 1.0348 | Average Loss: 1.6147 \n",
      "Epoch:70 | Iter:  260 | Time: 00:00:46 | Train Loss: 1.0409 | Average Loss: 1.6145 \n",
      "Accuracy: 0.354167 | Time: 00:00:00\n",
      "Epoch:71 | Iter:    0 | Time: 00:00:46 | Train Loss: 1.1611 | Average Loss: 1.6143 \n",
      "Epoch:71 | Iter:   20 | Time: 00:00:46 | Train Loss: 1.8076 | Average Loss: 1.6140 \n",
      "Epoch:71 | Iter:   40 | Time: 00:00:46 | Train Loss: 1.2634 | Average Loss: 1.6137 \n",
      "Epoch:71 | Iter:   60 | Time: 00:00:46 | Train Loss: 1.6379 | Average Loss: 1.6134 \n",
      "Epoch:71 | Iter:   80 | Time: 00:00:46 | Train Loss: 0.8466 | Average Loss: 1.6132 \n",
      "Epoch:71 | Iter:  100 | Time: 00:00:46 | Train Loss: 1.9387 | Average Loss: 1.6130 \n",
      "Epoch:71 | Iter:  120 | Time: 00:00:46 | Train Loss: 0.5856 | Average Loss: 1.6128 \n",
      "Epoch:71 | Iter:  140 | Time: 00:00:46 | Train Loss: 1.6270 | Average Loss: 1.6125 \n",
      "Epoch:71 | Iter:  160 | Time: 00:00:46 | Train Loss: 1.3749 | Average Loss: 1.6123 \n",
      "Epoch:71 | Iter:  180 | Time: 00:00:46 | Train Loss: 1.0885 | Average Loss: 1.6119 \n",
      "Epoch:71 | Iter:  200 | Time: 00:00:46 | Train Loss: 1.2420 | Average Loss: 1.6115 \n",
      "Epoch:71 | Iter:  220 | Time: 00:00:46 | Train Loss: 0.9397 | Average Loss: 1.6112 \n",
      "Epoch:71 | Iter:  240 | Time: 00:00:46 | Train Loss: 0.8863 | Average Loss: 1.6109 \n",
      "Epoch:71 | Iter:  260 | Time: 00:00:46 | Train Loss: 0.9861 | Average Loss: 1.6107 \n",
      "Accuracy: 0.312500 | Time: 00:00:00\n",
      "Epoch:72 | Iter:    0 | Time: 00:00:46 | Train Loss: 0.8139 | Average Loss: 1.6104 \n",
      "Epoch:72 | Iter:   20 | Time: 00:00:46 | Train Loss: 1.5753 | Average Loss: 1.6102 \n",
      "Epoch:72 | Iter:   40 | Time: 00:00:46 | Train Loss: 1.4178 | Average Loss: 1.6099 \n",
      "Epoch:72 | Iter:   60 | Time: 00:00:46 | Train Loss: 1.5875 | Average Loss: 1.6095 \n",
      "Epoch:72 | Iter:   80 | Time: 00:00:46 | Train Loss: 1.2410 | Average Loss: 1.6093 \n",
      "Epoch:72 | Iter:  100 | Time: 00:00:46 | Train Loss: 1.5332 | Average Loss: 1.6091 \n",
      "Epoch:72 | Iter:  120 | Time: 00:00:47 | Train Loss: 0.6541 | Average Loss: 1.6089 \n",
      "Epoch:72 | Iter:  140 | Time: 00:00:47 | Train Loss: 1.3523 | Average Loss: 1.6086 \n",
      "Epoch:72 | Iter:  160 | Time: 00:00:47 | Train Loss: 1.4999 | Average Loss: 1.6083 \n",
      "Epoch:72 | Iter:  180 | Time: 00:00:47 | Train Loss: 1.0328 | Average Loss: 1.6080 \n",
      "Epoch:72 | Iter:  200 | Time: 00:00:47 | Train Loss: 1.4867 | Average Loss: 1.6076 \n",
      "Epoch:72 | Iter:  220 | Time: 00:00:47 | Train Loss: 1.1690 | Average Loss: 1.6073 \n",
      "Epoch:72 | Iter:  240 | Time: 00:00:47 | Train Loss: 0.8546 | Average Loss: 1.6069 \n",
      "Epoch:72 | Iter:  260 | Time: 00:00:47 | Train Loss: 1.2720 | Average Loss: 1.6067 \n",
      "Accuracy: 0.354167 | Time: 00:00:00\n",
      "Epoch:73 | Iter:    0 | Time: 00:00:47 | Train Loss: 1.1703 | Average Loss: 1.6065 \n",
      "Epoch:73 | Iter:   20 | Time: 00:00:47 | Train Loss: 1.6582 | Average Loss: 1.6063 \n",
      "Epoch:73 | Iter:   40 | Time: 00:00:47 | Train Loss: 1.1556 | Average Loss: 1.6060 \n",
      "Epoch:73 | Iter:   60 | Time: 00:00:47 | Train Loss: 1.1196 | Average Loss: 1.6057 \n",
      "Epoch:73 | Iter:   80 | Time: 00:00:47 | Train Loss: 1.0477 | Average Loss: 1.6055 \n",
      "Epoch:73 | Iter:  100 | Time: 00:00:47 | Train Loss: 1.4117 | Average Loss: 1.6052 \n",
      "Epoch:73 | Iter:  120 | Time: 00:00:47 | Train Loss: 0.9676 | Average Loss: 1.6050 \n",
      "Epoch:73 | Iter:  140 | Time: 00:00:47 | Train Loss: 1.8550 | Average Loss: 1.6047 \n",
      "Epoch:73 | Iter:  160 | Time: 00:00:47 | Train Loss: 1.3674 | Average Loss: 1.6045 \n",
      "Epoch:73 | Iter:  180 | Time: 00:00:47 | Train Loss: 0.9383 | Average Loss: 1.6041 \n",
      "Epoch:73 | Iter:  200 | Time: 00:00:47 | Train Loss: 1.2265 | Average Loss: 1.6039 \n",
      "Epoch:73 | Iter:  220 | Time: 00:00:47 | Train Loss: 1.1850 | Average Loss: 1.6037 \n",
      "Epoch:73 | Iter:  240 | Time: 00:00:47 | Train Loss: 1.0725 | Average Loss: 1.6034 \n",
      "Epoch:73 | Iter:  260 | Time: 00:00:47 | Train Loss: 0.9291 | Average Loss: 1.6031 \n",
      "Accuracy: 0.354167 | Time: 00:00:00\n",
      "Epoch:74 | Iter:    0 | Time: 00:00:47 | Train Loss: 1.1820 | Average Loss: 1.6029 \n",
      "Epoch:74 | Iter:   20 | Time: 00:00:47 | Train Loss: 1.4918 | Average Loss: 1.6026 \n",
      "Epoch:74 | Iter:   40 | Time: 00:00:47 | Train Loss: 1.3417 | Average Loss: 1.6024 \n",
      "Epoch:74 | Iter:   60 | Time: 00:00:48 | Train Loss: 1.1257 | Average Loss: 1.6020 \n",
      "Epoch:74 | Iter:   80 | Time: 00:00:48 | Train Loss: 1.0325 | Average Loss: 1.6018 \n",
      "Epoch:74 | Iter:  100 | Time: 00:00:48 | Train Loss: 1.4264 | Average Loss: 1.6015 \n",
      "Epoch:74 | Iter:  120 | Time: 00:00:48 | Train Loss: 0.6960 | Average Loss: 1.6013 \n",
      "Epoch:74 | Iter:  140 | Time: 00:00:48 | Train Loss: 1.5187 | Average Loss: 1.6010 \n",
      "Epoch:74 | Iter:  160 | Time: 00:00:48 | Train Loss: 1.5305 | Average Loss: 1.6007 \n",
      "Epoch:74 | Iter:  180 | Time: 00:00:48 | Train Loss: 1.1733 | Average Loss: 1.6004 \n",
      "Epoch:74 | Iter:  200 | Time: 00:00:48 | Train Loss: 1.3259 | Average Loss: 1.6001 \n",
      "Epoch:74 | Iter:  220 | Time: 00:00:48 | Train Loss: 1.0021 | Average Loss: 1.5998 \n",
      "Epoch:74 | Iter:  240 | Time: 00:00:48 | Train Loss: 1.1046 | Average Loss: 1.5995 \n",
      "Epoch:74 | Iter:  260 | Time: 00:00:48 | Train Loss: 1.0789 | Average Loss: 1.5993 \n",
      "Accuracy: 0.354167 | Time: 00:00:00\n",
      "Epoch:75 | Iter:    0 | Time: 00:00:48 | Train Loss: 1.1369 | Average Loss: 1.5991 \n",
      "Epoch:75 | Iter:   20 | Time: 00:00:48 | Train Loss: 1.5572 | Average Loss: 1.5988 \n",
      "Epoch:75 | Iter:   40 | Time: 00:00:48 | Train Loss: 1.3422 | Average Loss: 1.5985 \n",
      "Epoch:75 | Iter:   60 | Time: 00:00:48 | Train Loss: 1.2681 | Average Loss: 1.5982 \n",
      "Epoch:75 | Iter:   80 | Time: 00:00:48 | Train Loss: 0.9825 | Average Loss: 1.5980 \n",
      "Epoch:75 | Iter:  100 | Time: 00:00:48 | Train Loss: 1.3177 | Average Loss: 1.5978 \n",
      "Epoch:75 | Iter:  120 | Time: 00:00:48 | Train Loss: 0.7071 | Average Loss: 1.5975 \n",
      "Epoch:75 | Iter:  140 | Time: 00:00:48 | Train Loss: 1.8875 | Average Loss: 1.5972 \n",
      "Epoch:75 | Iter:  160 | Time: 00:00:48 | Train Loss: 1.7288 | Average Loss: 1.5970 \n",
      "Epoch:75 | Iter:  180 | Time: 00:00:48 | Train Loss: 1.0031 | Average Loss: 1.5967 \n",
      "Epoch:75 | Iter:  200 | Time: 00:00:48 | Train Loss: 1.5369 | Average Loss: 1.5964 \n",
      "Epoch:75 | Iter:  220 | Time: 00:00:48 | Train Loss: 1.0107 | Average Loss: 1.5961 \n",
      "Epoch:75 | Iter:  240 | Time: 00:00:49 | Train Loss: 0.7892 | Average Loss: 1.5959 \n",
      "Epoch:75 | Iter:  260 | Time: 00:00:49 | Train Loss: 0.8559 | Average Loss: 1.5956 \n",
      "Accuracy: 0.333333 | Time: 00:00:00\n",
      "Epoch:76 | Iter:    0 | Time: 00:00:49 | Train Loss: 0.8235 | Average Loss: 1.5955 \n",
      "Epoch:76 | Iter:   20 | Time: 00:00:49 | Train Loss: 1.4512 | Average Loss: 1.5952 \n",
      "Epoch:76 | Iter:   40 | Time: 00:00:49 | Train Loss: 1.2294 | Average Loss: 1.5949 \n",
      "Epoch:76 | Iter:   60 | Time: 00:00:49 | Train Loss: 1.1097 | Average Loss: 1.5946 \n",
      "Epoch:76 | Iter:   80 | Time: 00:00:49 | Train Loss: 1.0504 | Average Loss: 1.5944 \n",
      "Epoch:76 | Iter:  100 | Time: 00:00:49 | Train Loss: 1.3256 | Average Loss: 1.5942 \n",
      "Epoch:76 | Iter:  120 | Time: 00:00:49 | Train Loss: 0.6313 | Average Loss: 1.5939 \n",
      "Epoch:76 | Iter:  140 | Time: 00:00:49 | Train Loss: 1.4862 | Average Loss: 1.5937 \n",
      "Epoch:76 | Iter:  160 | Time: 00:00:49 | Train Loss: 1.4017 | Average Loss: 1.5934 \n",
      "Epoch:76 | Iter:  180 | Time: 00:00:49 | Train Loss: 1.0067 | Average Loss: 1.5931 \n",
      "Epoch:76 | Iter:  200 | Time: 00:00:49 | Train Loss: 1.1383 | Average Loss: 1.5928 \n",
      "Epoch:76 | Iter:  220 | Time: 00:00:49 | Train Loss: 0.9255 | Average Loss: 1.5926 \n",
      "Epoch:76 | Iter:  240 | Time: 00:00:49 | Train Loss: 1.0980 | Average Loss: 1.5923 \n",
      "Epoch:76 | Iter:  260 | Time: 00:00:49 | Train Loss: 0.8301 | Average Loss: 1.5920 \n",
      "Accuracy: 0.312500 | Time: 00:00:00\n",
      "Epoch:77 | Iter:    0 | Time: 00:00:49 | Train Loss: 0.7838 | Average Loss: 1.5917 \n",
      "Epoch:77 | Iter:   20 | Time: 00:00:49 | Train Loss: 1.5718 | Average Loss: 1.5915 \n",
      "Epoch:77 | Iter:   40 | Time: 00:00:49 | Train Loss: 0.9740 | Average Loss: 1.5912 \n",
      "Epoch:77 | Iter:   60 | Time: 00:00:49 | Train Loss: 1.1875 | Average Loss: 1.5909 \n",
      "Epoch:77 | Iter:   80 | Time: 00:00:49 | Train Loss: 0.9827 | Average Loss: 1.5907 \n",
      "Epoch:77 | Iter:  100 | Time: 00:00:49 | Train Loss: 1.3223 | Average Loss: 1.5905 \n",
      "Epoch:77 | Iter:  120 | Time: 00:00:49 | Train Loss: 0.8462 | Average Loss: 1.5902 \n",
      "Epoch:77 | Iter:  140 | Time: 00:00:49 | Train Loss: 1.2714 | Average Loss: 1.5899 \n",
      "Epoch:77 | Iter:  160 | Time: 00:00:49 | Train Loss: 1.6057 | Average Loss: 1.5896 \n",
      "Epoch:77 | Iter:  180 | Time: 00:00:50 | Train Loss: 0.9886 | Average Loss: 1.5893 \n",
      "Epoch:77 | Iter:  200 | Time: 00:00:50 | Train Loss: 1.3166 | Average Loss: 1.5890 \n",
      "Epoch:77 | Iter:  220 | Time: 00:00:50 | Train Loss: 0.8161 | Average Loss: 1.5887 \n",
      "Epoch:77 | Iter:  240 | Time: 00:00:50 | Train Loss: 0.7829 | Average Loss: 1.5885 \n",
      "Epoch:77 | Iter:  260 | Time: 00:00:50 | Train Loss: 0.8973 | Average Loss: 1.5882 \n",
      "Accuracy: 0.333333 | Time: 00:00:00\n",
      "Epoch:78 | Iter:    0 | Time: 00:00:50 | Train Loss: 0.9488 | Average Loss: 1.5880 \n",
      "Epoch:78 | Iter:   20 | Time: 00:00:50 | Train Loss: 1.4559 | Average Loss: 1.5877 \n",
      "Epoch:78 | Iter:   40 | Time: 00:00:50 | Train Loss: 1.3696 | Average Loss: 1.5875 \n",
      "Epoch:78 | Iter:   60 | Time: 00:00:50 | Train Loss: 1.4129 | Average Loss: 1.5872 \n",
      "Epoch:78 | Iter:   80 | Time: 00:00:50 | Train Loss: 1.2554 | Average Loss: 1.5870 \n",
      "Epoch:78 | Iter:  100 | Time: 00:00:50 | Train Loss: 1.3533 | Average Loss: 1.5868 \n",
      "Epoch:78 | Iter:  120 | Time: 00:00:50 | Train Loss: 0.7455 | Average Loss: 1.5866 \n",
      "Epoch:78 | Iter:  140 | Time: 00:00:50 | Train Loss: 1.6358 | Average Loss: 1.5863 \n",
      "Epoch:78 | Iter:  160 | Time: 00:00:50 | Train Loss: 1.4315 | Average Loss: 1.5861 \n",
      "Epoch:78 | Iter:  180 | Time: 00:00:50 | Train Loss: 1.1762 | Average Loss: 1.5858 \n",
      "Epoch:78 | Iter:  200 | Time: 00:00:50 | Train Loss: 1.3464 | Average Loss: 1.5855 \n",
      "Epoch:78 | Iter:  220 | Time: 00:00:50 | Train Loss: 1.1272 | Average Loss: 1.5852 \n",
      "Epoch:78 | Iter:  240 | Time: 00:00:50 | Train Loss: 1.0563 | Average Loss: 1.5849 \n",
      "Epoch:78 | Iter:  260 | Time: 00:00:50 | Train Loss: 1.1252 | Average Loss: 1.5847 \n",
      "Accuracy: 0.333333 | Time: 00:00:00\n",
      "Epoch:79 | Iter:    0 | Time: 00:00:50 | Train Loss: 1.0376 | Average Loss: 1.5845 \n",
      "Epoch:79 | Iter:   20 | Time: 00:00:50 | Train Loss: 1.5722 | Average Loss: 1.5843 \n",
      "Epoch:79 | Iter:   40 | Time: 00:00:50 | Train Loss: 1.1892 | Average Loss: 1.5840 \n",
      "Epoch:79 | Iter:   60 | Time: 00:00:51 | Train Loss: 1.1379 | Average Loss: 1.5838 \n",
      "Epoch:79 | Iter:   80 | Time: 00:00:51 | Train Loss: 0.8511 | Average Loss: 1.5836 \n",
      "Epoch:79 | Iter:  100 | Time: 00:00:51 | Train Loss: 1.2542 | Average Loss: 1.5834 \n",
      "Epoch:79 | Iter:  120 | Time: 00:00:51 | Train Loss: 0.9940 | Average Loss: 1.5832 \n",
      "Epoch:79 | Iter:  140 | Time: 00:00:51 | Train Loss: 1.7154 | Average Loss: 1.5830 \n",
      "Epoch:79 | Iter:  160 | Time: 00:00:51 | Train Loss: 1.7382 | Average Loss: 1.5827 \n",
      "Epoch:79 | Iter:  180 | Time: 00:00:51 | Train Loss: 1.1853 | Average Loss: 1.5824 \n",
      "Epoch:79 | Iter:  200 | Time: 00:00:51 | Train Loss: 1.4448 | Average Loss: 1.5821 \n",
      "Epoch:79 | Iter:  220 | Time: 00:00:51 | Train Loss: 0.9424 | Average Loss: 1.5819 \n",
      "Epoch:79 | Iter:  240 | Time: 00:00:51 | Train Loss: 1.0845 | Average Loss: 1.5815 \n",
      "Epoch:79 | Iter:  260 | Time: 00:00:51 | Train Loss: 0.7624 | Average Loss: 1.5812 \n",
      "Accuracy: 0.312500 | Time: 00:00:00\n",
      "Epoch:80 | Iter:    0 | Time: 00:00:51 | Train Loss: 1.1495 | Average Loss: 1.5810 \n",
      "Epoch:80 | Iter:   20 | Time: 00:00:51 | Train Loss: 1.5130 | Average Loss: 1.5808 \n",
      "Epoch:80 | Iter:   40 | Time: 00:00:51 | Train Loss: 1.2016 | Average Loss: 1.5806 \n",
      "Epoch:80 | Iter:   60 | Time: 00:00:51 | Train Loss: 1.0667 | Average Loss: 1.5802 \n",
      "Epoch:80 | Iter:   80 | Time: 00:00:51 | Train Loss: 0.9392 | Average Loss: 1.5800 \n",
      "Epoch:80 | Iter:  100 | Time: 00:00:51 | Train Loss: 1.2441 | Average Loss: 1.5797 \n",
      "Epoch:80 | Iter:  120 | Time: 00:00:51 | Train Loss: 0.7218 | Average Loss: 1.5795 \n",
      "Epoch:80 | Iter:  140 | Time: 00:00:51 | Train Loss: 1.5133 | Average Loss: 1.5793 \n",
      "Epoch:80 | Iter:  160 | Time: 00:00:51 | Train Loss: 1.0141 | Average Loss: 1.5790 \n",
      "Epoch:80 | Iter:  180 | Time: 00:00:51 | Train Loss: 1.1426 | Average Loss: 1.5787 \n",
      "Epoch:80 | Iter:  200 | Time: 00:00:51 | Train Loss: 1.0963 | Average Loss: 1.5783 \n",
      "Epoch:80 | Iter:  220 | Time: 00:00:52 | Train Loss: 1.0357 | Average Loss: 1.5781 \n",
      "Epoch:80 | Iter:  240 | Time: 00:00:52 | Train Loss: 1.0523 | Average Loss: 1.5778 \n",
      "Epoch:80 | Iter:  260 | Time: 00:00:52 | Train Loss: 1.1353 | Average Loss: 1.5775 \n",
      "Accuracy: 0.333333 | Time: 00:00:00\n",
      "Epoch:81 | Iter:    0 | Time: 00:00:52 | Train Loss: 1.1009 | Average Loss: 1.5774 \n",
      "Epoch:81 | Iter:   20 | Time: 00:00:52 | Train Loss: 1.6760 | Average Loss: 1.5772 \n",
      "Epoch:81 | Iter:   40 | Time: 00:00:52 | Train Loss: 1.0077 | Average Loss: 1.5769 \n",
      "Epoch:81 | Iter:   60 | Time: 00:00:52 | Train Loss: 1.3204 | Average Loss: 1.5766 \n",
      "Epoch:81 | Iter:   80 | Time: 00:00:52 | Train Loss: 0.8774 | Average Loss: 1.5765 \n",
      "Epoch:81 | Iter:  100 | Time: 00:00:52 | Train Loss: 0.9864 | Average Loss: 1.5763 \n",
      "Epoch:81 | Iter:  120 | Time: 00:00:52 | Train Loss: 0.9968 | Average Loss: 1.5761 \n",
      "Epoch:81 | Iter:  140 | Time: 00:00:52 | Train Loss: 1.5078 | Average Loss: 1.5758 \n",
      "Epoch:81 | Iter:  160 | Time: 00:00:52 | Train Loss: 1.2301 | Average Loss: 1.5755 \n",
      "Epoch:81 | Iter:  180 | Time: 00:00:52 | Train Loss: 1.0861 | Average Loss: 1.5752 \n",
      "Epoch:81 | Iter:  200 | Time: 00:00:52 | Train Loss: 1.1082 | Average Loss: 1.5749 \n",
      "Epoch:81 | Iter:  220 | Time: 00:00:52 | Train Loss: 0.9693 | Average Loss: 1.5746 \n",
      "Epoch:81 | Iter:  240 | Time: 00:00:52 | Train Loss: 1.0164 | Average Loss: 1.5744 \n",
      "Epoch:81 | Iter:  260 | Time: 00:00:52 | Train Loss: 1.2057 | Average Loss: 1.5741 \n",
      "Accuracy: 0.291667 | Time: 00:00:00\n",
      "Epoch:82 | Iter:    0 | Time: 00:00:52 | Train Loss: 1.2425 | Average Loss: 1.5740 \n",
      "Epoch:82 | Iter:   20 | Time: 00:00:52 | Train Loss: 1.8338 | Average Loss: 1.5738 \n",
      "Epoch:82 | Iter:   40 | Time: 00:00:52 | Train Loss: 1.2344 | Average Loss: 1.5735 \n",
      "Epoch:82 | Iter:   60 | Time: 00:00:53 | Train Loss: 1.0896 | Average Loss: 1.5732 \n",
      "Epoch:82 | Iter:   80 | Time: 00:00:53 | Train Loss: 1.4581 | Average Loss: 1.5731 \n",
      "Epoch:82 | Iter:  100 | Time: 00:00:53 | Train Loss: 1.3822 | Average Loss: 1.5728 \n",
      "Epoch:82 | Iter:  120 | Time: 00:00:53 | Train Loss: 0.8402 | Average Loss: 1.5726 \n",
      "Epoch:82 | Iter:  140 | Time: 00:00:53 | Train Loss: 1.5907 | Average Loss: 1.5723 \n",
      "Epoch:82 | Iter:  160 | Time: 00:00:53 | Train Loss: 0.9194 | Average Loss: 1.5720 \n",
      "Epoch:82 | Iter:  180 | Time: 00:00:53 | Train Loss: 1.1126 | Average Loss: 1.5718 \n",
      "Epoch:82 | Iter:  200 | Time: 00:00:53 | Train Loss: 1.5685 | Average Loss: 1.5714 \n",
      "Epoch:82 | Iter:  220 | Time: 00:00:53 | Train Loss: 1.1478 | Average Loss: 1.5711 \n",
      "Epoch:82 | Iter:  240 | Time: 00:00:53 | Train Loss: 0.9203 | Average Loss: 1.5709 \n",
      "Epoch:82 | Iter:  260 | Time: 00:00:53 | Train Loss: 0.8793 | Average Loss: 1.5707 \n",
      "Accuracy: 0.333333 | Time: 00:00:00\n",
      "Epoch:83 | Iter:    0 | Time: 00:00:53 | Train Loss: 1.0301 | Average Loss: 1.5705 \n",
      "Epoch:83 | Iter:   20 | Time: 00:00:53 | Train Loss: 1.7070 | Average Loss: 1.5703 \n",
      "Epoch:83 | Iter:   40 | Time: 00:00:53 | Train Loss: 1.2020 | Average Loss: 1.5700 \n",
      "Epoch:83 | Iter:   60 | Time: 00:00:53 | Train Loss: 1.2777 | Average Loss: 1.5697 \n",
      "Epoch:83 | Iter:   80 | Time: 00:00:53 | Train Loss: 0.9369 | Average Loss: 1.5695 \n",
      "Epoch:83 | Iter:  100 | Time: 00:00:53 | Train Loss: 1.4269 | Average Loss: 1.5694 \n",
      "Epoch:83 | Iter:  120 | Time: 00:00:53 | Train Loss: 0.6076 | Average Loss: 1.5692 \n",
      "Epoch:83 | Iter:  140 | Time: 00:00:53 | Train Loss: 1.6241 | Average Loss: 1.5689 \n",
      "Epoch:83 | Iter:  160 | Time: 00:00:53 | Train Loss: 1.5344 | Average Loss: 1.5687 \n",
      "Epoch:83 | Iter:  180 | Time: 00:00:54 | Train Loss: 1.2211 | Average Loss: 1.5684 \n",
      "Epoch:83 | Iter:  200 | Time: 00:00:54 | Train Loss: 1.4884 | Average Loss: 1.5682 \n",
      "Epoch:83 | Iter:  220 | Time: 00:00:54 | Train Loss: 0.8782 | Average Loss: 1.5679 \n",
      "Epoch:83 | Iter:  240 | Time: 00:00:54 | Train Loss: 0.9921 | Average Loss: 1.5676 \n",
      "Epoch:83 | Iter:  260 | Time: 00:00:54 | Train Loss: 0.9671 | Average Loss: 1.5673 \n",
      "Accuracy: 0.354167 | Time: 00:00:00\n",
      "Epoch:84 | Iter:    0 | Time: 00:00:54 | Train Loss: 0.8061 | Average Loss: 1.5670 \n",
      "Epoch:84 | Iter:   20 | Time: 00:00:54 | Train Loss: 1.3829 | Average Loss: 1.5668 \n",
      "Epoch:84 | Iter:   40 | Time: 00:00:54 | Train Loss: 1.3126 | Average Loss: 1.5665 \n",
      "Epoch:84 | Iter:   60 | Time: 00:00:54 | Train Loss: 1.3895 | Average Loss: 1.5663 \n",
      "Epoch:84 | Iter:   80 | Time: 00:00:54 | Train Loss: 0.9785 | Average Loss: 1.5662 \n",
      "Epoch:84 | Iter:  100 | Time: 00:00:54 | Train Loss: 1.3161 | Average Loss: 1.5659 \n",
      "Epoch:84 | Iter:  120 | Time: 00:00:54 | Train Loss: 0.7494 | Average Loss: 1.5657 \n",
      "Epoch:84 | Iter:  140 | Time: 00:00:54 | Train Loss: 1.7473 | Average Loss: 1.5655 \n",
      "Epoch:84 | Iter:  160 | Time: 00:00:54 | Train Loss: 1.0244 | Average Loss: 1.5653 \n",
      "Epoch:84 | Iter:  180 | Time: 00:00:54 | Train Loss: 1.0404 | Average Loss: 1.5650 \n",
      "Epoch:84 | Iter:  200 | Time: 00:00:54 | Train Loss: 1.1883 | Average Loss: 1.5646 \n",
      "Epoch:84 | Iter:  220 | Time: 00:00:54 | Train Loss: 1.0310 | Average Loss: 1.5644 \n",
      "Epoch:84 | Iter:  240 | Time: 00:00:54 | Train Loss: 1.1083 | Average Loss: 1.5641 \n",
      "Epoch:84 | Iter:  260 | Time: 00:00:55 | Train Loss: 0.9733 | Average Loss: 1.5638 \n",
      "Accuracy: 0.291667 | Time: 00:00:00\n",
      "Epoch:85 | Iter:    0 | Time: 00:00:55 | Train Loss: 0.8863 | Average Loss: 1.5636 \n",
      "Epoch:85 | Iter:   20 | Time: 00:00:55 | Train Loss: 1.6927 | Average Loss: 1.5634 \n",
      "Epoch:85 | Iter:   40 | Time: 00:00:55 | Train Loss: 1.0107 | Average Loss: 1.5631 \n",
      "Epoch:85 | Iter:   60 | Time: 00:00:55 | Train Loss: 1.5731 | Average Loss: 1.5629 \n",
      "Epoch:85 | Iter:   80 | Time: 00:00:55 | Train Loss: 1.0703 | Average Loss: 1.5627 \n",
      "Epoch:85 | Iter:  100 | Time: 00:00:55 | Train Loss: 1.3487 | Average Loss: 1.5624 \n",
      "Epoch:85 | Iter:  120 | Time: 00:00:55 | Train Loss: 0.5301 | Average Loss: 1.5622 \n",
      "Epoch:85 | Iter:  140 | Time: 00:00:55 | Train Loss: 1.5927 | Average Loss: 1.5620 \n",
      "Epoch:85 | Iter:  160 | Time: 00:00:55 | Train Loss: 1.2754 | Average Loss: 1.5617 \n",
      "Epoch:85 | Iter:  180 | Time: 00:00:55 | Train Loss: 1.1675 | Average Loss: 1.5616 \n",
      "Epoch:85 | Iter:  200 | Time: 00:00:55 | Train Loss: 1.2046 | Average Loss: 1.5614 \n",
      "Epoch:85 | Iter:  220 | Time: 00:00:55 | Train Loss: 0.9795 | Average Loss: 1.5611 \n",
      "Epoch:85 | Iter:  240 | Time: 00:00:55 | Train Loss: 1.1427 | Average Loss: 1.5608 \n",
      "Epoch:85 | Iter:  260 | Time: 00:00:55 | Train Loss: 0.7261 | Average Loss: 1.5606 \n",
      "Accuracy: 0.312500 | Time: 00:00:00\n",
      "Epoch:86 | Iter:    0 | Time: 00:00:55 | Train Loss: 0.9791 | Average Loss: 1.5604 \n",
      "Epoch:86 | Iter:   20 | Time: 00:00:55 | Train Loss: 1.2970 | Average Loss: 1.5602 \n",
      "Epoch:86 | Iter:   40 | Time: 00:00:55 | Train Loss: 1.0489 | Average Loss: 1.5599 \n",
      "Epoch:86 | Iter:   60 | Time: 00:00:55 | Train Loss: 1.2401 | Average Loss: 1.5596 \n",
      "Epoch:86 | Iter:   80 | Time: 00:00:56 | Train Loss: 1.0532 | Average Loss: 1.5594 \n",
      "Epoch:86 | Iter:  100 | Time: 00:00:56 | Train Loss: 1.3426 | Average Loss: 1.5592 \n",
      "Epoch:86 | Iter:  120 | Time: 00:00:56 | Train Loss: 0.6304 | Average Loss: 1.5590 \n",
      "Epoch:86 | Iter:  140 | Time: 00:00:56 | Train Loss: 1.5856 | Average Loss: 1.5588 \n",
      "Epoch:86 | Iter:  160 | Time: 00:00:56 | Train Loss: 1.3544 | Average Loss: 1.5585 \n",
      "Epoch:86 | Iter:  180 | Time: 00:00:56 | Train Loss: 1.1669 | Average Loss: 1.5583 \n",
      "Epoch:86 | Iter:  200 | Time: 00:00:56 | Train Loss: 1.4525 | Average Loss: 1.5580 \n",
      "Epoch:86 | Iter:  220 | Time: 00:00:56 | Train Loss: 1.0949 | Average Loss: 1.5579 \n",
      "Epoch:86 | Iter:  240 | Time: 00:00:56 | Train Loss: 0.9733 | Average Loss: 1.5576 \n",
      "Epoch:86 | Iter:  260 | Time: 00:00:56 | Train Loss: 0.9483 | Average Loss: 1.5573 \n",
      "Accuracy: 0.333333 | Time: 00:00:00\n",
      "Epoch:87 | Iter:    0 | Time: 00:00:56 | Train Loss: 1.0272 | Average Loss: 1.5572 \n",
      "Epoch:87 | Iter:   20 | Time: 00:00:56 | Train Loss: 1.2920 | Average Loss: 1.5569 \n",
      "Epoch:87 | Iter:   40 | Time: 00:00:56 | Train Loss: 1.2576 | Average Loss: 1.5566 \n",
      "Epoch:87 | Iter:   60 | Time: 00:00:56 | Train Loss: 1.4338 | Average Loss: 1.5563 \n",
      "Epoch:87 | Iter:   80 | Time: 00:00:56 | Train Loss: 0.9553 | Average Loss: 1.5562 \n",
      "Epoch:87 | Iter:  100 | Time: 00:00:56 | Train Loss: 1.2829 | Average Loss: 1.5560 \n",
      "Epoch:87 | Iter:  120 | Time: 00:00:56 | Train Loss: 0.8108 | Average Loss: 1.5558 \n",
      "Epoch:87 | Iter:  140 | Time: 00:00:56 | Train Loss: 1.5069 | Average Loss: 1.5555 \n",
      "Epoch:87 | Iter:  160 | Time: 00:00:57 | Train Loss: 1.2071 | Average Loss: 1.5553 \n",
      "Epoch:87 | Iter:  180 | Time: 00:00:57 | Train Loss: 0.8067 | Average Loss: 1.5550 \n",
      "Epoch:87 | Iter:  200 | Time: 00:00:57 | Train Loss: 1.1982 | Average Loss: 1.5548 \n",
      "Epoch:87 | Iter:  220 | Time: 00:00:57 | Train Loss: 1.0743 | Average Loss: 1.5546 \n",
      "Epoch:87 | Iter:  240 | Time: 00:00:57 | Train Loss: 0.7510 | Average Loss: 1.5543 \n",
      "Epoch:87 | Iter:  260 | Time: 00:00:57 | Train Loss: 0.6980 | Average Loss: 1.5540 \n",
      "Accuracy: 0.312500 | Time: 00:00:00\n",
      "Epoch:88 | Iter:    0 | Time: 00:00:57 | Train Loss: 0.7686 | Average Loss: 1.5539 \n",
      "Epoch:88 | Iter:   20 | Time: 00:00:57 | Train Loss: 1.3198 | Average Loss: 1.5536 \n",
      "Epoch:88 | Iter:   40 | Time: 00:00:57 | Train Loss: 1.1925 | Average Loss: 1.5534 \n",
      "Epoch:88 | Iter:   60 | Time: 00:00:57 | Train Loss: 1.4423 | Average Loss: 1.5532 \n",
      "Epoch:88 | Iter:   80 | Time: 00:00:57 | Train Loss: 1.2301 | Average Loss: 1.5530 \n",
      "Epoch:88 | Iter:  100 | Time: 00:00:57 | Train Loss: 1.5693 | Average Loss: 1.5528 \n",
      "Epoch:88 | Iter:  120 | Time: 00:00:57 | Train Loss: 0.6399 | Average Loss: 1.5526 \n",
      "Epoch:88 | Iter:  140 | Time: 00:00:57 | Train Loss: 1.8945 | Average Loss: 1.5523 \n",
      "Epoch:88 | Iter:  160 | Time: 00:00:57 | Train Loss: 0.8743 | Average Loss: 1.5521 \n",
      "Epoch:88 | Iter:  180 | Time: 00:00:57 | Train Loss: 0.9180 | Average Loss: 1.5518 \n",
      "Epoch:88 | Iter:  200 | Time: 00:00:57 | Train Loss: 1.2716 | Average Loss: 1.5515 \n",
      "Epoch:88 | Iter:  220 | Time: 00:00:58 | Train Loss: 1.2948 | Average Loss: 1.5513 \n",
      "Epoch:88 | Iter:  240 | Time: 00:00:58 | Train Loss: 0.9353 | Average Loss: 1.5510 \n",
      "Epoch:88 | Iter:  260 | Time: 00:00:58 | Train Loss: 1.0008 | Average Loss: 1.5508 \n",
      "Accuracy: 0.333333 | Time: 00:00:00\n",
      "Epoch:89 | Iter:    0 | Time: 00:00:58 | Train Loss: 0.8804 | Average Loss: 1.5506 \n",
      "Epoch:89 | Iter:   20 | Time: 00:00:58 | Train Loss: 1.6719 | Average Loss: 1.5504 \n",
      "Epoch:89 | Iter:   40 | Time: 00:00:58 | Train Loss: 0.9230 | Average Loss: 1.5502 \n",
      "Epoch:89 | Iter:   60 | Time: 00:00:58 | Train Loss: 1.4359 | Average Loss: 1.5499 \n",
      "Epoch:89 | Iter:   80 | Time: 00:00:58 | Train Loss: 1.0017 | Average Loss: 1.5497 \n",
      "Epoch:89 | Iter:  100 | Time: 00:00:58 | Train Loss: 1.2534 | Average Loss: 1.5495 \n",
      "Epoch:89 | Iter:  120 | Time: 00:00:58 | Train Loss: 0.5560 | Average Loss: 1.5492 \n",
      "Epoch:89 | Iter:  140 | Time: 00:00:58 | Train Loss: 1.7308 | Average Loss: 1.5490 \n",
      "Epoch:89 | Iter:  160 | Time: 00:00:58 | Train Loss: 1.4553 | Average Loss: 1.5487 \n",
      "Epoch:89 | Iter:  180 | Time: 00:00:58 | Train Loss: 1.0853 | Average Loss: 1.5485 \n",
      "Epoch:89 | Iter:  200 | Time: 00:00:58 | Train Loss: 1.2083 | Average Loss: 1.5483 \n",
      "Epoch:89 | Iter:  220 | Time: 00:00:58 | Train Loss: 1.0614 | Average Loss: 1.5481 \n",
      "Epoch:89 | Iter:  240 | Time: 00:00:58 | Train Loss: 0.9193 | Average Loss: 1.5478 \n",
      "Epoch:89 | Iter:  260 | Time: 00:00:58 | Train Loss: 0.7823 | Average Loss: 1.5475 \n",
      "Accuracy: 0.312500 | Time: 00:00:00\n",
      "Epoch:90 | Iter:    0 | Time: 00:00:58 | Train Loss: 0.8703 | Average Loss: 1.5473 \n",
      "Epoch:90 | Iter:   20 | Time: 00:00:58 | Train Loss: 1.1958 | Average Loss: 1.5471 \n",
      "Epoch:90 | Iter:   40 | Time: 00:00:58 | Train Loss: 1.2303 | Average Loss: 1.5468 \n",
      "Epoch:90 | Iter:   60 | Time: 00:00:59 | Train Loss: 1.1636 | Average Loss: 1.5466 \n",
      "Epoch:90 | Iter:   80 | Time: 00:00:59 | Train Loss: 0.9380 | Average Loss: 1.5463 \n",
      "Epoch:90 | Iter:  100 | Time: 00:00:59 | Train Loss: 1.6360 | Average Loss: 1.5461 \n",
      "Epoch:90 | Iter:  120 | Time: 00:00:59 | Train Loss: 0.5219 | Average Loss: 1.5459 \n",
      "Epoch:90 | Iter:  140 | Time: 00:00:59 | Train Loss: 1.4677 | Average Loss: 1.5457 \n",
      "Epoch:90 | Iter:  160 | Time: 00:00:59 | Train Loss: 1.5601 | Average Loss: 1.5455 \n",
      "Epoch:90 | Iter:  180 | Time: 00:00:59 | Train Loss: 1.0042 | Average Loss: 1.5452 \n",
      "Epoch:90 | Iter:  200 | Time: 00:00:59 | Train Loss: 1.1945 | Average Loss: 1.5449 \n",
      "Epoch:90 | Iter:  220 | Time: 00:00:59 | Train Loss: 1.1322 | Average Loss: 1.5447 \n",
      "Epoch:90 | Iter:  240 | Time: 00:00:59 | Train Loss: 1.2331 | Average Loss: 1.5445 \n",
      "Epoch:90 | Iter:  260 | Time: 00:00:59 | Train Loss: 1.0543 | Average Loss: 1.5443 \n",
      "Accuracy: 0.291667 | Time: 00:00:00\n",
      "Epoch:91 | Iter:    0 | Time: 00:00:59 | Train Loss: 0.7109 | Average Loss: 1.5441 \n",
      "Epoch:91 | Iter:   20 | Time: 00:00:59 | Train Loss: 1.5361 | Average Loss: 1.5440 \n",
      "Epoch:91 | Iter:   40 | Time: 00:00:59 | Train Loss: 0.9634 | Average Loss: 1.5438 \n",
      "Epoch:91 | Iter:   60 | Time: 00:00:59 | Train Loss: 1.1519 | Average Loss: 1.5435 \n",
      "Epoch:91 | Iter:   80 | Time: 00:00:59 | Train Loss: 1.3689 | Average Loss: 1.5434 \n",
      "Epoch:91 | Iter:  100 | Time: 00:00:59 | Train Loss: 1.5446 | Average Loss: 1.5432 \n",
      "Epoch:91 | Iter:  120 | Time: 00:00:59 | Train Loss: 0.4182 | Average Loss: 1.5430 \n",
      "Epoch:91 | Iter:  140 | Time: 00:00:59 | Train Loss: 1.7122 | Average Loss: 1.5428 \n",
      "Epoch:91 | Iter:  160 | Time: 00:01:00 | Train Loss: 1.7444 | Average Loss: 1.5427 \n",
      "Epoch:91 | Iter:  180 | Time: 00:01:00 | Train Loss: 0.9540 | Average Loss: 1.5424 \n",
      "Epoch:91 | Iter:  200 | Time: 00:01:00 | Train Loss: 1.3835 | Average Loss: 1.5421 \n",
      "Epoch:91 | Iter:  220 | Time: 00:01:00 | Train Loss: 1.0837 | Average Loss: 1.5419 \n",
      "Epoch:91 | Iter:  240 | Time: 00:01:00 | Train Loss: 1.0195 | Average Loss: 1.5417 \n",
      "Epoch:91 | Iter:  260 | Time: 00:01:00 | Train Loss: 0.9027 | Average Loss: 1.5415 \n",
      "Accuracy: 0.312500 | Time: 00:00:00\n",
      "Epoch:92 | Iter:    0 | Time: 00:01:00 | Train Loss: 0.9194 | Average Loss: 1.5413 \n",
      "Epoch:92 | Iter:   20 | Time: 00:01:00 | Train Loss: 1.6539 | Average Loss: 1.5412 \n",
      "Epoch:92 | Iter:   40 | Time: 00:01:00 | Train Loss: 1.4096 | Average Loss: 1.5410 \n",
      "Epoch:92 | Iter:   60 | Time: 00:01:00 | Train Loss: 1.1672 | Average Loss: 1.5407 \n",
      "Epoch:92 | Iter:   80 | Time: 00:01:00 | Train Loss: 1.0103 | Average Loss: 1.5405 \n",
      "Epoch:92 | Iter:  100 | Time: 00:01:00 | Train Loss: 1.2675 | Average Loss: 1.5403 \n",
      "Epoch:92 | Iter:  120 | Time: 00:01:00 | Train Loss: 0.7875 | Average Loss: 1.5402 \n",
      "Epoch:92 | Iter:  140 | Time: 00:01:00 | Train Loss: 1.8418 | Average Loss: 1.5400 \n",
      "Epoch:92 | Iter:  160 | Time: 00:01:00 | Train Loss: 1.2282 | Average Loss: 1.5398 \n",
      "Epoch:92 | Iter:  180 | Time: 00:01:00 | Train Loss: 1.0290 | Average Loss: 1.5396 \n",
      "Epoch:92 | Iter:  200 | Time: 00:01:00 | Train Loss: 0.8366 | Average Loss: 1.5394 \n",
      "Epoch:92 | Iter:  220 | Time: 00:01:00 | Train Loss: 0.8637 | Average Loss: 1.5391 \n",
      "Epoch:92 | Iter:  240 | Time: 00:01:00 | Train Loss: 1.2077 | Average Loss: 1.5389 \n",
      "Epoch:92 | Iter:  260 | Time: 00:01:01 | Train Loss: 0.6996 | Average Loss: 1.5386 \n",
      "Accuracy: 0.312500 | Time: 00:00:00\n",
      "Epoch:93 | Iter:    0 | Time: 00:01:01 | Train Loss: 0.9494 | Average Loss: 1.5385 \n",
      "Epoch:93 | Iter:   20 | Time: 00:01:01 | Train Loss: 1.6754 | Average Loss: 1.5383 \n",
      "Epoch:93 | Iter:   40 | Time: 00:01:01 | Train Loss: 1.0156 | Average Loss: 1.5381 \n",
      "Epoch:93 | Iter:   60 | Time: 00:01:01 | Train Loss: 1.2772 | Average Loss: 1.5378 \n",
      "Epoch:93 | Iter:   80 | Time: 00:01:01 | Train Loss: 0.9711 | Average Loss: 1.5376 \n",
      "Epoch:93 | Iter:  100 | Time: 00:01:01 | Train Loss: 1.5419 | Average Loss: 1.5374 \n",
      "Epoch:93 | Iter:  120 | Time: 00:01:01 | Train Loss: 0.7807 | Average Loss: 1.5372 \n",
      "Epoch:93 | Iter:  140 | Time: 00:01:01 | Train Loss: 1.5178 | Average Loss: 1.5370 \n",
      "Epoch:93 | Iter:  160 | Time: 00:01:01 | Train Loss: 1.1074 | Average Loss: 1.5368 \n",
      "Epoch:93 | Iter:  180 | Time: 00:01:01 | Train Loss: 0.9950 | Average Loss: 1.5366 \n",
      "Epoch:93 | Iter:  200 | Time: 00:01:01 | Train Loss: 1.0295 | Average Loss: 1.5363 \n",
      "Epoch:93 | Iter:  220 | Time: 00:01:01 | Train Loss: 1.1122 | Average Loss: 1.5361 \n",
      "Epoch:93 | Iter:  240 | Time: 00:01:01 | Train Loss: 0.8746 | Average Loss: 1.5358 \n",
      "Epoch:93 | Iter:  260 | Time: 00:01:01 | Train Loss: 1.0523 | Average Loss: 1.5356 \n",
      "Accuracy: 0.312500 | Time: 00:00:00\n",
      "Epoch:94 | Iter:    0 | Time: 00:01:01 | Train Loss: 0.8248 | Average Loss: 1.5354 \n",
      "Epoch:94 | Iter:   20 | Time: 00:01:01 | Train Loss: 1.7664 | Average Loss: 1.5352 \n",
      "Epoch:94 | Iter:   40 | Time: 00:01:02 | Train Loss: 1.0964 | Average Loss: 1.5350 \n",
      "Epoch:94 | Iter:   60 | Time: 00:01:02 | Train Loss: 1.2765 | Average Loss: 1.5347 \n",
      "Epoch:94 | Iter:   80 | Time: 00:01:02 | Train Loss: 1.2196 | Average Loss: 1.5346 \n",
      "Epoch:94 | Iter:  100 | Time: 00:01:02 | Train Loss: 1.2672 | Average Loss: 1.5344 \n",
      "Epoch:94 | Iter:  120 | Time: 00:01:02 | Train Loss: 0.7373 | Average Loss: 1.5343 \n",
      "Epoch:94 | Iter:  140 | Time: 00:01:02 | Train Loss: 1.9525 | Average Loss: 1.5340 \n",
      "Epoch:94 | Iter:  160 | Time: 00:01:02 | Train Loss: 1.8456 | Average Loss: 1.5339 \n",
      "Epoch:94 | Iter:  180 | Time: 00:01:02 | Train Loss: 0.9664 | Average Loss: 1.5336 \n",
      "Epoch:94 | Iter:  200 | Time: 00:01:02 | Train Loss: 1.1126 | Average Loss: 1.5333 \n",
      "Epoch:94 | Iter:  220 | Time: 00:01:02 | Train Loss: 0.9847 | Average Loss: 1.5331 \n",
      "Epoch:94 | Iter:  240 | Time: 00:01:02 | Train Loss: 1.3684 | Average Loss: 1.5329 \n",
      "Epoch:94 | Iter:  260 | Time: 00:01:02 | Train Loss: 1.4577 | Average Loss: 1.5327 \n",
      "Accuracy: 0.250000 | Time: 00:00:00\n",
      "Epoch:95 | Iter:    0 | Time: 00:01:02 | Train Loss: 1.1538 | Average Loss: 1.5325 \n",
      "Epoch:95 | Iter:   20 | Time: 00:01:02 | Train Loss: 1.3887 | Average Loss: 1.5323 \n",
      "Epoch:95 | Iter:   40 | Time: 00:01:02 | Train Loss: 1.3299 | Average Loss: 1.5321 \n",
      "Epoch:95 | Iter:   60 | Time: 00:01:02 | Train Loss: 1.2578 | Average Loss: 1.5319 \n",
      "Epoch:95 | Iter:   80 | Time: 00:01:02 | Train Loss: 0.9124 | Average Loss: 1.5317 \n",
      "Epoch:95 | Iter:  100 | Time: 00:01:02 | Train Loss: 1.5561 | Average Loss: 1.5315 \n",
      "Epoch:95 | Iter:  120 | Time: 00:01:03 | Train Loss: 0.8590 | Average Loss: 1.5314 \n",
      "Epoch:95 | Iter:  140 | Time: 00:01:03 | Train Loss: 1.2209 | Average Loss: 1.5312 \n",
      "Epoch:95 | Iter:  160 | Time: 00:01:03 | Train Loss: 1.2525 | Average Loss: 1.5310 \n",
      "Epoch:95 | Iter:  180 | Time: 00:01:03 | Train Loss: 1.0679 | Average Loss: 1.5307 \n",
      "Epoch:95 | Iter:  200 | Time: 00:01:03 | Train Loss: 1.3406 | Average Loss: 1.5305 \n",
      "Epoch:95 | Iter:  220 | Time: 00:01:03 | Train Loss: 0.8757 | Average Loss: 1.5303 \n",
      "Epoch:95 | Iter:  240 | Time: 00:01:03 | Train Loss: 1.0354 | Average Loss: 1.5300 \n",
      "Epoch:95 | Iter:  260 | Time: 00:01:03 | Train Loss: 0.7182 | Average Loss: 1.5297 \n",
      "Accuracy: 0.395833 | Time: 00:00:00\n",
      "Epoch:96 | Iter:    0 | Time: 00:01:03 | Train Loss: 0.7866 | Average Loss: 1.5296 \n",
      "Epoch:96 | Iter:   20 | Time: 00:01:03 | Train Loss: 1.5960 | Average Loss: 1.5294 \n",
      "Epoch:96 | Iter:   40 | Time: 00:01:03 | Train Loss: 1.1117 | Average Loss: 1.5292 \n",
      "Epoch:96 | Iter:   60 | Time: 00:01:03 | Train Loss: 1.3239 | Average Loss: 1.5289 \n",
      "Epoch:96 | Iter:   80 | Time: 00:01:03 | Train Loss: 1.0812 | Average Loss: 1.5287 \n",
      "Epoch:96 | Iter:  100 | Time: 00:01:03 | Train Loss: 1.4716 | Average Loss: 1.5285 \n",
      "Epoch:96 | Iter:  120 | Time: 00:01:03 | Train Loss: 0.6086 | Average Loss: 1.5283 \n",
      "Epoch:96 | Iter:  140 | Time: 00:01:03 | Train Loss: 1.2237 | Average Loss: 1.5282 \n",
      "Epoch:96 | Iter:  160 | Time: 00:01:03 | Train Loss: 1.3402 | Average Loss: 1.5280 \n",
      "Epoch:96 | Iter:  180 | Time: 00:01:03 | Train Loss: 1.1579 | Average Loss: 1.5278 \n",
      "Epoch:96 | Iter:  200 | Time: 00:01:03 | Train Loss: 1.3160 | Average Loss: 1.5275 \n",
      "Epoch:96 | Iter:  220 | Time: 00:01:03 | Train Loss: 0.7431 | Average Loss: 1.5273 \n",
      "Epoch:96 | Iter:  240 | Time: 00:01:04 | Train Loss: 1.0561 | Average Loss: 1.5270 \n",
      "Epoch:96 | Iter:  260 | Time: 00:01:04 | Train Loss: 0.8502 | Average Loss: 1.5268 \n",
      "Accuracy: 0.312500 | Time: 00:00:00\n",
      "Epoch:97 | Iter:    0 | Time: 00:01:04 | Train Loss: 0.8202 | Average Loss: 1.5267 \n",
      "Epoch:97 | Iter:   20 | Time: 00:01:04 | Train Loss: 1.1761 | Average Loss: 1.5265 \n",
      "Epoch:97 | Iter:   40 | Time: 00:01:04 | Train Loss: 1.0063 | Average Loss: 1.5263 \n",
      "Epoch:97 | Iter:   60 | Time: 00:01:04 | Train Loss: 1.2372 | Average Loss: 1.5260 \n",
      "Epoch:97 | Iter:   80 | Time: 00:01:04 | Train Loss: 0.8929 | Average Loss: 1.5259 \n",
      "Epoch:97 | Iter:  100 | Time: 00:01:04 | Train Loss: 1.4259 | Average Loss: 1.5257 \n",
      "Epoch:97 | Iter:  120 | Time: 00:01:04 | Train Loss: 0.6833 | Average Loss: 1.5256 \n",
      "Epoch:97 | Iter:  140 | Time: 00:01:04 | Train Loss: 1.3475 | Average Loss: 1.5254 \n",
      "Epoch:97 | Iter:  160 | Time: 00:01:04 | Train Loss: 1.3648 | Average Loss: 1.5252 \n",
      "Epoch:97 | Iter:  180 | Time: 00:01:04 | Train Loss: 0.8900 | Average Loss: 1.5250 \n",
      "Epoch:97 | Iter:  200 | Time: 00:01:04 | Train Loss: 1.0440 | Average Loss: 1.5248 \n",
      "Epoch:97 | Iter:  220 | Time: 00:01:04 | Train Loss: 1.1026 | Average Loss: 1.5245 \n",
      "Epoch:97 | Iter:  240 | Time: 00:01:04 | Train Loss: 1.0489 | Average Loss: 1.5243 \n",
      "Epoch:97 | Iter:  260 | Time: 00:01:04 | Train Loss: 0.9431 | Average Loss: 1.5240 \n",
      "Accuracy: 0.354167 | Time: 00:00:00\n",
      "Epoch:98 | Iter:    0 | Time: 00:01:04 | Train Loss: 0.9567 | Average Loss: 1.5238 \n",
      "Epoch:98 | Iter:   20 | Time: 00:01:04 | Train Loss: 1.4321 | Average Loss: 1.5237 \n",
      "Epoch:98 | Iter:   40 | Time: 00:01:04 | Train Loss: 1.1823 | Average Loss: 1.5235 \n",
      "Epoch:98 | Iter:   60 | Time: 00:01:04 | Train Loss: 1.0329 | Average Loss: 1.5232 \n",
      "Epoch:98 | Iter:   80 | Time: 00:01:05 | Train Loss: 0.9746 | Average Loss: 1.5231 \n",
      "Epoch:98 | Iter:  100 | Time: 00:01:05 | Train Loss: 1.3404 | Average Loss: 1.5229 \n",
      "Epoch:98 | Iter:  120 | Time: 00:01:05 | Train Loss: 0.5207 | Average Loss: 1.5227 \n",
      "Epoch:98 | Iter:  140 | Time: 00:01:05 | Train Loss: 1.4975 | Average Loss: 1.5226 \n",
      "Epoch:98 | Iter:  160 | Time: 00:01:05 | Train Loss: 1.4381 | Average Loss: 1.5224 \n",
      "Epoch:98 | Iter:  180 | Time: 00:01:05 | Train Loss: 0.9776 | Average Loss: 1.5222 \n",
      "Epoch:98 | Iter:  200 | Time: 00:01:05 | Train Loss: 1.2209 | Average Loss: 1.5220 \n",
      "Epoch:98 | Iter:  220 | Time: 00:01:05 | Train Loss: 1.2018 | Average Loss: 1.5218 \n",
      "Epoch:98 | Iter:  240 | Time: 00:01:05 | Train Loss: 1.1639 | Average Loss: 1.5215 \n",
      "Epoch:98 | Iter:  260 | Time: 00:01:05 | Train Loss: 0.8622 | Average Loss: 1.5213 \n",
      "Accuracy: 0.333333 | Time: 00:00:00\n",
      "Epoch:99 | Iter:    0 | Time: 00:01:05 | Train Loss: 1.2028 | Average Loss: 1.5211 \n",
      "Epoch:99 | Iter:   20 | Time: 00:01:05 | Train Loss: 1.3129 | Average Loss: 1.5209 \n",
      "Epoch:99 | Iter:   40 | Time: 00:01:05 | Train Loss: 1.1292 | Average Loss: 1.5207 \n",
      "Epoch:99 | Iter:   60 | Time: 00:01:05 | Train Loss: 1.2553 | Average Loss: 1.5205 \n",
      "Epoch:99 | Iter:   80 | Time: 00:01:05 | Train Loss: 1.0658 | Average Loss: 1.5204 \n",
      "Epoch:99 | Iter:  100 | Time: 00:01:05 | Train Loss: 1.1548 | Average Loss: 1.5202 \n",
      "Epoch:99 | Iter:  120 | Time: 00:01:05 | Train Loss: 0.8783 | Average Loss: 1.5200 \n",
      "Epoch:99 | Iter:  140 | Time: 00:01:05 | Train Loss: 1.5524 | Average Loss: 1.5198 \n",
      "Epoch:99 | Iter:  160 | Time: 00:01:05 | Train Loss: 1.4524 | Average Loss: 1.5196 \n",
      "Epoch:99 | Iter:  180 | Time: 00:01:06 | Train Loss: 1.3779 | Average Loss: 1.5194 \n",
      "Epoch:99 | Iter:  200 | Time: 00:01:06 | Train Loss: 1.2613 | Average Loss: 1.5191 \n",
      "Epoch:99 | Iter:  220 | Time: 00:01:06 | Train Loss: 0.7909 | Average Loss: 1.5189 \n",
      "Epoch:99 | Iter:  240 | Time: 00:01:06 | Train Loss: 1.2253 | Average Loss: 1.5187 \n",
      "Epoch:99 | Iter:  260 | Time: 00:01:06 | Train Loss: 0.7994 | Average Loss: 1.5184 \n",
      "Accuracy: 0.333333 | Time: 00:00:00\n",
      "Epoch:100 | Iter:    0 | Time: 00:01:06 | Train Loss: 0.8774 | Average Loss: 1.5183 \n",
      "Epoch:100 | Iter:   20 | Time: 00:01:06 | Train Loss: 1.3775 | Average Loss: 1.5182 \n",
      "Epoch:100 | Iter:   40 | Time: 00:01:06 | Train Loss: 0.9288 | Average Loss: 1.5180 \n",
      "Epoch:100 | Iter:   60 | Time: 00:01:06 | Train Loss: 1.2858 | Average Loss: 1.5177 \n",
      "Epoch:100 | Iter:   80 | Time: 00:01:06 | Train Loss: 0.9449 | Average Loss: 1.5175 \n",
      "Epoch:100 | Iter:  100 | Time: 00:01:06 | Train Loss: 1.5729 | Average Loss: 1.5174 \n",
      "Epoch:100 | Iter:  120 | Time: 00:01:06 | Train Loss: 0.6582 | Average Loss: 1.5172 \n",
      "Epoch:100 | Iter:  140 | Time: 00:01:06 | Train Loss: 1.3396 | Average Loss: 1.5170 \n",
      "Epoch:100 | Iter:  160 | Time: 00:01:06 | Train Loss: 1.3419 | Average Loss: 1.5168 \n",
      "Epoch:100 | Iter:  180 | Time: 00:01:06 | Train Loss: 0.8716 | Average Loss: 1.5165 \n",
      "Epoch:100 | Iter:  200 | Time: 00:01:06 | Train Loss: 1.0625 | Average Loss: 1.5163 \n",
      "Epoch:100 | Iter:  220 | Time: 00:01:06 | Train Loss: 1.2247 | Average Loss: 1.5161 \n",
      "Epoch:100 | Iter:  240 | Time: 00:01:06 | Train Loss: 1.2198 | Average Loss: 1.5159 \n",
      "Epoch:100 | Iter:  260 | Time: 00:01:06 | Train Loss: 0.9761 | Average Loss: 1.5157 \n",
      "Accuracy: 0.270833 | Time: 00:00:00\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------------\n",
    "#       Start Training & Evaluation\n",
    "#--------------------------------------------------\n",
    "net = TNet()\n",
    "train_option = {}\n",
    "train_option['lr'] = 0.00095\n",
    "train_option['epoch'] = 100\n",
    "train_option['device'] = 'gpu'\n",
    "trainModel(net, trainloader_small, train_option, testloader_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MxlW_6DqLzmM"
   },
   "source": [
    "\n",
    "Method 1: add batch normalization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IkePS4VnHnZO"
   },
   "source": [
    "### Technique 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "j5ozpyI0T8aE"
   },
   "outputs": [],
   "source": [
    "#--------------------------------------------------\n",
    "#       Define Network Architecture\n",
    "#--------------------------------------------------\n",
    "\n",
    "class TNet(nn.Module):\n",
    "    def __init__(self):\n",
    "          super(TNet, self).__init__()\n",
    "          self.inners = nn.Sequential(\n",
    "                nn.Conv2d(1, 16, 3),\n",
    "                nn.Tanh(),\n",
    "                nn.BatchNorm2d(16),\n",
    "                nn.MaxPool2d(2, stride=2),\n",
    "                nn.Conv2d(16, 32, 3),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm2d(32),\n",
    "                nn.MaxPool2d(4, stride=2),\n",
    "                nn.Dropout(0.5),\n",
    "                nn.Conv2d(32, 64, 3),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.MaxPool2d(2, stride=2),\n",
    "          )\n",
    "          \n",
    "          self.classifier = nn.Sequential(\n",
    "              nn.Linear(1600, 100),\n",
    "              nn.ReLU(),\n",
    "              nn.Linear(100, 20)\n",
    "          )\n",
    "          # self.inners = nn.Sequential(\n",
    "          #   nn.Conv2d(1, 16, 3),\n",
    "          #   nn.BatchNorm2d(16),  # Add Batch Normalization\n",
    "          #   nn.Tanh(),\n",
    "          #   nn.MaxPool2d(2, stride=2),\n",
    "          #   nn.Conv2d(16, 16, 3),\n",
    "          #   nn.BatchNorm2d(16),  # Add Batch Normalization\n",
    "          #   nn.Tanh(),\n",
    "          #   nn.MaxPool2d(2, stride=2),\n",
    "          #   nn.Dropout(0.5),\n",
    "          #   nn.Conv2d(16, 16, 3),\n",
    "          #   nn.BatchNorm2d(16),  # Add Batch Normalization\n",
    "          #   nn.Tanh(),\n",
    "          #   nn.MaxPool2d(2, stride=2),\n",
    "          # )\n",
    "          \n",
    "          # self.classifier = nn.Sequential(\n",
    "          #   nn.Linear(576, 20)\n",
    "          # )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.inners(x)  \n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "sdy9SRdFsyWZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Iter:    0 | Time: 00:00:00 | Train Loss: 2.9222 | Average Loss: 2.9222 \n",
      "Epoch: 1 | Iter:   20 | Time: 00:00:00 | Train Loss: 2.8443 | Average Loss: 3.1660 \n",
      "Epoch: 1 | Iter:   40 | Time: 00:00:00 | Train Loss: 2.8189 | Average Loss: 2.9930 \n",
      "Epoch: 1 | Iter:   60 | Time: 00:00:00 | Train Loss: 2.4450 | Average Loss: 2.8717 \n",
      "Epoch: 1 | Iter:   80 | Time: 00:00:00 | Train Loss: 2.1218 | Average Loss: 2.8050 \n",
      "Epoch: 1 | Iter:  100 | Time: 00:00:00 | Train Loss: 2.1117 | Average Loss: 2.7207 \n",
      "Epoch: 1 | Iter:  120 | Time: 00:00:00 | Train Loss: 2.1392 | Average Loss: 2.6918 \n",
      "Epoch: 1 | Iter:  140 | Time: 00:00:00 | Train Loss: 2.2186 | Average Loss: 2.6465 \n",
      "Epoch: 1 | Iter:  160 | Time: 00:00:00 | Train Loss: 2.6859 | Average Loss: 2.6137 \n",
      "Epoch: 1 | Iter:  180 | Time: 00:00:00 | Train Loss: 2.2196 | Average Loss: 2.5884 \n",
      "Epoch: 1 | Iter:  200 | Time: 00:00:00 | Train Loss: 2.3806 | Average Loss: 2.5568 \n",
      "Epoch: 1 | Iter:  220 | Time: 00:00:00 | Train Loss: 2.0908 | Average Loss: 2.5267 \n",
      "Epoch: 1 | Iter:  240 | Time: 00:00:00 | Train Loss: 2.1531 | Average Loss: 2.4994 \n",
      "Epoch: 1 | Iter:  260 | Time: 00:00:00 | Train Loss: 1.7244 | Average Loss: 2.4670 \n",
      "Accuracy: 0.208333 | Time: 00:00:00\n",
      "Epoch: 2 | Iter:    0 | Time: 00:00:00 | Train Loss: 1.7810 | Average Loss: 2.4451 \n",
      "Epoch: 2 | Iter:   20 | Time: 00:00:00 | Train Loss: 2.3808 | Average Loss: 2.4230 \n",
      "Epoch: 2 | Iter:   40 | Time: 00:00:00 | Train Loss: 2.1229 | Average Loss: 2.3993 \n",
      "Epoch: 2 | Iter:   60 | Time: 00:00:00 | Train Loss: 1.8996 | Average Loss: 2.3831 \n",
      "Epoch: 2 | Iter:   80 | Time: 00:00:00 | Train Loss: 1.6601 | Average Loss: 2.3669 \n",
      "Epoch: 2 | Iter:  100 | Time: 00:00:01 | Train Loss: 1.6083 | Average Loss: 2.3445 \n",
      "Epoch: 2 | Iter:  120 | Time: 00:00:01 | Train Loss: 1.5508 | Average Loss: 2.3270 \n",
      "Epoch: 2 | Iter:  140 | Time: 00:00:01 | Train Loss: 2.0243 | Average Loss: 2.3042 \n",
      "Epoch: 2 | Iter:  160 | Time: 00:00:01 | Train Loss: 2.0504 | Average Loss: 2.2869 \n",
      "Epoch: 2 | Iter:  180 | Time: 00:00:01 | Train Loss: 1.6381 | Average Loss: 2.2665 \n",
      "Epoch: 2 | Iter:  200 | Time: 00:00:01 | Train Loss: 1.9029 | Average Loss: 2.2482 \n",
      "Epoch: 2 | Iter:  220 | Time: 00:00:01 | Train Loss: 1.5536 | Average Loss: 2.2311 \n",
      "Epoch: 2 | Iter:  240 | Time: 00:00:01 | Train Loss: 1.5344 | Average Loss: 2.2135 \n",
      "Epoch: 2 | Iter:  260 | Time: 00:00:01 | Train Loss: 1.1611 | Average Loss: 2.1924 \n",
      "Accuracy: 0.208333 | Time: 00:00:00\n",
      "Epoch: 3 | Iter:    0 | Time: 00:00:01 | Train Loss: 1.4981 | Average Loss: 2.1767 \n",
      "Epoch: 3 | Iter:   20 | Time: 00:00:01 | Train Loss: 1.7138 | Average Loss: 2.1601 \n",
      "Epoch: 3 | Iter:   40 | Time: 00:00:01 | Train Loss: 1.9228 | Average Loss: 2.1439 \n",
      "Epoch: 3 | Iter:   60 | Time: 00:00:01 | Train Loss: 1.5236 | Average Loss: 2.1293 \n",
      "Epoch: 3 | Iter:   80 | Time: 00:00:01 | Train Loss: 1.2117 | Average Loss: 2.1143 \n",
      "Epoch: 3 | Iter:  100 | Time: 00:00:01 | Train Loss: 1.6449 | Average Loss: 2.0970 \n",
      "Epoch: 3 | Iter:  120 | Time: 00:00:01 | Train Loss: 1.0426 | Average Loss: 2.0828 \n",
      "Epoch: 3 | Iter:  140 | Time: 00:00:01 | Train Loss: 1.4951 | Average Loss: 2.0648 \n",
      "Epoch: 3 | Iter:  160 | Time: 00:00:01 | Train Loss: 1.9425 | Average Loss: 2.0510 \n",
      "Epoch: 3 | Iter:  180 | Time: 00:00:01 | Train Loss: 1.2013 | Average Loss: 2.0343 \n",
      "Epoch: 3 | Iter:  200 | Time: 00:00:01 | Train Loss: 1.5825 | Average Loss: 2.0205 \n",
      "Epoch: 3 | Iter:  220 | Time: 00:00:02 | Train Loss: 1.5814 | Average Loss: 2.0088 \n",
      "Epoch: 3 | Iter:  240 | Time: 00:00:02 | Train Loss: 1.1518 | Average Loss: 1.9948 \n",
      "Epoch: 3 | Iter:  260 | Time: 00:00:02 | Train Loss: 1.1898 | Average Loss: 1.9776 \n",
      "Accuracy: 0.145833 | Time: 00:00:00\n",
      "Epoch: 4 | Iter:    0 | Time: 00:00:02 | Train Loss: 1.0798 | Average Loss: 1.9666 \n",
      "Epoch: 4 | Iter:   20 | Time: 00:00:02 | Train Loss: 1.4793 | Average Loss: 1.9543 \n",
      "Epoch: 4 | Iter:   40 | Time: 00:00:02 | Train Loss: 1.3712 | Average Loss: 1.9410 \n",
      "Epoch: 4 | Iter:   60 | Time: 00:00:02 | Train Loss: 1.3852 | Average Loss: 1.9280 \n",
      "Epoch: 4 | Iter:   80 | Time: 00:00:02 | Train Loss: 0.8157 | Average Loss: 1.9134 \n",
      "Epoch: 4 | Iter:  100 | Time: 00:00:02 | Train Loss: 0.9781 | Average Loss: 1.8984 \n",
      "Epoch: 4 | Iter:  120 | Time: 00:00:02 | Train Loss: 0.8701 | Average Loss: 1.8854 \n",
      "Epoch: 4 | Iter:  140 | Time: 00:00:02 | Train Loss: 1.4415 | Average Loss: 1.8717 \n",
      "Epoch: 4 | Iter:  160 | Time: 00:00:02 | Train Loss: 1.5174 | Average Loss: 1.8590 \n",
      "Epoch: 4 | Iter:  180 | Time: 00:00:02 | Train Loss: 1.4882 | Average Loss: 1.8469 \n",
      "Epoch: 4 | Iter:  200 | Time: 00:00:02 | Train Loss: 1.1430 | Average Loss: 1.8351 \n",
      "Epoch: 4 | Iter:  220 | Time: 00:00:02 | Train Loss: 1.3596 | Average Loss: 1.8233 \n",
      "Epoch: 4 | Iter:  240 | Time: 00:00:02 | Train Loss: 0.7879 | Average Loss: 1.8097 \n",
      "Epoch: 4 | Iter:  260 | Time: 00:00:02 | Train Loss: 0.8977 | Average Loss: 1.7976 \n",
      "Accuracy: 0.229167 | Time: 00:00:00\n",
      "Epoch: 5 | Iter:    0 | Time: 00:00:02 | Train Loss: 0.8100 | Average Loss: 1.7858 \n",
      "Epoch: 5 | Iter:   20 | Time: 00:00:02 | Train Loss: 1.2595 | Average Loss: 1.7734 \n",
      "Epoch: 5 | Iter:   40 | Time: 00:00:02 | Train Loss: 1.0426 | Average Loss: 1.7609 \n",
      "Epoch: 5 | Iter:   60 | Time: 00:00:03 | Train Loss: 1.0092 | Average Loss: 1.7481 \n",
      "Epoch: 5 | Iter:   80 | Time: 00:00:03 | Train Loss: 0.4987 | Average Loss: 1.7354 \n",
      "Epoch: 5 | Iter:  100 | Time: 00:00:03 | Train Loss: 0.8399 | Average Loss: 1.7225 \n",
      "Epoch: 5 | Iter:  120 | Time: 00:00:03 | Train Loss: 0.5699 | Average Loss: 1.7109 \n",
      "Epoch: 5 | Iter:  140 | Time: 00:00:03 | Train Loss: 1.1387 | Average Loss: 1.6990 \n",
      "Epoch: 5 | Iter:  160 | Time: 00:00:03 | Train Loss: 1.0118 | Average Loss: 1.6863 \n",
      "Epoch: 5 | Iter:  180 | Time: 00:00:03 | Train Loss: 0.9614 | Average Loss: 1.6736 \n",
      "Epoch: 5 | Iter:  200 | Time: 00:00:03 | Train Loss: 1.0376 | Average Loss: 1.6630 \n",
      "Epoch: 5 | Iter:  220 | Time: 00:00:03 | Train Loss: 0.9433 | Average Loss: 1.6529 \n",
      "Epoch: 5 | Iter:  240 | Time: 00:00:03 | Train Loss: 0.4941 | Average Loss: 1.6416 \n",
      "Epoch: 5 | Iter:  260 | Time: 00:00:03 | Train Loss: 0.6872 | Average Loss: 1.6298 \n",
      "Accuracy: 0.354167 | Time: 00:00:00\n",
      "Epoch: 6 | Iter:    0 | Time: 00:00:03 | Train Loss: 0.5012 | Average Loss: 1.6210 \n",
      "Epoch: 6 | Iter:   20 | Time: 00:00:03 | Train Loss: 1.3184 | Average Loss: 1.6113 \n",
      "Epoch: 6 | Iter:   40 | Time: 00:00:03 | Train Loss: 0.6753 | Average Loss: 1.6007 \n",
      "Epoch: 6 | Iter:   60 | Time: 00:00:03 | Train Loss: 0.7904 | Average Loss: 1.5895 \n",
      "Epoch: 6 | Iter:   80 | Time: 00:00:03 | Train Loss: 0.5164 | Average Loss: 1.5787 \n",
      "Epoch: 6 | Iter:  100 | Time: 00:00:03 | Train Loss: 0.5782 | Average Loss: 1.5666 \n",
      "Epoch: 6 | Iter:  120 | Time: 00:00:03 | Train Loss: 0.2190 | Average Loss: 1.5554 \n",
      "Epoch: 6 | Iter:  140 | Time: 00:00:03 | Train Loss: 1.2211 | Average Loss: 1.5447 \n",
      "Epoch: 6 | Iter:  160 | Time: 00:00:03 | Train Loss: 1.0252 | Average Loss: 1.5347 \n",
      "Epoch: 6 | Iter:  180 | Time: 00:00:04 | Train Loss: 0.7211 | Average Loss: 1.5240 \n",
      "Epoch: 6 | Iter:  200 | Time: 00:00:04 | Train Loss: 0.4238 | Average Loss: 1.5149 \n",
      "Epoch: 6 | Iter:  220 | Time: 00:00:04 | Train Loss: 1.2635 | Average Loss: 1.5060 \n",
      "Epoch: 6 | Iter:  240 | Time: 00:00:04 | Train Loss: 0.4612 | Average Loss: 1.4963 \n",
      "Epoch: 6 | Iter:  260 | Time: 00:00:04 | Train Loss: 0.2954 | Average Loss: 1.4865 \n",
      "Accuracy: 0.312500 | Time: 00:00:00\n",
      "Epoch: 7 | Iter:    0 | Time: 00:00:04 | Train Loss: 0.4970 | Average Loss: 1.4782 \n",
      "Epoch: 7 | Iter:   20 | Time: 00:00:04 | Train Loss: 0.8989 | Average Loss: 1.4688 \n",
      "Epoch: 7 | Iter:   40 | Time: 00:00:04 | Train Loss: 0.6081 | Average Loss: 1.4596 \n",
      "Epoch: 7 | Iter:   60 | Time: 00:00:04 | Train Loss: 0.6148 | Average Loss: 1.4498 \n",
      "Epoch: 7 | Iter:   80 | Time: 00:00:04 | Train Loss: 0.4940 | Average Loss: 1.4405 \n",
      "Epoch: 7 | Iter:  100 | Time: 00:00:04 | Train Loss: 0.4507 | Average Loss: 1.4310 \n",
      "Epoch: 7 | Iter:  120 | Time: 00:00:04 | Train Loss: 0.6476 | Average Loss: 1.4224 \n",
      "Epoch: 7 | Iter:  140 | Time: 00:00:04 | Train Loss: 1.0126 | Average Loss: 1.4137 \n",
      "Epoch: 7 | Iter:  160 | Time: 00:00:04 | Train Loss: 0.6814 | Average Loss: 1.4054 \n",
      "Epoch: 7 | Iter:  180 | Time: 00:00:04 | Train Loss: 0.7498 | Average Loss: 1.3967 \n",
      "Epoch: 7 | Iter:  200 | Time: 00:00:04 | Train Loss: 0.0541 | Average Loss: 1.3880 \n",
      "Epoch: 7 | Iter:  220 | Time: 00:00:04 | Train Loss: 0.8300 | Average Loss: 1.3802 \n",
      "Epoch: 7 | Iter:  240 | Time: 00:00:04 | Train Loss: 0.1724 | Average Loss: 1.3713 \n",
      "Epoch: 7 | Iter:  260 | Time: 00:00:04 | Train Loss: 0.4737 | Average Loss: 1.3621 \n",
      "Accuracy: 0.270833 | Time: 00:00:00\n",
      "Epoch: 8 | Iter:    0 | Time: 00:00:04 | Train Loss: 0.1687 | Average Loss: 1.3543 \n",
      "Epoch: 8 | Iter:   20 | Time: 00:00:05 | Train Loss: 0.4890 | Average Loss: 1.3461 \n",
      "Epoch: 8 | Iter:   40 | Time: 00:00:05 | Train Loss: 0.7943 | Average Loss: 1.3374 \n",
      "Epoch: 8 | Iter:   60 | Time: 00:00:05 | Train Loss: 0.6365 | Average Loss: 1.3291 \n",
      "Epoch: 8 | Iter:   80 | Time: 00:00:05 | Train Loss: 0.2285 | Average Loss: 1.3206 \n",
      "Epoch: 8 | Iter:  100 | Time: 00:00:05 | Train Loss: 0.7571 | Average Loss: 1.3129 \n",
      "Epoch: 8 | Iter:  120 | Time: 00:00:05 | Train Loss: 0.2910 | Average Loss: 1.3056 \n",
      "Epoch: 8 | Iter:  140 | Time: 00:00:05 | Train Loss: 0.8105 | Average Loss: 1.2977 \n",
      "Epoch: 8 | Iter:  160 | Time: 00:00:05 | Train Loss: 0.6088 | Average Loss: 1.2909 \n",
      "Epoch: 8 | Iter:  180 | Time: 00:00:05 | Train Loss: 0.5984 | Average Loss: 1.2835 \n",
      "Epoch: 8 | Iter:  200 | Time: 00:00:05 | Train Loss: 0.4234 | Average Loss: 1.2760 \n",
      "Epoch: 8 | Iter:  220 | Time: 00:00:05 | Train Loss: 0.7456 | Average Loss: 1.2687 \n",
      "Epoch: 8 | Iter:  240 | Time: 00:00:05 | Train Loss: 0.2721 | Average Loss: 1.2618 \n",
      "Epoch: 8 | Iter:  260 | Time: 00:00:05 | Train Loss: 0.4810 | Average Loss: 1.2550 \n",
      "Accuracy: 0.312500 | Time: 00:00:00\n",
      "Epoch: 9 | Iter:    0 | Time: 00:00:05 | Train Loss: 0.5482 | Average Loss: 1.2485 \n",
      "Epoch: 9 | Iter:   20 | Time: 00:00:05 | Train Loss: 0.6326 | Average Loss: 1.2416 \n",
      "Epoch: 9 | Iter:   40 | Time: 00:00:05 | Train Loss: 0.5960 | Average Loss: 1.2349 \n",
      "Epoch: 9 | Iter:   60 | Time: 00:00:05 | Train Loss: 0.7521 | Average Loss: 1.2281 \n",
      "Epoch: 9 | Iter:   80 | Time: 00:00:05 | Train Loss: 0.5510 | Average Loss: 1.2211 \n",
      "Epoch: 9 | Iter:  100 | Time: 00:00:05 | Train Loss: 0.5765 | Average Loss: 1.2149 \n",
      "Epoch: 9 | Iter:  120 | Time: 00:00:06 | Train Loss: 0.1505 | Average Loss: 1.2089 \n",
      "Epoch: 9 | Iter:  140 | Time: 00:00:06 | Train Loss: 0.5479 | Average Loss: 1.2033 \n",
      "Epoch: 9 | Iter:  160 | Time: 00:00:06 | Train Loss: 0.3162 | Average Loss: 1.1965 \n",
      "Epoch: 9 | Iter:  180 | Time: 00:00:06 | Train Loss: 0.2635 | Average Loss: 1.1899 \n",
      "Epoch: 9 | Iter:  200 | Time: 00:00:06 | Train Loss: 0.2587 | Average Loss: 1.1832 \n",
      "Epoch: 9 | Iter:  220 | Time: 00:00:06 | Train Loss: 0.2728 | Average Loss: 1.1765 \n",
      "Epoch: 9 | Iter:  240 | Time: 00:00:06 | Train Loss: 0.6252 | Average Loss: 1.1704 \n",
      "Epoch: 9 | Iter:  260 | Time: 00:00:06 | Train Loss: 0.1694 | Average Loss: 1.1638 \n",
      "Accuracy: 0.375000 | Time: 00:00:00\n",
      "Epoch:10 | Iter:    0 | Time: 00:00:06 | Train Loss: 0.0864 | Average Loss: 1.1576 \n",
      "Epoch:10 | Iter:   20 | Time: 00:00:06 | Train Loss: 0.5793 | Average Loss: 1.1511 \n",
      "Epoch:10 | Iter:   40 | Time: 00:00:06 | Train Loss: 0.2632 | Average Loss: 1.1452 \n",
      "Epoch:10 | Iter:   60 | Time: 00:00:06 | Train Loss: 0.2506 | Average Loss: 1.1395 \n",
      "Epoch:10 | Iter:   80 | Time: 00:00:06 | Train Loss: 0.0943 | Average Loss: 1.1334 \n",
      "Epoch:10 | Iter:  100 | Time: 00:00:06 | Train Loss: 0.2501 | Average Loss: 1.1275 \n",
      "Epoch:10 | Iter:  120 | Time: 00:00:06 | Train Loss: 0.6103 | Average Loss: 1.1220 \n",
      "Epoch:10 | Iter:  140 | Time: 00:00:06 | Train Loss: 0.2962 | Average Loss: 1.1165 \n",
      "Epoch:10 | Iter:  160 | Time: 00:00:06 | Train Loss: 0.4493 | Average Loss: 1.1114 \n",
      "Epoch:10 | Iter:  180 | Time: 00:00:06 | Train Loss: 0.1925 | Average Loss: 1.1054 \n",
      "Epoch:10 | Iter:  200 | Time: 00:00:06 | Train Loss: 0.0633 | Average Loss: 1.0999 \n",
      "Epoch:10 | Iter:  220 | Time: 00:00:06 | Train Loss: 0.4087 | Average Loss: 1.0949 \n",
      "Epoch:10 | Iter:  240 | Time: 00:00:07 | Train Loss: 0.0692 | Average Loss: 1.0890 \n",
      "Epoch:10 | Iter:  260 | Time: 00:00:07 | Train Loss: 0.0648 | Average Loss: 1.0834 \n",
      "Accuracy: 0.291667 | Time: 00:00:00\n",
      "Epoch:11 | Iter:    0 | Time: 00:00:07 | Train Loss: 0.2115 | Average Loss: 1.0784 \n",
      "Epoch:11 | Iter:   20 | Time: 00:00:07 | Train Loss: 0.3316 | Average Loss: 1.0729 \n",
      "Epoch:11 | Iter:   40 | Time: 00:00:07 | Train Loss: 0.5183 | Average Loss: 1.0673 \n",
      "Epoch:11 | Iter:   60 | Time: 00:00:07 | Train Loss: 0.3032 | Average Loss: 1.0620 \n",
      "Epoch:11 | Iter:   80 | Time: 00:00:07 | Train Loss: 0.4590 | Average Loss: 1.0573 \n",
      "Epoch:11 | Iter:  100 | Time: 00:00:07 | Train Loss: 0.1755 | Average Loss: 1.0522 \n",
      "Epoch:11 | Iter:  120 | Time: 00:00:07 | Train Loss: 0.3361 | Average Loss: 1.0473 \n",
      "Epoch:11 | Iter:  140 | Time: 00:00:07 | Train Loss: 0.5603 | Average Loss: 1.0425 \n",
      "Epoch:11 | Iter:  160 | Time: 00:00:07 | Train Loss: 0.5703 | Average Loss: 1.0387 \n",
      "Epoch:11 | Iter:  180 | Time: 00:00:07 | Train Loss: 0.4489 | Average Loss: 1.0339 \n",
      "Epoch:11 | Iter:  200 | Time: 00:00:07 | Train Loss: 0.2147 | Average Loss: 1.0294 \n",
      "Epoch:11 | Iter:  220 | Time: 00:00:07 | Train Loss: 0.2053 | Average Loss: 1.0258 \n",
      "Epoch:11 | Iter:  240 | Time: 00:00:07 | Train Loss: 0.2595 | Average Loss: 1.0207 \n",
      "Epoch:11 | Iter:  260 | Time: 00:00:07 | Train Loss: 0.2447 | Average Loss: 1.0165 \n",
      "Accuracy: 0.250000 | Time: 00:00:00\n",
      "Epoch:12 | Iter:    0 | Time: 00:00:07 | Train Loss: 0.6493 | Average Loss: 1.0124 \n",
      "Epoch:12 | Iter:   20 | Time: 00:00:07 | Train Loss: 0.5292 | Average Loss: 1.0074 \n",
      "Epoch:12 | Iter:   40 | Time: 00:00:07 | Train Loss: 0.0706 | Average Loss: 1.0022 \n",
      "Epoch:12 | Iter:   60 | Time: 00:00:07 | Train Loss: 0.0723 | Average Loss: 0.9973 \n",
      "Epoch:12 | Iter:   80 | Time: 00:00:08 | Train Loss: 0.0058 | Average Loss: 0.9927 \n",
      "Epoch:12 | Iter:  100 | Time: 00:00:08 | Train Loss: 0.1472 | Average Loss: 0.9880 \n",
      "Epoch:12 | Iter:  120 | Time: 00:00:08 | Train Loss: 0.1056 | Average Loss: 0.9833 \n",
      "Epoch:12 | Iter:  140 | Time: 00:00:08 | Train Loss: 0.3231 | Average Loss: 0.9786 \n",
      "Epoch:12 | Iter:  160 | Time: 00:00:08 | Train Loss: 0.5468 | Average Loss: 0.9741 \n",
      "Epoch:12 | Iter:  180 | Time: 00:00:08 | Train Loss: 0.6228 | Average Loss: 0.9698 \n",
      "Epoch:12 | Iter:  200 | Time: 00:00:08 | Train Loss: 0.4032 | Average Loss: 0.9654 \n",
      "Epoch:12 | Iter:  220 | Time: 00:00:08 | Train Loss: 0.3953 | Average Loss: 0.9609 \n",
      "Epoch:12 | Iter:  240 | Time: 00:00:08 | Train Loss: 0.1020 | Average Loss: 0.9565 \n",
      "Epoch:12 | Iter:  260 | Time: 00:00:08 | Train Loss: 0.2086 | Average Loss: 0.9521 \n",
      "Accuracy: 0.291667 | Time: 00:00:00\n",
      "Epoch:13 | Iter:    0 | Time: 00:00:08 | Train Loss: 0.5021 | Average Loss: 0.9485 \n",
      "Epoch:13 | Iter:   20 | Time: 00:00:08 | Train Loss: 0.3167 | Average Loss: 0.9446 \n",
      "Epoch:13 | Iter:   40 | Time: 00:00:08 | Train Loss: 0.0980 | Average Loss: 0.9405 \n",
      "Epoch:13 | Iter:   60 | Time: 00:00:08 | Train Loss: 0.3481 | Average Loss: 0.9365 \n",
      "Epoch:13 | Iter:   80 | Time: 00:00:08 | Train Loss: 0.2586 | Average Loss: 0.9329 \n",
      "Epoch:13 | Iter:  100 | Time: 00:00:08 | Train Loss: 0.3488 | Average Loss: 0.9293 \n",
      "Epoch:13 | Iter:  120 | Time: 00:00:08 | Train Loss: 0.3459 | Average Loss: 0.9256 \n",
      "Epoch:13 | Iter:  140 | Time: 00:00:08 | Train Loss: 0.3213 | Average Loss: 0.9227 \n",
      "Epoch:13 | Iter:  160 | Time: 00:00:08 | Train Loss: 0.5218 | Average Loss: 0.9196 \n",
      "Epoch:13 | Iter:  180 | Time: 00:00:09 | Train Loss: 0.3725 | Average Loss: 0.9159 \n",
      "Epoch:13 | Iter:  200 | Time: 00:00:09 | Train Loss: 0.2963 | Average Loss: 0.9126 \n",
      "Epoch:13 | Iter:  220 | Time: 00:00:09 | Train Loss: 0.2682 | Average Loss: 0.9089 \n",
      "Epoch:13 | Iter:  240 | Time: 00:00:09 | Train Loss: 0.0911 | Average Loss: 0.9049 \n",
      "Epoch:13 | Iter:  260 | Time: 00:00:09 | Train Loss: 0.1477 | Average Loss: 0.9019 \n",
      "Accuracy: 0.291667 | Time: 00:00:00\n",
      "Epoch:14 | Iter:    0 | Time: 00:00:09 | Train Loss: 0.1417 | Average Loss: 0.8983 \n",
      "Epoch:14 | Iter:   20 | Time: 00:00:09 | Train Loss: 0.2667 | Average Loss: 0.8948 \n",
      "Epoch:14 | Iter:   40 | Time: 00:00:09 | Train Loss: 0.6262 | Average Loss: 0.8915 \n",
      "Epoch:14 | Iter:   60 | Time: 00:00:09 | Train Loss: 0.4328 | Average Loss: 0.8880 \n",
      "Epoch:14 | Iter:   80 | Time: 00:00:09 | Train Loss: 0.0172 | Average Loss: 0.8848 \n",
      "Epoch:14 | Iter:  100 | Time: 00:00:09 | Train Loss: 0.1356 | Average Loss: 0.8812 \n",
      "Epoch:14 | Iter:  120 | Time: 00:00:09 | Train Loss: 0.5383 | Average Loss: 0.8777 \n",
      "Epoch:14 | Iter:  140 | Time: 00:00:09 | Train Loss: 0.0426 | Average Loss: 0.8743 \n",
      "Epoch:14 | Iter:  160 | Time: 00:00:09 | Train Loss: 0.8845 | Average Loss: 0.8714 \n",
      "Epoch:14 | Iter:  180 | Time: 00:00:09 | Train Loss: 0.1793 | Average Loss: 0.8687 \n",
      "Epoch:14 | Iter:  200 | Time: 00:00:09 | Train Loss: 0.2222 | Average Loss: 0.8657 \n",
      "Epoch:14 | Iter:  220 | Time: 00:00:09 | Train Loss: 0.3042 | Average Loss: 0.8624 \n",
      "Epoch:14 | Iter:  240 | Time: 00:00:09 | Train Loss: 0.1019 | Average Loss: 0.8588 \n",
      "Epoch:14 | Iter:  260 | Time: 00:00:09 | Train Loss: 0.0518 | Average Loss: 0.8557 \n",
      "Accuracy: 0.312500 | Time: 00:00:00\n",
      "Epoch:15 | Iter:    0 | Time: 00:00:10 | Train Loss: 0.1188 | Average Loss: 0.8527 \n",
      "Epoch:15 | Iter:   20 | Time: 00:00:10 | Train Loss: 0.3682 | Average Loss: 0.8493 \n",
      "Epoch:15 | Iter:   40 | Time: 00:00:10 | Train Loss: 0.3494 | Average Loss: 0.8465 \n",
      "Epoch:15 | Iter:   60 | Time: 00:00:10 | Train Loss: 0.1514 | Average Loss: 0.8434 \n",
      "Epoch:15 | Iter:   80 | Time: 00:00:10 | Train Loss: 0.0828 | Average Loss: 0.8398 \n",
      "Epoch:15 | Iter:  100 | Time: 00:00:10 | Train Loss: 0.0982 | Average Loss: 0.8371 \n",
      "Epoch:15 | Iter:  120 | Time: 00:00:10 | Train Loss: 0.2302 | Average Loss: 0.8341 \n",
      "Epoch:15 | Iter:  140 | Time: 00:00:10 | Train Loss: 0.3627 | Average Loss: 0.8311 \n",
      "Epoch:15 | Iter:  160 | Time: 00:00:10 | Train Loss: 0.2875 | Average Loss: 0.8282 \n",
      "Epoch:15 | Iter:  180 | Time: 00:00:10 | Train Loss: 0.0660 | Average Loss: 0.8251 \n",
      "Epoch:15 | Iter:  200 | Time: 00:00:10 | Train Loss: 0.2563 | Average Loss: 0.8224 \n",
      "Epoch:15 | Iter:  220 | Time: 00:00:10 | Train Loss: 0.2204 | Average Loss: 0.8195 \n",
      "Epoch:15 | Iter:  240 | Time: 00:00:10 | Train Loss: 0.2092 | Average Loss: 0.8167 \n",
      "Epoch:15 | Iter:  260 | Time: 00:00:10 | Train Loss: 0.0375 | Average Loss: 0.8135 \n",
      "Accuracy: 0.250000 | Time: 00:00:00\n",
      "Epoch:16 | Iter:    0 | Time: 00:00:10 | Train Loss: 0.1871 | Average Loss: 0.8109 \n",
      "Epoch:16 | Iter:   20 | Time: 00:00:10 | Train Loss: 0.2877 | Average Loss: 0.8080 \n",
      "Epoch:16 | Iter:   40 | Time: 00:00:10 | Train Loss: 0.7500 | Average Loss: 0.8051 \n",
      "Epoch:16 | Iter:   60 | Time: 00:00:10 | Train Loss: 0.1295 | Average Loss: 0.8020 \n",
      "Epoch:16 | Iter:   80 | Time: 00:00:10 | Train Loss: 0.1303 | Average Loss: 0.7989 \n",
      "Epoch:16 | Iter:  100 | Time: 00:00:10 | Train Loss: 0.3332 | Average Loss: 0.7960 \n",
      "Epoch:16 | Iter:  120 | Time: 00:00:11 | Train Loss: 0.5319 | Average Loss: 0.7934 \n",
      "Epoch:16 | Iter:  140 | Time: 00:00:11 | Train Loss: 0.3141 | Average Loss: 0.7907 \n",
      "Epoch:16 | Iter:  160 | Time: 00:00:11 | Train Loss: 0.5073 | Average Loss: 0.7881 \n",
      "Epoch:16 | Iter:  180 | Time: 00:00:11 | Train Loss: 0.9964 | Average Loss: 0.7855 \n",
      "Epoch:16 | Iter:  200 | Time: 00:00:11 | Train Loss: 0.3750 | Average Loss: 0.7827 \n",
      "Epoch:16 | Iter:  220 | Time: 00:00:11 | Train Loss: 0.0212 | Average Loss: 0.7800 \n",
      "Epoch:16 | Iter:  240 | Time: 00:00:11 | Train Loss: 0.0960 | Average Loss: 0.7777 \n",
      "Epoch:16 | Iter:  260 | Time: 00:00:11 | Train Loss: 0.2266 | Average Loss: 0.7751 \n",
      "Accuracy: 0.270833 | Time: 00:00:00\n",
      "Epoch:17 | Iter:    0 | Time: 00:00:11 | Train Loss: 0.5091 | Average Loss: 0.7726 \n",
      "Epoch:17 | Iter:   20 | Time: 00:00:11 | Train Loss: 0.4074 | Average Loss: 0.7699 \n",
      "Epoch:17 | Iter:   40 | Time: 00:00:11 | Train Loss: 0.0860 | Average Loss: 0.7671 \n",
      "Epoch:17 | Iter:   60 | Time: 00:00:11 | Train Loss: 0.3897 | Average Loss: 0.7644 \n",
      "Epoch:17 | Iter:   80 | Time: 00:00:11 | Train Loss: 0.0385 | Average Loss: 0.7618 \n",
      "Epoch:17 | Iter:  100 | Time: 00:00:11 | Train Loss: 0.0964 | Average Loss: 0.7593 \n",
      "Epoch:17 | Iter:  120 | Time: 00:00:11 | Train Loss: 0.8583 | Average Loss: 0.7568 \n",
      "Epoch:17 | Iter:  140 | Time: 00:00:11 | Train Loss: 0.7013 | Average Loss: 0.7546 \n",
      "Epoch:17 | Iter:  160 | Time: 00:00:11 | Train Loss: 0.2165 | Average Loss: 0.7523 \n",
      "Epoch:17 | Iter:  180 | Time: 00:00:11 | Train Loss: 0.1241 | Average Loss: 0.7502 \n",
      "Epoch:17 | Iter:  200 | Time: 00:00:11 | Train Loss: 0.1160 | Average Loss: 0.7485 \n",
      "Epoch:17 | Iter:  220 | Time: 00:00:12 | Train Loss: 0.0539 | Average Loss: 0.7468 \n",
      "Epoch:17 | Iter:  240 | Time: 00:00:12 | Train Loss: 0.2630 | Average Loss: 0.7445 \n",
      "Epoch:17 | Iter:  260 | Time: 00:00:12 | Train Loss: 0.0216 | Average Loss: 0.7422 \n",
      "Accuracy: 0.312500 | Time: 00:00:00\n",
      "Epoch:18 | Iter:    0 | Time: 00:00:12 | Train Loss: 0.0269 | Average Loss: 0.7402 \n",
      "Epoch:18 | Iter:   20 | Time: 00:00:12 | Train Loss: 0.5247 | Average Loss: 0.7381 \n",
      "Epoch:18 | Iter:   40 | Time: 00:00:12 | Train Loss: 0.3407 | Average Loss: 0.7361 \n",
      "Epoch:18 | Iter:   60 | Time: 00:00:12 | Train Loss: 0.0853 | Average Loss: 0.7338 \n",
      "Epoch:18 | Iter:   80 | Time: 00:00:12 | Train Loss: 0.0060 | Average Loss: 0.7315 \n",
      "Epoch:18 | Iter:  100 | Time: 00:00:12 | Train Loss: 0.4017 | Average Loss: 0.7292 \n",
      "Epoch:18 | Iter:  120 | Time: 00:00:12 | Train Loss: 0.3731 | Average Loss: 0.7271 \n",
      "Epoch:18 | Iter:  140 | Time: 00:00:12 | Train Loss: 0.0847 | Average Loss: 0.7253 \n",
      "Epoch:18 | Iter:  160 | Time: 00:00:12 | Train Loss: 0.1745 | Average Loss: 0.7231 \n",
      "Epoch:18 | Iter:  180 | Time: 00:00:12 | Train Loss: 0.4090 | Average Loss: 0.7212 \n",
      "Epoch:18 | Iter:  200 | Time: 00:00:12 | Train Loss: 0.0288 | Average Loss: 0.7192 \n",
      "Epoch:18 | Iter:  220 | Time: 00:00:12 | Train Loss: 0.1041 | Average Loss: 0.7177 \n",
      "Epoch:18 | Iter:  240 | Time: 00:00:12 | Train Loss: 0.0184 | Average Loss: 0.7155 \n",
      "Epoch:18 | Iter:  260 | Time: 00:00:12 | Train Loss: 0.0086 | Average Loss: 0.7135 \n",
      "Accuracy: 0.291667 | Time: 00:00:00\n",
      "Epoch:19 | Iter:    0 | Time: 00:00:12 | Train Loss: 0.0197 | Average Loss: 0.7116 \n",
      "Epoch:19 | Iter:   20 | Time: 00:00:12 | Train Loss: 0.0600 | Average Loss: 0.7094 \n",
      "Epoch:19 | Iter:   40 | Time: 00:00:12 | Train Loss: 0.0311 | Average Loss: 0.7073 \n",
      "Epoch:19 | Iter:   60 | Time: 00:00:13 | Train Loss: 0.2634 | Average Loss: 0.7050 \n",
      "Epoch:19 | Iter:   80 | Time: 00:00:13 | Train Loss: 0.2875 | Average Loss: 0.7032 \n",
      "Epoch:19 | Iter:  100 | Time: 00:00:13 | Train Loss: 0.0504 | Average Loss: 0.7011 \n",
      "Epoch:19 | Iter:  120 | Time: 00:00:13 | Train Loss: 0.4010 | Average Loss: 0.6991 \n",
      "Epoch:19 | Iter:  140 | Time: 00:00:13 | Train Loss: 0.1513 | Average Loss: 0.6971 \n",
      "Epoch:19 | Iter:  160 | Time: 00:00:13 | Train Loss: 0.0114 | Average Loss: 0.6951 \n",
      "Epoch:19 | Iter:  180 | Time: 00:00:13 | Train Loss: 0.0734 | Average Loss: 0.6929 \n",
      "Epoch:19 | Iter:  200 | Time: 00:00:13 | Train Loss: 0.0827 | Average Loss: 0.6910 \n",
      "Epoch:19 | Iter:  220 | Time: 00:00:13 | Train Loss: 0.1203 | Average Loss: 0.6891 \n",
      "Epoch:19 | Iter:  240 | Time: 00:00:13 | Train Loss: 0.0715 | Average Loss: 0.6869 \n",
      "Epoch:19 | Iter:  260 | Time: 00:00:13 | Train Loss: 0.0699 | Average Loss: 0.6850 \n",
      "Accuracy: 0.375000 | Time: 00:00:00\n",
      "Epoch:20 | Iter:    0 | Time: 00:00:13 | Train Loss: 0.1359 | Average Loss: 0.6831 \n",
      "Epoch:20 | Iter:   20 | Time: 00:00:13 | Train Loss: 0.1137 | Average Loss: 0.6810 \n",
      "Epoch:20 | Iter:   40 | Time: 00:00:13 | Train Loss: 0.2593 | Average Loss: 0.6791 \n",
      "Epoch:20 | Iter:   60 | Time: 00:00:13 | Train Loss: 0.0503 | Average Loss: 0.6771 \n",
      "Epoch:20 | Iter:   80 | Time: 00:00:13 | Train Loss: 0.0214 | Average Loss: 0.6750 \n",
      "Epoch:20 | Iter:  100 | Time: 00:00:13 | Train Loss: 0.1774 | Average Loss: 0.6733 \n",
      "Epoch:20 | Iter:  120 | Time: 00:00:13 | Train Loss: 0.0044 | Average Loss: 0.6714 \n",
      "Epoch:20 | Iter:  140 | Time: 00:00:14 | Train Loss: 0.1010 | Average Loss: 0.6695 \n",
      "Epoch:20 | Iter:  160 | Time: 00:00:14 | Train Loss: 0.1181 | Average Loss: 0.6677 \n",
      "Epoch:20 | Iter:  180 | Time: 00:00:14 | Train Loss: 0.0721 | Average Loss: 0.6659 \n",
      "Epoch:20 | Iter:  200 | Time: 00:00:14 | Train Loss: 0.0254 | Average Loss: 0.6642 \n",
      "Epoch:20 | Iter:  220 | Time: 00:00:14 | Train Loss: 0.0313 | Average Loss: 0.6623 \n",
      "Epoch:20 | Iter:  240 | Time: 00:00:14 | Train Loss: 0.1631 | Average Loss: 0.6603 \n",
      "Epoch:20 | Iter:  260 | Time: 00:00:14 | Train Loss: 0.3598 | Average Loss: 0.6586 \n",
      "Accuracy: 0.291667 | Time: 00:00:00\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------------\n",
    "#       Start Training & Evaluation\n",
    "#--------------------------------------------------\n",
    "\n",
    "net = TNet()\n",
    "train_option = {}\n",
    "train_option['lr'] = 0.002\n",
    "train_option['epoch'] = 20\n",
    "train_option['device'] = 'gpu'\n",
    "trainModel(net, trainloader_small, train_option, testloader_small)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MxlW_6DqLzmM"
   },
   "source": [
    "\n",
    "Method 2: replace activation layer into Tanh with larger learning rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "nBHKIxzAYYM2",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## Fine Tuning a Pre-Trained Deep Network\n",
    "Our convolutional network to this point isn't \"deep\". Fortunately, the representations learned by deep convolutional networks is that they generalize surprisingly well to other recognition tasks.\n",
    "\n",
    "But how do we use an existing deep network for a new recognition task? For instance, the [AlexNet](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks) network has 1000 units in the final layer corresponding to the 1000 ImageNet categories.\n",
    "\n",
    "- Many pre-trained models are available in PyTorch at [here](http://pytorch.org/docs/master/torchvision/models.html).\n",
    "- To fine-tune a pretrained network using PyTorch, please read this [tutorial](http://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ALojw_R5LrRt"
   },
   "source": [
    "### AlexNet\n",
    "\n",
    "**Strategy A**: *Fine-tune* an existing network. In this scenario we take an existing network, replace the final layer (or more) with random weights, and train the entire network again with images and ground truth labels for the recognition task. We are effectively treating the pre-trained deep network as a better initialization than the random weights used when training from scratch. When we don't have enough training data to train a complex network from scratch (e.g. with the 22 classes) this is an attractive option. Fine-tuning can work far better than Strategy B of taking the activations directly from a pre-trained CNN. For example, in [this paper](http://www.cc.gatech.edu/~hays/papers/deep_geo.pdf) from CVPR 2015, there wasn't enough data to train a deep network from scratch, but fine tuning led to 4 times higher accuracy than using off-the-shelf networks directly.\n",
    "\n",
    "Below is an implementation for Strategy A to fine-tune a pre-trained **AlexNet** for this scene classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "VLG3WtEmYYM3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images from class: 0\n",
      "Loading images from class: 1\n",
      "Loading images from class: 2\n",
      "Loading images from class: 3\n",
      "Loading images from class: 4\n",
      "Loading images from class: 5\n",
      "Loading images from class: 6\n",
      "Loading images from class: 7\n",
      "Loading images from class: 8\n",
      "Loading images from class: 9\n",
      "Loading images from class: 10\n",
      "Loading images from class: 11\n",
      "Loading images from class: 12\n",
      "Loading images from class: 13\n",
      "Loading images from class: 14\n",
      "Loading images from class: 15\n",
      "Loading images from class: 16\n",
      "Loading images from class: 17\n",
      "Loading images from class: 18\n",
      "Loading images from class: 19\n",
      "Finish loading 34 minibatches (batch_size=16) of training samples.\n",
      "Loading images from class: 0\n",
      "Loading images from class: 1\n",
      "Loading images from class: 2\n",
      "Loading images from class: 3\n",
      "Loading images from class: 4\n",
      "Loading images from class: 5\n",
      "Loading images from class: 6\n",
      "Loading images from class: 7\n",
      "Loading images from class: 8\n",
      "Loading images from class: 9\n",
      "Loading images from class: 10\n",
      "Loading images from class: 11\n",
      "Loading images from class: 12\n",
      "Loading images from class: 13\n",
      "Loading images from class: 14\n",
      "Loading images from class: 15\n",
      "Loading images from class: 16\n",
      "Loading images from class: 17\n",
      "Loading images from class: 18\n",
      "Loading images from class: 19\n",
      "Finish loading 3 minibatches (batch_size=16) of testing samples.\n"
     ]
    }
   ],
   "source": [
    "# reload data with a larger size\n",
    "img_size = (224, 224)\n",
    "batch_size = 16 # training sample number per batch\n",
    "\n",
    "# load training dataset\n",
    "trainloader_large = list(load_dataset('./data/train/', img_size, batch_size=batch_size, shuffle=True,\n",
    "                                      augment=False, is_color=True, zero_centered=True))\n",
    "train_num = len(trainloader_large)\n",
    "print(\"Finish loading %d minibatches (batch_size=%d) of training samples.\" % (train_num, batch_size))\n",
    "\n",
    "# load testing dataset\n",
    "testloader_large = list(load_dataset('./data/test/', img_size, num_per_class=50, batch_size=batch_size, is_color=True, zero_centered=True))\n",
    "test_num = len(testloader_large)\n",
    "print(\"Finish loading %d minibatches (batch_size=%d) of testing samples.\" % (test_num, batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "btOal_ampEnm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Iter:    0 | Time: 00:00:00 | Train Loss: 3.1710 | Average Loss: 3.1710 \n",
      "Epoch: 1 | Iter:   20 | Time: 00:00:00 | Train Loss: 1.9814 | Average Loss: 2.7591 \n",
      "Accuracy: 0.520833 | Time: 00:00:00\n",
      "Epoch: 2 | Iter:    0 | Time: 00:00:00 | Train Loss: 0.7317 | Average Loss: 2.4149 \n",
      "Epoch: 2 | Iter:   20 | Time: 00:00:00 | Train Loss: 0.7037 | Average Loss: 1.8953 \n",
      "Accuracy: 0.708333 | Time: 00:00:00\n",
      "Epoch: 3 | Iter:    0 | Time: 00:00:00 | Train Loss: 0.1731 | Average Loss: 1.6482 \n",
      "Epoch: 3 | Iter:   20 | Time: 00:00:00 | Train Loss: 0.2814 | Average Loss: 1.3695 \n",
      "Accuracy: 0.729167 | Time: 00:00:00\n",
      "Epoch: 4 | Iter:    0 | Time: 00:00:00 | Train Loss: 0.0745 | Average Loss: 1.2270 \n",
      "Epoch: 4 | Iter:   20 | Time: 00:00:00 | Train Loss: 0.1419 | Average Loss: 1.0653 \n",
      "Accuracy: 0.750000 | Time: 00:00:00\n",
      "Epoch: 5 | Iter:    0 | Time: 00:00:00 | Train Loss: 0.0577 | Average Loss: 0.9796 \n",
      "Epoch: 5 | Iter:   20 | Time: 00:00:00 | Train Loss: 0.1297 | Average Loss: 0.8751 \n",
      "Accuracy: 0.812500 | Time: 00:00:00\n",
      "Epoch: 6 | Iter:    0 | Time: 00:00:00 | Train Loss: 0.0597 | Average Loss: 0.8148 \n",
      "Epoch: 6 | Iter:   20 | Time: 00:00:01 | Train Loss: 0.0633 | Average Loss: 0.7418 \n",
      "Accuracy: 0.854167 | Time: 00:00:00\n",
      "Epoch: 7 | Iter:    0 | Time: 00:00:01 | Train Loss: 0.0741 | Average Loss: 0.6971 \n",
      "Epoch: 7 | Iter:   20 | Time: 00:00:01 | Train Loss: 0.0302 | Average Loss: 0.6397 \n",
      "Accuracy: 0.875000 | Time: 00:00:00\n",
      "Epoch: 8 | Iter:    0 | Time: 00:00:01 | Train Loss: 0.0154 | Average Loss: 0.6060 \n",
      "Epoch: 8 | Iter:   20 | Time: 00:00:01 | Train Loss: 0.0861 | Average Loss: 0.5628 \n",
      "Accuracy: 0.854167 | Time: 00:00:00\n",
      "Epoch: 9 | Iter:    0 | Time: 00:00:01 | Train Loss: 0.0199 | Average Loss: 0.5367 \n",
      "Epoch: 9 | Iter:   20 | Time: 00:00:01 | Train Loss: 0.0229 | Average Loss: 0.5027 \n",
      "Accuracy: 0.875000 | Time: 00:00:00\n",
      "Epoch:10 | Iter:    0 | Time: 00:00:01 | Train Loss: 0.0135 | Average Loss: 0.4814 \n",
      "Epoch:10 | Iter:   20 | Time: 00:00:01 | Train Loss: 0.0222 | Average Loss: 0.4537 \n",
      "Accuracy: 0.854167 | Time: 00:00:00\n",
      "Epoch:11 | Iter:    0 | Time: 00:00:01 | Train Loss: 0.0200 | Average Loss: 0.4362 \n",
      "Epoch:11 | Iter:   20 | Time: 00:00:01 | Train Loss: 0.0229 | Average Loss: 0.4135 \n",
      "Accuracy: 0.854167 | Time: 00:00:00\n",
      "Epoch:12 | Iter:    0 | Time: 00:00:01 | Train Loss: 0.0109 | Average Loss: 0.3990 \n",
      "Epoch:12 | Iter:   20 | Time: 00:00:01 | Train Loss: 0.0304 | Average Loss: 0.3800 \n",
      "Accuracy: 0.875000 | Time: 00:00:00\n",
      "Epoch:13 | Iter:    0 | Time: 00:00:01 | Train Loss: 0.0181 | Average Loss: 0.3679 \n",
      "Epoch:13 | Iter:   20 | Time: 00:00:02 | Train Loss: 0.0215 | Average Loss: 0.3519 \n",
      "Accuracy: 0.875000 | Time: 00:00:00\n",
      "Epoch:14 | Iter:    0 | Time: 00:00:02 | Train Loss: 0.0100 | Average Loss: 0.3414 \n",
      "Epoch:14 | Iter:   20 | Time: 00:00:02 | Train Loss: 0.0112 | Average Loss: 0.3275 \n",
      "Accuracy: 0.875000 | Time: 00:00:00\n",
      "Epoch:15 | Iter:    0 | Time: 00:00:02 | Train Loss: 0.0052 | Average Loss: 0.3185 \n",
      "Epoch:15 | Iter:   20 | Time: 00:00:02 | Train Loss: 0.0134 | Average Loss: 0.3065 \n",
      "Accuracy: 0.854167 | Time: 00:00:00\n",
      "Epoch:16 | Iter:    0 | Time: 00:00:02 | Train Loss: 0.0141 | Average Loss: 0.2986 \n",
      "Epoch:16 | Iter:   20 | Time: 00:00:02 | Train Loss: 0.0191 | Average Loss: 0.2879 \n",
      "Accuracy: 0.875000 | Time: 00:00:00\n",
      "Epoch:17 | Iter:    0 | Time: 00:00:02 | Train Loss: 0.0094 | Average Loss: 0.2808 \n",
      "Epoch:17 | Iter:   20 | Time: 00:00:02 | Train Loss: 0.0172 | Average Loss: 0.2713 \n",
      "Accuracy: 0.875000 | Time: 00:00:00\n",
      "Epoch:18 | Iter:    0 | Time: 00:00:02 | Train Loss: 0.0037 | Average Loss: 0.2651 \n",
      "Epoch:18 | Iter:   20 | Time: 00:00:02 | Train Loss: 0.0122 | Average Loss: 0.2567 \n",
      "Accuracy: 0.875000 | Time: 00:00:00\n",
      "Epoch:19 | Iter:    0 | Time: 00:00:02 | Train Loss: 0.0077 | Average Loss: 0.2511 \n",
      "Epoch:19 | Iter:   20 | Time: 00:00:02 | Train Loss: 0.0135 | Average Loss: 0.2436 \n",
      "Accuracy: 0.875000 | Time: 00:00:00\n",
      "Epoch:20 | Iter:    0 | Time: 00:00:02 | Train Loss: 0.0042 | Average Loss: 0.2386 \n",
      "Epoch:20 | Iter:   20 | Time: 00:00:03 | Train Loss: 0.0096 | Average Loss: 0.2318 \n",
      "Accuracy: 0.875000 | Time: 00:00:00\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------------\n",
    "#       Fine-Tune Pretrained Network\n",
    "#--------------------------------------------------\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from torchvision import models\n",
    "\n",
    "class alexTNet(nn.Module):\n",
    "  def __init__(self):\n",
    "      super(alexTNet, self).__init__()\n",
    "      self.features = models.alexnet(weights='DEFAULT').features\n",
    "      # Freeze convolutional layers\n",
    "      for param in self.features.parameters():\n",
    "        param.requires_grad = False\n",
    "      self.classifier = nn.Sequential(\n",
    "            nn.Linear(256 * 6 * 6, 1024),  # Modify input size to match AlexNet's output\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(1024, 20)  # Modify output size to match number of classes\n",
    "        )  \n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "net = alexTNet()\n",
    "train_option = {}\n",
    "train_option['epoch'] = 20\n",
    "train_option['lr'] = 0.001\n",
    "train_option['device'] = 'gpu'\n",
    "train_option['optimizer'] = optim.SGD(net.parameters(), lr = train_option['lr'], momentum=0.9)\n",
    "trainModel(net, trainloader_large, train_option, testloader_large)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "asBxQT03O1XC"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "The last two FC layers have been replaced.\n",
    "\n",
    "The architecture of the new layers added including activation methods:\n",
    "\n",
    "Data augmentation: left-right flip (mirror)\n",
    "\n",
    "Data normalization: subtract the mean from every image\n",
    "\n",
    "Features Layers : same as AlexNet\n",
    "\n",
    "Classifier Layer 1: Dropout: same as AlexNet\n",
    "\n",
    "Classifier Layer 2: FC: same as AlexNet\n",
    "\n",
    "Classifier Layer 3: ReLU: same as AlexNet\n",
    "\n",
    "Classifier Layer 4: Dropout: same as AlexNet\n",
    "\n",
    "Classifier Layer 5: FC: nn.Linear(4096, 1024)\n",
    "\n",
    "Classifier Layer 6: ReLU:\n",
    "\n",
    "Classifier Layer 7: FC: nn.Linear(1024, 22)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nzEaPw-kMudB"
   },
   "source": [
    "### SVM\n",
    "**Strategy B**: One could use those 1000 activations as a feature in place of a hand crafted feature such as a bag-of-features representation. We trained a classifier (typically a linear SVM) in that 1000 dimensional feature space. However, those activations are clearly very object specific and may not generalize well to new recognition tasks. It is generally better to use the activations in slightly earlier layers of the network, e.g. the 4096 activations in the last 2nd fully-connected layer.\n",
    "\n",
    "Below is an implementation for Strategy B where we use the activations of the pre-trained network as features to train one-vs-all SVMs for the scene classification task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "K9pmTLWH7KIs"
   },
   "outputs": [],
   "source": [
    "#--------------------------------------------------\n",
    "#       Get Features from AlexNet\n",
    "#--------------------------------------------------\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from torchvision import models\n",
    "from sklearn import svm\n",
    "\n",
    "class alexFeatTNet(nn.Module):\n",
    "  def __init__(self):\n",
    "    # super(alexFeatTNet, self).__init__()\n",
    "    # self.alexnet = models.alexnet(weights = 'DEFAULT')\n",
    "    # self.features = nn.Sequential(*list(self.alexnet.features.children()))\n",
    "    # for w in self.features.parameters():\n",
    "    #     w.requires_grad = False\n",
    "    super(alexFeatTNet, self).__init__()\n",
    "    self.alexnet = models.alexnet(weights='DEFAULT')\n",
    "    self.features = nn.Sequential(*list(self.alexnet.features.children()))\n",
    "    \n",
    "    # Remove last linear layer\n",
    "    self.avgpool = nn.AdaptiveAvgPool2d(output_size=(6, 6))\n",
    "    self.classifier = nn.Sequential(*list(self.alexnet.classifier.children())[:-2])\n",
    "    \n",
    "    # Freeze the parameters of the feature extractor\n",
    "    for w in self.features.parameters():\n",
    "        w.requires_grad = False\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    # features = self.features(x)\n",
    "    # print(features.shape)\n",
    "    features = self.features(x)\n",
    "    features = self.avgpool(features)\n",
    "    features = features.view(features.size(0), -1)\n",
    "    features = self.classifier(features)\n",
    "    return features[:, :1000]\n",
    "\n",
    "def predFeat(net, testloader_large):\n",
    "    features_list = []\n",
    "    label_list = []\n",
    "    \n",
    "    # Set the model to evaluation mode\n",
    "    net.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader_large:\n",
    "            images = images.float()  # Ensure images are on the same device as the network\n",
    "            labels = labels.float().cpu().numpy()  # Convert labels to numpy on CPU\n",
    "            \n",
    "            # Compute features in batches to utilize GPU efficiently\n",
    "            features = net(images).detach().cpu().numpy().reshape(len(images), -1)\n",
    "            features_list.append(features)\n",
    "            label_list.extend(labels)\n",
    "    \n",
    "    # Concatenate features and labels\n",
    "    feat = np.vstack(features_list)\n",
    "    label = np.array(label_list)\n",
    "    # print(feat.shape)\n",
    "    # print(label.shape)\n",
    "    return feat, label\n",
    "\n",
    "\n",
    "net = alexFeatTNet()\n",
    "# print(net)\n",
    "train_feat, train_label = predFeat(net, trainloader_large)\n",
    "test_feat, test_label = predFeat(net, testloader_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "ghPvfDAUT-Xs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0.8870413303375244\n",
      "Accuracy:0.812500 | Time: 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# model training: take feature and label, return model\n",
    "def train_SVM(X, Y):\n",
    "    clf = svm.LinearSVC(random_state=0, tol=1e-4, C=22, loss='squared_hinge', max_iter=1000)\n",
    "    clf.fit(X, Y)\n",
    "    return clf\n",
    "\n",
    "# prediction: take feature and model, return label\n",
    "def predict_SVM(clf, X):\n",
    "    predict = clf.predict(X)\n",
    "    return predict\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "start_time = time.time()\n",
    "train_feat_cut = list(map(lambda x:x[0:1000],train_feat))\n",
    "clfs = train_SVM(train_feat_cut, train_label)\n",
    "\n",
    "time_lapse = time.time() - start_time\n",
    "print('Time: {}'.format(time_lapse))\n",
    "\n",
    "start_time = time.time()\n",
    "# evaluation\n",
    "predictions = [-1]*len(test_feat)\n",
    "for i in np.arange(len(test_feat)):\n",
    "    predictions[i] = predict_SVM(clfs, np.reshape(test_feat[i][0:1000],(1,-1)))\n",
    "\n",
    "predictions = np.reshape(np.array(predictions),(-1))\n",
    "\n",
    "acc = np.sum(np.array(predictions) == np.array(test_label)) / float(len(test_label))\n",
    "time_lapse = time.strftime('%H:%M:%S', time.gmtime(time.time() - start_time))\n",
    "print('Accuracy:{:5f} | Time: {}'.format(acc,time_lapse))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ner3KHojkCuy"
   },
   "source": [
    "### Resnet34\n",
    "Fine tune the [ResNet network](https://pytorch.org/docs/stable/_modules/torchvision/models/resnet.html) [(paper)](https://arxiv.org/abs/1512.03385) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "wC0I4MWBk7pr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Iter:    0 | Time: 00:00:00 | Train Loss: 6.8686 | Average Loss: 6.8686 \n",
      "Epoch: 1 | Iter:   20 | Time: 00:00:00 | Train Loss: 3.3252 | Average Loss: 5.5078 \n",
      "Accuracy: 0.488971 | Time: 00:00:00\n",
      "Epoch: 2 | Iter:    0 | Time: 00:00:00 | Train Loss: 1.8236 | Average Loss: 4.4289 \n",
      "Epoch: 2 | Iter:   20 | Time: 00:00:01 | Train Loss: 1.4850 | Average Loss: 3.5438 \n",
      "Accuracy: 0.757353 | Time: 00:00:00\n",
      "Epoch: 3 | Iter:    0 | Time: 00:00:01 | Train Loss: 0.9701 | Average Loss: 3.1415 \n",
      "Epoch: 3 | Iter:   20 | Time: 00:00:01 | Train Loss: 0.9258 | Average Loss: 2.7270 \n",
      "Accuracy: 0.895221 | Time: 00:00:00\n",
      "Epoch: 4 | Iter:    0 | Time: 00:00:02 | Train Loss: 0.6365 | Average Loss: 2.5015 \n",
      "Epoch: 4 | Iter:   20 | Time: 00:00:02 | Train Loss: 0.6450 | Average Loss: 2.2444 \n",
      "Accuracy: 0.944853 | Time: 00:00:00\n",
      "Epoch: 5 | Iter:    0 | Time: 00:00:02 | Train Loss: 0.4547 | Average Loss: 2.0958 \n",
      "Epoch: 5 | Iter:   20 | Time: 00:00:03 | Train Loss: 0.4855 | Average Loss: 1.9181 \n",
      "Accuracy: 0.968750 | Time: 00:00:00\n",
      "Epoch: 6 | Iter:    0 | Time: 00:00:03 | Train Loss: 0.3538 | Average Loss: 1.8118 \n",
      "Epoch: 6 | Iter:   20 | Time: 00:00:03 | Train Loss: 0.3869 | Average Loss: 1.6809 \n",
      "Accuracy: 0.972426 | Time: 00:00:00\n",
      "Epoch: 7 | Iter:    0 | Time: 00:00:04 | Train Loss: 0.2876 | Average Loss: 1.6007 \n",
      "Epoch: 7 | Iter:   20 | Time: 00:00:04 | Train Loss: 0.3215 | Average Loss: 1.4999 \n",
      "Accuracy: 0.979779 | Time: 00:00:00\n",
      "Epoch: 8 | Iter:    0 | Time: 00:00:04 | Train Loss: 0.2412 | Average Loss: 1.4370 \n",
      "Epoch: 8 | Iter:   20 | Time: 00:00:05 | Train Loss: 0.2761 | Average Loss: 1.3568 \n",
      "Accuracy: 0.981618 | Time: 00:00:00\n",
      "Epoch: 9 | Iter:    0 | Time: 00:00:05 | Train Loss: 0.2075 | Average Loss: 1.3060 \n",
      "Epoch: 9 | Iter:   20 | Time: 00:00:05 | Train Loss: 0.2426 | Average Loss: 1.2405 \n",
      "Accuracy: 0.983456 | Time: 00:00:00\n",
      "Epoch:10 | Iter:    0 | Time: 00:00:06 | Train Loss: 0.1820 | Average Loss: 1.1986 \n",
      "Epoch:10 | Iter:   20 | Time: 00:00:06 | Train Loss: 0.2167 | Average Loss: 1.1439 \n",
      "Accuracy: 0.983456 | Time: 00:00:00\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------------\n",
    "#       Fine-Tune Pretrained Network\n",
    "#--------------------------------------------------\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from torchvision import models\n",
    "\n",
    "class resTNet(nn.Module):\n",
    "  def __init__(self):\n",
    "      super(resTNet, self).__init__()\n",
    "      self.resTNet = models.resnet34(weights = 'DEFAULT')\n",
    "      self.features = nn.Sequential(*list(self.resTNet.children())[:-1])\n",
    "      self.fc = nn.Linear(in_features=512, out_features=1000, bias=True)\n",
    "      # print(self)\n",
    "      # Freeze convolutional layers\n",
    "      for param in self.features.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "net = resTNet()\n",
    "train_option={}\n",
    "train_option['lr'] = 0.0015\n",
    "train_option['epoch'] = 10\n",
    "train_option['device'] = 'gpu'\n",
    "train_option['optimizer'] = optim.SGD(net.parameters(), lr = train_option['lr'], momentum=0.9)\n",
    "trainModel(net, trainloader_large, train_option, trainloader_large)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bTAM2H6JkRt3"
   },
   "source": [
    "### Tramsformers\n",
    "We built a Vision Transformer (ViT)! Transformers have achieved significant success in the field of Natural Language Processing, such as machine translation. ViT explores how to utilize transformers into computer vision tasks and attains excellent results compared to state-of-the-art convolutional networks.\n",
    "\n",
    "Transformer paper: https://arxiv.org/pdf/1706.03762.pdf\n",
    "\n",
    "Transformer tutorial: http://jalammar.github.io/illustrated-transformer/\n",
    "\n",
    "ViT paper: https://openreview.net/pdf?id=YicbFdNTTy\n",
    "\n",
    "ViT tutorial: https://www.youtube.com/watch?v=HZ4j_U3FC94\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jYLg1O6GnxOb"
   },
   "source": [
    "ViT consists of multiple transformer encoder layers (i.e. the left block of the following image). Our implementation includes a multi-head attention layer, a feed forward layer, two norm layers and two residual connections.\n",
    "\n",
    "\n",
    "<img src=\"https://production-media.paperswithcode.com/method_collections/trans.jpeg\" width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "FWXXZiYpUbH-"
   },
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "  def __init__(self,\n",
    "               embedding_dims = 128,\n",
    "               dropout=0.1,\n",
    "               mlp_hidden_dim = 32,\n",
    "               num_heads = 2,\n",
    "               ):\n",
    "    super().__init__()\n",
    "\n",
    "    self.self_attn = nn.MultiheadAttention(embedding_dims, num_heads, batch_first = True)\n",
    "    self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embedding_dims, mlp_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_hidden_dim, embedding_dims)\n",
    "    )\n",
    "    self.norm1 = nn.LayerNorm(embedding_dims)\n",
    "    self.norm2 = nn.LayerNorm(embedding_dims)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "\n",
    "  def forward(self,x): \n",
    "    [attn_output, _] = self.self_attn(x,x,x)\n",
    "    x = self.norm1(x + self.dropout(attn_output))\n",
    "    ff_output = self.feed_forward(x)\n",
    "    x = self.norm2(x + self.dropout(ff_output))\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HgAZ-9DQueBl"
   },
   "source": [
    "<img src=https://production-media.paperswithcode.com/methods/Screen_Shot_2021-01-26_at_9.43.31_PM_uI4jjMq.png width=\"400\">\n",
    "\n",
    "ViT consists:\n",
    "\n",
    "\n",
    "**1) Data Preprocessing**: Patchify process to transfer the image (batch_size, channel, height, width) to a sequence of tokens (batch_size, num_tokens, embedding_dimension).\n",
    "\n",
    "**2) Positional Encoding**\n",
    "\n",
    "**3) Extra Learnable [CLASS] embedding**\n",
    "\n",
    "**4) Transformer Encoder**\n",
    "\n",
    "**5) Prediction**: MLP Head and perform classification.\n",
    "\n",
    "Test ViT on the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "cRWlimcVUYgT"
   },
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "  def __init__(self,\n",
    "               img_size = 64,\n",
    "               in_channels = 1,\n",
    "               patch_size = 16,\n",
    "               embedding_dims = 128,\n",
    "               num_transformer_layers = 2,\n",
    "               dropout = 0.1,\n",
    "               mlp_hidden_dim = 128,\n",
    "               num_heads = 2,\n",
    "               num_classes = 22):\n",
    "    super().__init__()\n",
    "    '''\n",
    "    img_size: img height and width.\n",
    "    in_channels: 3 if RGB image, 1 if gray iamge\n",
    "    patch_size: cut the image into patch_size * patch_size sub-regions\n",
    "    embedding_dims: feature dimension of tokens\n",
    "    num_transformer_layers: number of transformer encoder layer\n",
    "    dropout: probability of dropout\n",
    "    mlp_hidden_dim: hidden dim of MLP block\n",
    "    num_heads: number of heads in multi-head attention\n",
    "    num_classes: number of classes to predict\n",
    "\n",
    "    You are suggested but not required to use all inputs.\n",
    "    '''\n",
    "    self.img_size = img_size\n",
    "    self.patch_size = patch_size\n",
    "    self.in_channels = in_channels\n",
    "    \n",
    "    # Determine the number of patches\n",
    "    self.num_patches = (img_size // patch_size) ** 2\n",
    "    \n",
    "    # Patchify layer\n",
    "    self.patchify = nn.Conv2d(in_channels, embedding_dims, kernel_size=patch_size, stride=patch_size)\n",
    "    \n",
    "    # Positional encoding\n",
    "    self.positional_encoding = nn.Parameter(torch.randn(1, self.num_patches + 1, embedding_dims))\n",
    "    \n",
    "    # Extra learnable [CLASS] embedding\n",
    "    self.cls_token = nn.Parameter(torch.randn(1,1,embedding_dims))\n",
    "    \n",
    "    # Transformer encoder layers\n",
    "    self.transformer_encoder_layers = nn.ModuleList([\n",
    "        TransformerEncoderLayer(embedding_dims, dropout, mlp_hidden_dim, num_heads)\n",
    "        # nn.TransformerEncoderLayer(embedding_dims, num_heads, mlp_hidden_dim, dropout, batch_first = True)\n",
    "        for _ in range(num_transformer_layers)\n",
    "    ])\n",
    "    \n",
    "    # MLP head for classification\n",
    "    self.mlp_head = nn.Sequential(\n",
    "        nn.Linear(embedding_dims, mlp_hidden_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(dropout),\n",
    "        nn.Linear(mlp_hidden_dim, num_classes)\n",
    "    )\n",
    "    # print(self)\n",
    "\n",
    "  def forward(self, x):\n",
    "    bs, c, h, w = x.shape\n",
    "    # print(\"before patchify:\", x.shape)\n",
    "    # Patchify the input image\n",
    "    x = self.patchify(x)\n",
    "    x = x.flatten(2).transpose(1, 2)  # (batch_size, num_patches, embedding_dims)\n",
    "    # print(\"after patchify:\", x.shape)\n",
    "    # Repeat self.cls_token along the batch dimension\n",
    "    # print(\"cls_token shape:\", self.cls_token.shape)\n",
    "    \n",
    "    # Concatenate cls_tokens and x along the batch dimension\n",
    "    x = torch.cat((x,self.cls_token.repeat(bs, 1, 1)), dim = 1)\n",
    "    \n",
    "    # print(\"cls token embedding\", x.shape)\n",
    "    \n",
    "    # Add positional encoding\n",
    "    x = x + self.positional_encoding\n",
    "    # print(\"after positional encoding:\", x.shape)\n",
    "    \n",
    "    # Transformer encoder layers\n",
    "    for layer in self.transformer_encoder_layers:\n",
    "        x = layer(x)\n",
    "    \n",
    "    # Extract class token and pass through MLP head for classification\n",
    "    cls_token = x[:, 0]\n",
    "    # print(cls_token)\n",
    "    x = self.mlp_head(cls_token)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "acpN4cQWk98O"
   },
   "outputs": [],
   "source": [
    "from torchvision.datasets.mnist import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform = ToTensor()\n",
    "train_set = MNIST(root='/content/sample_data', train=True, download=True, transform=transform)\n",
    "test_set = MNIST(root='/content/sample_data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_set, shuffle=True, batch_size=128)\n",
    "test_loader = DataLoader(test_set, shuffle=False, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "L5wRBH8KlAM_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Iter:    0 | Time: 00:00:00 | Train Loss: 2.3257 | Average Loss: 2.3257 \n",
      "Epoch: 1 | Iter:   20 | Time: 00:00:00 | Train Loss: 2.0714 | Average Loss: 2.2610 \n",
      "Epoch: 1 | Iter:   40 | Time: 00:00:00 | Train Loss: 1.1242 | Average Loss: 1.8970 \n",
      "Epoch: 1 | Iter:   60 | Time: 00:00:00 | Train Loss: 0.5462 | Average Loss: 1.5308 \n",
      "Epoch: 1 | Iter:   80 | Time: 00:00:01 | Train Loss: 0.5871 | Average Loss: 1.2971 \n",
      "Epoch: 1 | Iter:  100 | Time: 00:00:01 | Train Loss: 0.4029 | Average Loss: 1.1287 \n",
      "Epoch: 1 | Iter:  120 | Time: 00:00:01 | Train Loss: 0.3442 | Average Loss: 1.0102 \n",
      "Epoch: 1 | Iter:  140 | Time: 00:00:01 | Train Loss: 0.2633 | Average Loss: 0.9147 \n",
      "Epoch: 1 | Iter:  160 | Time: 00:00:02 | Train Loss: 0.2420 | Average Loss: 0.8416 \n",
      "Epoch: 1 | Iter:  180 | Time: 00:00:02 | Train Loss: 0.2500 | Average Loss: 0.7804 \n",
      "Epoch: 1 | Iter:  200 | Time: 00:00:02 | Train Loss: 0.3058 | Average Loss: 0.7284 \n",
      "Epoch: 1 | Iter:  220 | Time: 00:00:02 | Train Loss: 0.3297 | Average Loss: 0.6887 \n",
      "Epoch: 1 | Iter:  240 | Time: 00:00:03 | Train Loss: 0.1352 | Average Loss: 0.6544 \n",
      "Epoch: 1 | Iter:  260 | Time: 00:00:03 | Train Loss: 0.3202 | Average Loss: 0.6249 \n",
      "Epoch: 1 | Iter:  280 | Time: 00:00:03 | Train Loss: 0.3059 | Average Loss: 0.6029 \n",
      "Epoch: 1 | Iter:  300 | Time: 00:00:03 | Train Loss: 0.2957 | Average Loss: 0.5798 \n",
      "Epoch: 1 | Iter:  320 | Time: 00:00:04 | Train Loss: 0.2319 | Average Loss: 0.5597 \n",
      "Epoch: 1 | Iter:  340 | Time: 00:00:04 | Train Loss: 0.2402 | Average Loss: 0.5402 \n",
      "Epoch: 1 | Iter:  360 | Time: 00:00:04 | Train Loss: 0.1832 | Average Loss: 0.5221 \n",
      "Epoch: 1 | Iter:  380 | Time: 00:00:04 | Train Loss: 0.4197 | Average Loss: 0.5071 \n",
      "Epoch: 1 | Iter:  400 | Time: 00:00:05 | Train Loss: 0.2821 | Average Loss: 0.4922 \n",
      "Epoch: 1 | Iter:  420 | Time: 00:00:05 | Train Loss: 0.1111 | Average Loss: 0.4782 \n",
      "Epoch: 1 | Iter:  440 | Time: 00:00:05 | Train Loss: 0.1927 | Average Loss: 0.4648 \n",
      "Epoch: 1 | Iter:  460 | Time: 00:00:06 | Train Loss: 0.1838 | Average Loss: 0.4542 \n",
      "Accuracy: 0.941500 | Time: 00:00:00\n",
      "Epoch: 2 | Iter:    0 | Time: 00:00:06 | Train Loss: 0.1503 | Average Loss: 0.4496 \n",
      "Epoch: 2 | Iter:   20 | Time: 00:00:07 | Train Loss: 0.1650 | Average Loss: 0.4401 \n",
      "Epoch: 2 | Iter:   40 | Time: 00:00:07 | Train Loss: 0.2311 | Average Loss: 0.4298 \n",
      "Epoch: 2 | Iter:   60 | Time: 00:00:07 | Train Loss: 0.3200 | Average Loss: 0.4212 \n",
      "Epoch: 2 | Iter:   80 | Time: 00:00:07 | Train Loss: 0.1874 | Average Loss: 0.4132 \n",
      "Epoch: 2 | Iter:  100 | Time: 00:00:08 | Train Loss: 0.1214 | Average Loss: 0.4045 \n",
      "Epoch: 2 | Iter:  120 | Time: 00:00:08 | Train Loss: 0.1253 | Average Loss: 0.3965 \n",
      "Epoch: 2 | Iter:  140 | Time: 00:00:08 | Train Loss: 0.1585 | Average Loss: 0.3893 \n",
      "Epoch: 2 | Iter:  160 | Time: 00:00:08 | Train Loss: 0.1360 | Average Loss: 0.3826 \n",
      "Epoch: 2 | Iter:  180 | Time: 00:00:09 | Train Loss: 0.1463 | Average Loss: 0.3759 \n",
      "Epoch: 2 | Iter:  200 | Time: 00:00:09 | Train Loss: 0.1680 | Average Loss: 0.3707 \n",
      "Epoch: 2 | Iter:  220 | Time: 00:00:09 | Train Loss: 0.1847 | Average Loss: 0.3648 \n",
      "Epoch: 2 | Iter:  240 | Time: 00:00:09 | Train Loss: 0.1333 | Average Loss: 0.3589 \n",
      "Epoch: 2 | Iter:  260 | Time: 00:00:10 | Train Loss: 0.1149 | Average Loss: 0.3532 \n",
      "Epoch: 2 | Iter:  280 | Time: 00:00:10 | Train Loss: 0.1110 | Average Loss: 0.3474 \n",
      "Epoch: 2 | Iter:  300 | Time: 00:00:10 | Train Loss: 0.1595 | Average Loss: 0.3426 \n",
      "Epoch: 2 | Iter:  320 | Time: 00:00:10 | Train Loss: 0.1867 | Average Loss: 0.3381 \n",
      "Epoch: 2 | Iter:  340 | Time: 00:00:11 | Train Loss: 0.1654 | Average Loss: 0.3334 \n",
      "Epoch: 2 | Iter:  360 | Time: 00:00:11 | Train Loss: 0.0804 | Average Loss: 0.3290 \n",
      "Epoch: 2 | Iter:  380 | Time: 00:00:11 | Train Loss: 0.1523 | Average Loss: 0.3250 \n",
      "Epoch: 2 | Iter:  400 | Time: 00:00:12 | Train Loss: 0.1427 | Average Loss: 0.3211 \n",
      "Epoch: 2 | Iter:  420 | Time: 00:00:12 | Train Loss: 0.1174 | Average Loss: 0.3171 \n",
      "Epoch: 2 | Iter:  440 | Time: 00:00:12 | Train Loss: 0.0766 | Average Loss: 0.3133 \n",
      "Epoch: 2 | Iter:  460 | Time: 00:00:12 | Train Loss: 0.1088 | Average Loss: 0.3102 \n",
      "Accuracy: 0.955000 | Time: 00:00:00\n",
      "Epoch: 3 | Iter:    0 | Time: 00:00:13 | Train Loss: 0.1318 | Average Loss: 0.3086 \n",
      "Epoch: 3 | Iter:   20 | Time: 00:00:13 | Train Loss: 0.1243 | Average Loss: 0.3050 \n",
      "Epoch: 3 | Iter:   40 | Time: 00:00:14 | Train Loss: 0.0810 | Average Loss: 0.3011 \n",
      "Epoch: 3 | Iter:   60 | Time: 00:00:14 | Train Loss: 0.0970 | Average Loss: 0.2977 \n",
      "Epoch: 3 | Iter:   80 | Time: 00:00:14 | Train Loss: 0.1838 | Average Loss: 0.2944 \n",
      "Epoch: 3 | Iter:  100 | Time: 00:00:14 | Train Loss: 0.0763 | Average Loss: 0.2914 \n",
      "Epoch: 3 | Iter:  120 | Time: 00:00:15 | Train Loss: 0.1670 | Average Loss: 0.2887 \n",
      "Epoch: 3 | Iter:  140 | Time: 00:00:15 | Train Loss: 0.1375 | Average Loss: 0.2862 \n",
      "Epoch: 3 | Iter:  160 | Time: 00:00:15 | Train Loss: 0.1505 | Average Loss: 0.2830 \n",
      "Epoch: 3 | Iter:  180 | Time: 00:00:16 | Train Loss: 0.1293 | Average Loss: 0.2805 \n",
      "Epoch: 3 | Iter:  200 | Time: 00:00:16 | Train Loss: 0.1837 | Average Loss: 0.2780 \n",
      "Epoch: 3 | Iter:  220 | Time: 00:00:16 | Train Loss: 0.1368 | Average Loss: 0.2754 \n",
      "Epoch: 3 | Iter:  240 | Time: 00:00:16 | Train Loss: 0.0762 | Average Loss: 0.2727 \n",
      "Epoch: 3 | Iter:  260 | Time: 00:00:17 | Train Loss: 0.1505 | Average Loss: 0.2702 \n",
      "Epoch: 3 | Iter:  280 | Time: 00:00:17 | Train Loss: 0.2346 | Average Loss: 0.2680 \n",
      "Epoch: 3 | Iter:  300 | Time: 00:00:17 | Train Loss: 0.1142 | Average Loss: 0.2659 \n",
      "Epoch: 3 | Iter:  320 | Time: 00:00:17 | Train Loss: 0.2645 | Average Loss: 0.2637 \n",
      "Epoch: 3 | Iter:  340 | Time: 00:00:18 | Train Loss: 0.0821 | Average Loss: 0.2617 \n",
      "Epoch: 3 | Iter:  360 | Time: 00:00:18 | Train Loss: 0.1273 | Average Loss: 0.2593 \n",
      "Epoch: 3 | Iter:  380 | Time: 00:00:18 | Train Loss: 0.1518 | Average Loss: 0.2576 \n",
      "Epoch: 3 | Iter:  400 | Time: 00:00:18 | Train Loss: 0.1045 | Average Loss: 0.2555 \n",
      "Epoch: 3 | Iter:  420 | Time: 00:00:19 | Train Loss: 0.1826 | Average Loss: 0.2536 \n",
      "Epoch: 3 | Iter:  440 | Time: 00:00:19 | Train Loss: 0.2053 | Average Loss: 0.2515 \n",
      "Epoch: 3 | Iter:  460 | Time: 00:00:19 | Train Loss: 0.1082 | Average Loss: 0.2496 \n",
      "Accuracy: 0.972200 | Time: 00:00:00\n",
      "Epoch: 4 | Iter:    0 | Time: 00:00:20 | Train Loss: 0.0498 | Average Loss: 0.2487 \n",
      "Epoch: 4 | Iter:   20 | Time: 00:00:20 | Train Loss: 0.1284 | Average Loss: 0.2466 \n",
      "Epoch: 4 | Iter:   40 | Time: 00:00:21 | Train Loss: 0.0891 | Average Loss: 0.2447 \n",
      "Epoch: 4 | Iter:   60 | Time: 00:00:21 | Train Loss: 0.1953 | Average Loss: 0.2431 \n",
      "Epoch: 4 | Iter:   80 | Time: 00:00:21 | Train Loss: 0.1441 | Average Loss: 0.2414 \n",
      "Epoch: 4 | Iter:  100 | Time: 00:00:21 | Train Loss: 0.0867 | Average Loss: 0.2397 \n",
      "Epoch: 4 | Iter:  120 | Time: 00:00:22 | Train Loss: 0.1207 | Average Loss: 0.2382 \n",
      "Epoch: 4 | Iter:  140 | Time: 00:00:22 | Train Loss: 0.0884 | Average Loss: 0.2366 \n",
      "Epoch: 4 | Iter:  160 | Time: 00:00:22 | Train Loss: 0.1410 | Average Loss: 0.2351 \n",
      "Epoch: 4 | Iter:  180 | Time: 00:00:22 | Train Loss: 0.1054 | Average Loss: 0.2335 \n",
      "Epoch: 4 | Iter:  200 | Time: 00:00:23 | Train Loss: 0.0774 | Average Loss: 0.2318 \n",
      "Epoch: 4 | Iter:  220 | Time: 00:00:23 | Train Loss: 0.0439 | Average Loss: 0.2302 \n",
      "Epoch: 4 | Iter:  240 | Time: 00:00:23 | Train Loss: 0.1394 | Average Loss: 0.2288 \n",
      "Epoch: 4 | Iter:  260 | Time: 00:00:23 | Train Loss: 0.1210 | Average Loss: 0.2275 \n",
      "Epoch: 4 | Iter:  280 | Time: 00:00:24 | Train Loss: 0.1284 | Average Loss: 0.2264 \n",
      "Epoch: 4 | Iter:  300 | Time: 00:00:24 | Train Loss: 0.1360 | Average Loss: 0.2249 \n",
      "Epoch: 4 | Iter:  320 | Time: 00:00:24 | Train Loss: 0.2608 | Average Loss: 0.2240 \n",
      "Epoch: 4 | Iter:  340 | Time: 00:00:25 | Train Loss: 0.2160 | Average Loss: 0.2229 \n",
      "Epoch: 4 | Iter:  360 | Time: 00:00:25 | Train Loss: 0.2039 | Average Loss: 0.2217 \n",
      "Epoch: 4 | Iter:  380 | Time: 00:00:25 | Train Loss: 0.0761 | Average Loss: 0.2205 \n",
      "Epoch: 4 | Iter:  400 | Time: 00:00:25 | Train Loss: 0.0798 | Average Loss: 0.2191 \n",
      "Epoch: 4 | Iter:  420 | Time: 00:00:26 | Train Loss: 0.0718 | Average Loss: 0.2179 \n",
      "Epoch: 4 | Iter:  440 | Time: 00:00:26 | Train Loss: 0.1324 | Average Loss: 0.2166 \n",
      "Epoch: 4 | Iter:  460 | Time: 00:00:26 | Train Loss: 0.1248 | Average Loss: 0.2158 \n",
      "Accuracy: 0.966700 | Time: 00:00:00\n",
      "Epoch: 5 | Iter:    0 | Time: 00:00:27 | Train Loss: 0.1034 | Average Loss: 0.2153 \n",
      "Epoch: 5 | Iter:   20 | Time: 00:00:27 | Train Loss: 0.0624 | Average Loss: 0.2140 \n",
      "Epoch: 5 | Iter:   40 | Time: 00:00:27 | Train Loss: 0.0759 | Average Loss: 0.2127 \n",
      "Epoch: 5 | Iter:   60 | Time: 00:00:28 | Train Loss: 0.0853 | Average Loss: 0.2114 \n",
      "Epoch: 5 | Iter:   80 | Time: 00:00:28 | Train Loss: 0.0731 | Average Loss: 0.2101 \n",
      "Epoch: 5 | Iter:  100 | Time: 00:00:28 | Train Loss: 0.1345 | Average Loss: 0.2092 \n",
      "Epoch: 5 | Iter:  120 | Time: 00:00:28 | Train Loss: 0.0815 | Average Loss: 0.2082 \n",
      "Epoch: 5 | Iter:  140 | Time: 00:00:29 | Train Loss: 0.1545 | Average Loss: 0.2072 \n",
      "Epoch: 5 | Iter:  160 | Time: 00:00:29 | Train Loss: 0.0692 | Average Loss: 0.2064 \n",
      "Epoch: 5 | Iter:  180 | Time: 00:00:29 | Train Loss: 0.0836 | Average Loss: 0.2055 \n",
      "Epoch: 5 | Iter:  200 | Time: 00:00:29 | Train Loss: 0.1291 | Average Loss: 0.2046 \n",
      "Epoch: 5 | Iter:  220 | Time: 00:00:30 | Train Loss: 0.0614 | Average Loss: 0.2038 \n",
      "Epoch: 5 | Iter:  240 | Time: 00:00:30 | Train Loss: 0.1024 | Average Loss: 0.2029 \n",
      "Epoch: 5 | Iter:  260 | Time: 00:00:30 | Train Loss: 0.0921 | Average Loss: 0.2020 \n",
      "Epoch: 5 | Iter:  280 | Time: 00:00:30 | Train Loss: 0.0201 | Average Loss: 0.2008 \n",
      "Epoch: 5 | Iter:  300 | Time: 00:00:31 | Train Loss: 0.0896 | Average Loss: 0.2001 \n",
      "Epoch: 5 | Iter:  320 | Time: 00:00:31 | Train Loss: 0.0730 | Average Loss: 0.1991 \n",
      "Epoch: 5 | Iter:  340 | Time: 00:00:31 | Train Loss: 0.0823 | Average Loss: 0.1983 \n",
      "Epoch: 5 | Iter:  360 | Time: 00:00:31 | Train Loss: 0.0919 | Average Loss: 0.1975 \n",
      "Epoch: 5 | Iter:  380 | Time: 00:00:32 | Train Loss: 0.0665 | Average Loss: 0.1967 \n",
      "Epoch: 5 | Iter:  400 | Time: 00:00:32 | Train Loss: 0.0255 | Average Loss: 0.1957 \n",
      "Epoch: 5 | Iter:  420 | Time: 00:00:32 | Train Loss: 0.1486 | Average Loss: 0.1948 \n",
      "Epoch: 5 | Iter:  440 | Time: 00:00:32 | Train Loss: 0.0757 | Average Loss: 0.1940 \n",
      "Epoch: 5 | Iter:  460 | Time: 00:00:33 | Train Loss: 0.2183 | Average Loss: 0.1932 \n",
      "Accuracy: 0.973200 | Time: 00:00:00\n"
     ]
    }
   ],
   "source": [
    "net = ViT(img_size = 28,\n",
    "          in_channels = 1,\n",
    "          patch_size = 7,\n",
    "          embedding_dims = 128,\n",
    "          num_transformer_layers = 3,\n",
    "          dropout = 0.1,\n",
    "          mlp_hidden_dim = 128,\n",
    "          num_heads = 2,\n",
    "          num_classes = 10)\n",
    "train_option = {}\n",
    "train_option['lr'] = 0.001\n",
    "train_option['epoch'] = 5\n",
    "train_option['device'] = 'gpu'\n",
    "trainModel(net, train_loader, train_option, test_loader)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [
    "ZGgDlKIVH_wK"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
